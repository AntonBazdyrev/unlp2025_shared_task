{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6d3328-8f17-4fbb-afa7-a4f8ce573570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbazdyrev99\u001b[0m (\u001b[33mbazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abazdyrev/comps/clf-techs/wandb/run-20250128_001724-k9paygj5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task/runs/k9paygj5' target=\"_blank\">bright-sun-3</a></strong> to <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task/runs/k9paygj5' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task/runs/k9paygj5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-clf-task/runs/k9paygj5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79f824479310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(project=\"unlp-clf-task\", entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5187a235-1e7a-493c-af0d-4dabc7c34b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('train.parquet')\n",
    "\n",
    "ssubmission = pd.read_csv('sample_submission.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e56efd-3482-415f-8100-90792e4d56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df['is_valid'] = np.random.binomial(1, 0.2, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ace449a-f98f-45b8-8558-8e2f27fafac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def prompt_generator(text):\n",
    "    conversation = text\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63be635-42db-40f0-993e-65951e360ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'prompt'] = df.content.apply(prompt_generator)\n",
    "test.loc[:, 'prompt'] = test.content.apply(prompt_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db4eb59-0240-4a32-9d22-d38e39073373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56855/3250556059.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6f97768364a1ab3e5a289e0d49625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b08f521959141069605c9747d02a294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "PRETRAINED_MODEL = 'bert-base-multilingual-cased'\n",
    "MAX_LENGTH = 500\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PRETRAINED_MODEL\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "df['full_text'] = df.prompt.progress_apply(\n",
    "    lambda x: tokenizer.decode(tokenizer(x, add_special_tokens=False)['input_ids'][:MAX_LENGTH])\n",
    ")\n",
    "test['full_text'] = test.prompt.progress_apply(\n",
    "    lambda x: tokenizer.decode(tokenizer(x, add_special_tokens=False)['input_ids'][:MAX_LENGTH])\n",
    ")\n",
    "\n",
    "def tokenize(sample):\n",
    "    tokenized = tokenizer(sample['full_text'])\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9bc9269-5b25-4521-8847-fffaa60579ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssubmission = pd.read_csv('sample_submission.csv')\n",
    "targets = ssubmission.set_index('id').columns\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "for col in targets:\n",
    "    df[col] = 0\n",
    "\n",
    "import numpy as np\n",
    "for ind, row in df.iterrows():\n",
    "    if isinstance(row['techniques'], Iterable):\n",
    "        for t in row['techniques']:\n",
    "            df.loc[ind, t] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaba9797-79db-4fa1-b419-ad2315e0cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = list(df[targets].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d12087d-15ac-4c9f-b29a-6e3400c35b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242e8d44132446959cade8f64a11b4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94896e65b544fdfaf809a5413642cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acb691613bb4aae8da4932e1673a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds_train = Dataset.from_pandas(df[df.is_valid == 0][['full_text', 'labels']].copy())\n",
    "ds_eval = Dataset.from_pandas(df[df.is_valid == 1][['full_text', 'labels']].copy())\n",
    "ds_test = Dataset.from_pandas(test[['full_text']].copy())\n",
    "\n",
    "ds_train = ds_train.map(tokenize)\n",
    "remove_columns = [c for c in ds_train.features.keys() if c not in ['input_ids', 'attention_mask', 'labels']]\n",
    "ds_train = ds_train.remove_columns(remove_columns)\n",
    "\n",
    "ds_eval = ds_eval.map(tokenize)\n",
    "remove_columns = [c for c in ds_eval.features.keys() if c not in ['input_ids', 'attention_mask', 'labels']]\n",
    "ds_eval = ds_eval.remove_columns(remove_columns)\n",
    "\n",
    "ds_test = ds_test.map(tokenize)\n",
    "remove_columns = [c for c in ds_test.features.keys() if c not in ['input_ids', 'attention_mask', 'labels']]\n",
    "ds_test = ds_test.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a95a43a-10de-4318-abee-6d4230ab375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a200f391-0721-4ca6-aaa4-156f33a6fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73bd5bed-ec13-4898-805c-92d5861a125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (AutoTokenizer, TrainingArguments, Trainer,\n",
    "                          AutoModelForSequenceClassification, DataCollatorWithPadding)\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits >= 0.0\n",
    "    return {\"f1\": f1_score(labels, predictions, average=\"macro\")}\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"unlp-clf-task\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='model_checkpoints_bert_base',\n",
    "    logging_dir='./model_logs_bert_base',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_torch',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a763d67-ea26-4c83-a860-47c0618c8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = torch.tensor((1/df[targets].mean()).tolist()).cuda()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, *args, **kwargs):\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Initialize BCEWithLogitsLoss with class weights\n",
    "        #loss_fn = BCEWithLogitsLoss(weight=self.class_weights)\n",
    "        loss_fn = BCEWithLogitsLoss()\n",
    "        #print(logits[:2])\n",
    "        #print(inputs['labels'][:2])\n",
    "        loss = loss_fn(logits, inputs['labels'].float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5e0b25-01d0-4cbe-9d80-67d0b162ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56855/2697179512.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [960/960 05:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.275432</td>\n",
       "      <td>0.109515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.256188</td>\n",
       "      <td>0.158918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.254837</td>\n",
       "      <td>0.210196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.254313</td>\n",
       "      <td>0.220650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=960, training_loss=0.2595095120370388, metrics={'train_runtime': 315.8963, 'train_samples_per_second': 48.465, 'train_steps_per_second': 3.039, 'total_flos': 3617273178101688.0, 'train_loss': 0.2595095120370388, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model, \n",
    "    args=train_args, \n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f93f8-7330-4a0c-aab1-cfdb68a02c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a755727-8209-48c7-8676-38cbc7ba9d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
