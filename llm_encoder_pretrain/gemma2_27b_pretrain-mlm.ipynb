{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002d76d4-6170-47d0-989b-29d3a58a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a4f201-7777-4cea-833f-8b1534c7daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PRETRAINED_MODEL = '/home/abazdyrev/pretrained_models/gemma-2-27b-it/'\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e94584-dc08-428e-be4b-15f996a1441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from packaging import version\n",
    "import importlib.metadata\n",
    "\n",
    "from transformers import Gemma2Model, Gemma2ForCausalLM, Gemma2PreTrainedModel, Gemma2Config\n",
    "from transformers.models.gemma2.modeling_gemma2 import (\n",
    "    Gemma2DecoderLayer,\n",
    "    Gemma2Attention,\n",
    "    Gemma2FlashAttention2,\n",
    "    Gemma2SdpaAttention,\n",
    "    Gemma2MLP,\n",
    "    Gemma2RMSNorm,\n",
    ")\n",
    "\n",
    "from torch import nn\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers.utils.import_utils import _is_package_available\n",
    "from transformers.cache_utils import Cache, StaticCache\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers.models.gemma2.modeling_gemma2 import *\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def is_transformers_attn_greater_or_equal_4_41():\n",
    "    if not _is_package_available(\"transformers\"):\n",
    "        return False\n",
    "\n",
    "    return version.parse(importlib.metadata.version(\"transformers\")) >= version.parse(\n",
    "        \"4.41.0\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ModifiedGemma2Attention(Gemma2Attention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedGemma2FlashAttention2(Gemma2FlashAttention2):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedGemma2SdpaAttention(Gemma2SdpaAttention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "GEMMA2_ATTENTION_CLASSES = {\n",
    "    \"eager\": ModifiedGemma2Attention,\n",
    "    \"flash_attention_2\": ModifiedGemma2FlashAttention2,\n",
    "    \"sdpa\": ModifiedGemma2SdpaAttention,\n",
    "}\n",
    "\n",
    "\n",
    "class ModifiedGemma2DecoderLayer(Gemma2DecoderLayer):\n",
    "    def __init__(self, config: Gemma2Config, layer_idx: int):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = GEMMA2_ATTENTION_CLASSES[config._attn_implementation](\n",
    "            config=config, layer_idx=layer_idx\n",
    "        )\n",
    "\n",
    "        self.mlp = Gemma2MLP(config)\n",
    "        self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.is_sliding = not bool(layer_idx % 2)\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.sliding_window = config.sliding_window\n",
    "\n",
    "\n",
    "\n",
    "class Gemma2BiModel(Gemma2Model):\n",
    "    _no_split_modules = [\"ModifiedGemma2DecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: Gemma2Config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ModifiedGemma2DecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if past_key_values is not None:\n",
    "            target_length = past_key_values.get_max_length()\n",
    "        else:\n",
    "            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n",
    "\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n",
    "            if attention_mask.max() != 0:\n",
    "                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            causal_mask = torch.zeros(\n",
    "                (sequence_length, target_length), dtype=dtype, device=device\n",
    "            )\n",
    "            # causal_mask = torch.full(\n",
    "            #     (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            # )\n",
    "            # if sequence_length != 1:\n",
    "            #     causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                    padding_mask, min_dtype\n",
    "                )\n",
    "        return causal_mask\n",
    "\n",
    "\n",
    "class Gemma2BiForMNTP(Gemma2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.model = Gemma2BiModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # getter for PEFT model\n",
    "    def get_model_for_peft(self):\n",
    "        return self.model\n",
    "\n",
    "    # setter for PEFT model\n",
    "    def set_model_for_peft(self, model: PeftModel):\n",
    "        self.model = model\n",
    "\n",
    "    # save the PEFT model\n",
    "    def save_peft_model(self, path):\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "\n",
    "class Gemma2BiMLM(Gemma2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.model = Gemma2BiModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[HybridCache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **loss_kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(prediction_scores.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    # getter for PEFT model\n",
    "    def get_model_for_peft(self):\n",
    "        return self.model\n",
    "\n",
    "    # setter for PEFT model\n",
    "    def set_model_for_peft(self, model: PeftModel):\n",
    "        self.model = model\n",
    "\n",
    "    # save the PEFT model\n",
    "    def save_peft_model(self, path):\n",
    "        self.model.save_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394fb175-ff9d-4ecc-a703-503eb4cee2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2a3de48c9c47e7a5358087fcc4bdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM, AutoModelForMaskedLM, Gemma2ForCausalLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "\n",
    "from transformers import Gemma2ForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = Gemma2BiMLM.from_pretrained(\n",
    "    PRETRAINED_MODEL, torch_dtype=torch.bfloat16,\n",
    "    quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6306d712-ed67-42eb-95bb-49f63fe5b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 456,720,384 || all params: 27,683,848,704 || trainable%: 1.6498\n"
     ]
    }
   ],
   "source": [
    "from transformers import Gemma2ForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=128, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout=0.05, \n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ") \n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15bbc421-90aa-4331-a9a0-8c8bdc82fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "#https://huggingface.co/datasets/zeusfsx/ukrainian-news\n",
    "ds_ua = load_dataset('json', data_dir='/home/abazdyrev/pretrain_data/zeusfsx_ukrainian_news/')\n",
    "sampled_dataset_ua = ds_ua[\"train\"].shuffle(seed=42).select(range(100_000)).select_columns([\"text\"])\n",
    "#https://huggingface.co/datasets/AIR-Bench/qa_news_ru\n",
    "ds_ru = load_dataset('json', data_files='/home/abazdyrev/pretrain_data/qa_news_ru/AIR-Bench_24.05/default/corpus.jsonl')\n",
    "sampled_dataset_ru = ds_ru[\"train\"].shuffle(seed=42).select(range(100_000)).select_columns([\"text\"])\n",
    "\n",
    "dataset = concatenate_datasets([sampled_dataset_ua, sampled_dataset_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6a5a31-3098-4c1e-b078-7f18c5baf7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 200000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c644db-0b01-4a0d-9817-78ae4b4613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=10).remove_columns([\"text\"])    # Adjust num_proc based on CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd216d52-b37d-4375-97e6-821641712837",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token = '<mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c0a9c0-0eba-4af8-b724-6217f1462fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbazdyrev99\u001b[0m (\u001b[33mIASA-BA-Diploma-Ivan-Bashtovyi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abazdyrev/repos/unlp2025_shared_task/llm_encoder_pretrain/wandb/run-20250327_183245-qrrbk3j9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain/runs/qrrbk3j9' target=\"_blank\">gemma2-27b-200k-rows-bidirectional-mlm</a></strong> to <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain/runs/qrrbk3j9' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain/runs/qrrbk3j9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-mlm-pretrain/runs/qrrbk3j9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e9bd1eb0f20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(\n",
    "    project=\"unlp-mlm-pretrain\",\n",
    "    entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\", \n",
    "    name='gemma2-27b-200k-rows-bidirectional-mlm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659e8d50-7e57-4283-ab9c-0650f0f2facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Data collator used for dynamic masking\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./model_checkpoints_gemma2-27-mlm',\n",
    "    logging_dir='./model_logs_gemma2-27-mlm',\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_8bit',\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_steps=3_000\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc6b89-4c98-4b69-8609-78790ab4d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/abazdyrev/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 127/6250 45:30 < 37:08:53, 0.05 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.807700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.731900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.491400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5984a8e-fe32-4b05-9f66-9b515e6a7b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59de93f-dcf8-4872-b176-a9ac87c9c7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c311f52fbd854efa83739f5e30ba07ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM, AutoModelForMaskedLM, Gemma2ForCausalLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "\n",
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM, AutoModelForMaskedLM, Gemma2ForCausalLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "\n",
    "from transformers import Gemma2ForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = Gemma2BiMLM.from_pretrained(\n",
    "    PRETRAINED_MODEL, torch_dtype=torch.bfloat16,\n",
    "    #quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a496200c-f856-453d-adab-922735ac73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d336160a-6126-408c-8d90-1b2c41b7076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, './model_checkpoints_gemma2-27-mlm/checkpoint-6250/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc83e157-6ad1-4f5e-94e5-06f241cd2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617dfb7a-fcd4-455c-91f8-c9286d9a1796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2BiMLM(\n",
       "  (model): Gemma2BiModel(\n",
       "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-45): 46 x ModifiedGemma2DecoderLayer(\n",
       "        (self_attn): ModifiedGemma2Attention(\n",
       "          (q_proj): Linear(in_features=4608, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4608, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
       "          (up_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
       "          (down_proj): Linear(in_features=36864, out_features=4608, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4608, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484bfca2-5c71-46c5-b526-4b4f0b4feb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_directory = \"/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked\"\n",
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13621be-084a-4dd5-b2e9-870ecbc49c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked/tokenizer_config.json',\n",
       " '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked/special_tokens_map.json',\n",
       " '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked/tokenizer.model',\n",
       " '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked/added_tokens.json',\n",
       " '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model weights as a safetensors file (e.g., model.safetensors)\n",
    "merged_model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer files\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04e28a-047c-4683-ba57-0404c35bfbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
