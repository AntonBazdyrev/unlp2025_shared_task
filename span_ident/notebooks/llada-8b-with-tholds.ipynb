{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10668454,"sourceType":"datasetVersion","datasetId":6607405}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f9906349-e332-40f3-a0a8-a766fad53464","cell_type":"code","source":"!pip install bitsandbytes==0.43.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:14.449297Z","iopub.execute_input":"2025-03-06T18:06:14.449519Z","iopub.status.idle":"2025-03-06T18:06:24.500161Z","shell.execute_reply.started":"2025-03-06T18:06:14.449498Z","shell.execute_reply":"2025-03-06T18:06:24.498920Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.43.2\n  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.2) (2.5.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.2) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes==0.43.2) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.2) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.43.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.43.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bitsandbytes==0.43.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bitsandbytes==0.43.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bitsandbytes==0.43.2) (2024.2.0)\nDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.2\n","output_type":"stream"}],"execution_count":1},{"id":"0f36883a-1d31-40ba-8d7f-bfc46f02fa52","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hugginface_key\")\nsecret_value_1 = user_secrets.get_secret(\"wandb_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:24.501322Z","iopub.execute_input":"2025-03-06T18:06:24.501614Z","iopub.status.idle":"2025-03-06T18:06:24.703327Z","shell.execute_reply.started":"2025-03-06T18:06:24.501587Z","shell.execute_reply":"2025-03-06T18:06:24.702307Z"}},"outputs":[],"execution_count":2},{"id":"11242624-d025-4abe-8af0-7ed5335f83ae","cell_type":"code","source":"from huggingface_hub import login\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:24.704181Z","iopub.execute_input":"2025-03-06T18:06:24.704483Z","iopub.status.idle":"2025-03-06T18:06:25.459853Z","shell.execute_reply.started":"2025-03-06T18:06:24.704458Z","shell.execute_reply":"2025-03-06T18:06:25.459252Z"}},"outputs":[],"execution_count":3},{"id":"9283525b-ba75-467a-8b90-9c190ce7f885","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet\")#'train.parquet')\ncv = pd.read_csv(\"/kaggle/input/span-ident-cv-split-csv/cv_split.csv\")#'cv_split.csv')\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(\"/kaggle/input/unlp-2025-shared-task-span-identification/test.csv\")#'test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:25.462222Z","iopub.execute_input":"2025-03-06T18:06:25.462448Z","iopub.status.idle":"2025-03-06T18:06:26.237934Z","shell.execute_reply.started":"2025-03-06T18:06:25.462427Z","shell.execute_reply":"2025-03-06T18:06:26.237186Z"}},"outputs":[],"execution_count":4},{"id":"443e4c9f-7dba-4634-a8b0-5b65f78044b3","cell_type":"code","source":"import spacy\n\nfrom spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\nfrom tqdm.autonotebook import tqdm\ntqdm.pandas()\n\ndf.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:26.239519Z","iopub.execute_input":"2025-03-06T18:06:26.239857Z","iopub.status.idle":"2025-03-06T18:06:32.748595Z","shell.execute_reply.started":"2025-03-06T18:06:26.239822Z","shell.execute_reply":"2025-03-06T18:06:32.747512Z"}},"outputs":[],"execution_count":5},{"id":"3de39a37-a1fb-49a7-af42-f78be7037717","cell_type":"code","source":"PRETRAINED_MODEL = 'GSAI-ML/LLaDA-8B-Base'\n#\"unsloth/gemma-2-2b-it-bnb-4bit\"\n#\"unsloth/gemma-2-9b-it-bnb-4bit\"\n#\"unsloth/gemma-2-2b-it-bnb-4bit\"\n#'google/gemma-2-2b-it'\n#'google/gemma-2-9b-it'\n\nMAX_LEN = 700","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:32.749692Z","iopub.execute_input":"2025-03-06T18:06:32.750339Z","iopub.status.idle":"2025-03-06T18:06:32.755596Z","shell.execute_reply.started":"2025-03-06T18:06:32.750302Z","shell.execute_reply":"2025-03-06T18:06:32.754445Z"}},"outputs":[],"execution_count":6},{"id":"db0d2f0e-3850-4750-b6e0-d8ff71910992","cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:32.756573Z","iopub.execute_input":"2025-03-06T18:06:32.756822Z","iopub.status.idle":"2025-03-06T18:06:38.272086Z","shell.execute_reply.started":"2025-03-06T18:06:32.756799Z","shell.execute_reply":"2025-03-06T18:06:38.271236Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f047f5fb2e4868a863eb125fcf2d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0ad477a6244f37ba8951918f293c64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/766 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a768ec85279945db87e7d5a9f0896618"}},"metadata":{}}],"execution_count":7},{"id":"020ee767-721c-4ae4-867d-f880cb02d398","cell_type":"code","source":"def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n    tokenized_output = tokenizer(\n        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n        truncation=True, padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n\n    # Get subword tokenized versions of the text\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    \n    # Initialize labels as 'O'\n    labels = [0] * len(tokens)\n\n    if trigger_spans is not None:\n        # Assign 'TRIGGER' to overlapping tokens\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n                    labels[i] = 1\n\n    tokenized_output['labels'] = labels\n    # tokenized_output['token_strings'] = token_strings\n    return tokenized_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:38.273101Z","iopub.execute_input":"2025-03-06T18:06:38.273706Z","iopub.status.idle":"2025-03-06T18:06:38.279906Z","shell.execute_reply.started":"2025-03-06T18:06:38.273658Z","shell.execute_reply":"2025-03-06T18:06:38.278616Z"}},"outputs":[],"execution_count":8},{"id":"c668d782-c0ef-42ae-a366-c6a2fe032d25","cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ntqdm.pandas()\n\ndf['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\nfor column in df.seq_labels.iloc[0].keys():\n    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n\ndf_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\nfor column in df_test.seq_labels.iloc[0].keys():\n    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:38.281017Z","iopub.execute_input":"2025-03-06T18:06:38.281300Z","iopub.status.idle":"2025-03-06T18:06:49.919231Z","shell.execute_reply.started":"2025-03-06T18:06:38.281276Z","shell.execute_reply":"2025-03-06T18:06:49.918137Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3822 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7995335e1d4838ba1825a75f77f081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce87bc6e257e454ba8633fc121d131c0"}},"metadata":{}}],"execution_count":9},{"id":"3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f","cell_type":"code","source":"from datasets import Dataset\nimport numpy as np\n\ndf['is_valid'] = df.fold == 4\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\nds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content']\nds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:49.920438Z","iopub.execute_input":"2025-03-06T18:06:49.920777Z","iopub.status.idle":"2025-03-06T18:06:53.908150Z","shell.execute_reply.started":"2025-03-06T18:06:49.920742Z","shell.execute_reply":"2025-03-06T18:06:53.907075Z"}},"outputs":[],"execution_count":10},{"id":"80462f35-ccd1-41f6-9f71-d34047d36beb","cell_type":"markdown","source":"# LLada\n","metadata":{}},{"id":"ecca3041-a7b5-4450-b1c3-43153aed3470","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel, BitsAndBytesConfig\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:06:53.909260Z","iopub.execute_input":"2025-03-06T18:06:53.909833Z","iopub.status.idle":"2025-03-06T18:07:07.782728Z","shell.execute_reply.started":"2025-03-06T18:06:53.909804Z","shell.execute_reply":"2025-03-06T18:07:07.781649Z"}},"outputs":[],"execution_count":11},{"id":"5bcf0026-58c9-4b14-a94c-74c310e767c7","cell_type":"code","source":"class LLadaForTokenClassification(nn.Module):\n    def __init__(\n        self,\n        pretrained_model_name,\n        num_labels,\n        quantization_config=None,\n        device_map=None,\n        id2label=None,\n        label2id=None\n    ):\n        super().__init__()\n        # Load the base LLada model with optional quantization configuration.\n        self.llada = AutoModel.from_pretrained(\n            pretrained_model_name,\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            quantization_config=quantization_config,\n            device_map=device_map\n        )\n        # Expose the configuration and update it with label mappings.\n        self.config = self.llada.config\n        if id2label is not None:\n            self.config.id2label = id2label\n        if label2id is not None:\n            self.config.label2id = label2id\n        self.config.num_labels = num_labels\n        \n        hidden_size = self.config.hidden_size\n        # Add a linear classification head.\n        self.classifier = nn.Linear(hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n\n        kwargs.pop(\"output_hidden_states\", None)\n        kwargs.pop(\"return_dict\", None)\n    \n        outputs = self.llada(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            output_hidden_states=True, \n            return_dict=True,\n            **kwargs\n        )\n        # Use the last hidden state from the tuple of hidden states.\n        sequence_output = outputs.hidden_states[-1]\n        logits = self.classifier(sequence_output)\n        \n        loss = None\n        if labels is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return {'loss': loss, 'logits': logits}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:07:07.783697Z","iopub.execute_input":"2025-03-06T18:07:07.784302Z","iopub.status.idle":"2025-03-06T18:07:07.791292Z","shell.execute_reply.started":"2025-03-06T18:07:07.784276Z","shell.execute_reply":"2025-03-06T18:07:07.790231Z"}},"outputs":[],"execution_count":12},{"id":"306567fb-c428-4ef1-834c-3b5f15f43e8d","cell_type":"code","source":"# Define the quantization configuration (using 4-bit nf4 as in your Gemma2 example).\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Model and label definitions.\nPRETRAINED_MODEL = 'GSAI-ML/LLaDA-8B-Base'\nnum_labels = 2  # 0: non-trigger, 1: trigger\n\nid2label = {0: 0, 1: 1}\nlabel2id = {0: 0, 1: 1}\n\nmodel = LLadaForTokenClassification(\n    pretrained_model_name=PRETRAINED_MODEL,\n    num_labels=2,\n    quantization_config=nf4_config,\n    device_map=\"cuda:0\",\n    id2label=id2label,\n    label2id=label2id\n)\n\n# Prepare the model for k-bit (quantized) training.\nmodel = prepare_model_for_kbit_training(model)\n\n# Define the LoRA configuration.\nlora_config = LoraConfig(\n    r=16,                   # Rank of the low-rank matrices\n    lora_alpha=16,          # Scaling factor for LoRA activations\n    lora_dropout=0.05,\n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.TOKEN_CLS,\n    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\"] #, \"gate_proj\", \"down_proj\", \"up_proj\"]\n)\n\n# Wrap the model with LoRA.\nmodel = get_peft_model(model, lora_config)\n\n# Print the trainable parameters for verification.\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:07:07.796876Z","iopub.execute_input":"2025-03-06T18:07:07.797151Z","iopub.status.idle":"2025-03-06T18:09:43.111226Z","shell.execute_reply.started":"2025-03-06T18:07:07.797126Z","shell.execute_reply":"2025-03-06T18:09:43.110323Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c62e3e77f4487890dc735cd0c2e524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_llada.py:   0%|          | 0.00/12.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d89f5d3d0374227a2b30e287012aa5f"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n- configuration_llada.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_llada.py:   0%|          | 0.00/60.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8ce8cca915461186f7b17ef035b084"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Base:\n- modeling_llada.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ab24dbf01b3496cae35a5bff7dd7b36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7406d163c418442eabb8a2161dce9002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcab9baa19994b388ffcf3eba376fe7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd5a8b00cf04e45b77f5ad892435448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d5c805558b4f99b99e87ce385c1a36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f43bb1094b2e4538b3332a668969139e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e071e6729e5a4b5e9608abd2b87f1b4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3958c5f723349aa9d41623e69b36d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00e5a6c7f5941ecaf629662d1d90b49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/128 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"186e3b5d9d7c47b88fb00d6bd79e7546"}},"metadata":{}},{"name":"stdout","text":"trainable params: 12,591,106 || all params: 8,028,180,484 || trainable%: 0.1568\n","output_type":"stream"}],"execution_count":13},{"id":"505482f7-21b0-415f-bb79-cd882cd053d9","cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:09:43.113163Z","iopub.execute_input":"2025-03-06T18:09:43.113465Z","iopub.status.idle":"2025-03-06T18:09:43.125318Z","shell.execute_reply.started":"2025-03-06T18:09:43.113439Z","shell.execute_reply":"2025-03-06T18:09:43.124299Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModelForTokenClassification(\n  (base_model): LoraModel(\n    (model): LLadaForTokenClassification(\n      (llada): LLaDAModelLM(\n        (model): LLaDAModel(\n          (transformer): ModuleDict(\n            (wte): Embedding(126464, 4096)\n            (emb_drop): Dropout(p=0.0, inplace=False)\n            (ln_f): RMSLayerNorm()\n            (blocks): ModuleList(\n              (0-31): 32 x LLaDALlamaBlock(\n                (dropout): Dropout(p=0.0, inplace=False)\n                (act): SiLU()\n                (attn_out): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (ff_out): Linear4bit(in_features=12288, out_features=4096, bias=False)\n                (rotary_emb): RotaryEmbedding()\n                (attn_norm): RMSLayerNorm()\n                (ff_norm): RMSLayerNorm()\n                (q_proj): lora.Linear(\n                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (ff_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n                (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n              )\n            )\n            (ff_out): Linear(in_features=4096, out_features=126464, bias=False)\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=4096, out_features=2, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=4096, out_features=2, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":14},{"id":"35fcc28d-281d-4a1f-931e-a88378bc6286","cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:09:43.126345Z","iopub.execute_input":"2025-03-06T18:09:43.126723Z","iopub.status.idle":"2025-03-06T18:09:43.150401Z","shell.execute_reply.started":"2025-03-06T18:09:43.126687Z","shell.execute_reply":"2025-03-06T18:09:43.149371Z"}},"outputs":[],"execution_count":15},{"id":"7e3b6fad-eacf-427f-b17b-ff9a4224e755","cell_type":"code","source":"import math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom transformers.trainer_utils import EvalPrediction\n\n\ntrain_args = TrainingArguments(\n    output_dir='model_checkpoints_llada-8b-seq-trunc',\n    logging_dir='./model_logs_llada-8b-seq-trunc',\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.0,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,#2\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,#4\n    # bf16=True,\n    fp16=True,\n    report_to=\"wandb\",\n    optim='adamw_8bit',\n    eval_strategy='steps',\n    save_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=20,\n    save_steps=200,\n    save_total_limit=10,\n    metric_for_best_model='eval_f1',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:09:43.151413Z","iopub.execute_input":"2025-03-06T18:09:43.151675Z","iopub.status.idle":"2025-03-06T18:09:44.728366Z","shell.execute_reply.started":"2025-03-06T18:09:43.151652Z","shell.execute_reply":"2025-03-06T18:09:44.727302Z"}},"outputs":[],"execution_count":16},{"id":"006bb1d7-e8cf-4b56-8323-14b40552494e","cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_1)\n\n# Initialize with team/entity\nwandb.init(\n    project=\"unlp-span-ident-task\",\n    entity=\"IASA-BA-Diploma-Ivan-Bashtovyi\", \n    name='llada-8b-seq-1024-r32-alpha16',\n    settings=wandb.Settings(init_timeout=180)  # Increase timeout\n)\n\nimport os\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:09:44.729364Z","iopub.execute_input":"2025-03-06T18:09:44.729634Z","iopub.status.idle":"2025-03-06T18:09:59.982137Z","shell.execute_reply.started":"2025-03-06T18:09:44.729610Z","shell.execute_reply":"2025-03-06T18:09:59.981161Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivanbashtovyi1\u001b[0m (\u001b[33mIASA-BA-Diploma-Ivan-Bashtovyi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250306_180953-um35zp85</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/um35zp85' target=\"_blank\">llada-8b-seq-1024-r32-alpha16</a></strong> to <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/um35zp85' target=\"_blank\">https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/um35zp85</a>"},"metadata":{}}],"execution_count":17},{"id":"dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5","cell_type":"code","source":"from itertools import chain\n\npositive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\npositive_class_balance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:09:59.983086Z","iopub.execute_input":"2025-03-06T18:09:59.983427Z","iopub.status.idle":"2025-03-06T18:10:00.365980Z","shell.execute_reply.started":"2025-03-06T18:09:59.983389Z","shell.execute_reply":"2025-03-06T18:10:00.365149Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.2483721209153103"},"metadata":{}}],"execution_count":18},{"id":"d7abe3ad-0619-40d3-90cc-70fd52617d8b","cell_type":"code","source":"import math\nfrom transformers import Trainer, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\nclass SpanEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        desired_positive_ratio: float = 0.25,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.desired_positive_ratio = desired_positive_ratio\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.desired_positive_ratio)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th\n        \n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n        eval_dataset = self.eval_dataset\n        logits, labels = eval_pred\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:00.366762Z","iopub.execute_input":"2025-03-06T18:10:00.366982Z","iopub.status.idle":"2025-03-06T18:10:00.383343Z","shell.execute_reply.started":"2025-03-06T18:10:00.366962Z","shell.execute_reply":"2025-03-06T18:10:00.382415Z"}},"outputs":[],"execution_count":19},{"id":"2463e605-3f7f-4456-a648-fa64d0f0acb5","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:00.384168Z","iopub.execute_input":"2025-03-06T18:10:00.384449Z","iopub.status.idle":"2025-03-06T18:10:00.418840Z","shell.execute_reply.started":"2025-03-06T18:10:00.384417Z","shell.execute_reply":"2025-03-06T18:10:00.417967Z"}},"outputs":[],"execution_count":20},{"id":"e2e33e84-5eac-48fe-948e-f53acd597002","cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = SpanEvaluationTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    desired_positive_ratio=positive_class_balance\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:00.419806Z","iopub.execute_input":"2025-03-06T18:10:00.420117Z","iopub.status.idle":"2025-03-06T18:10:04.423297Z","shell.execute_reply.started":"2025-03-06T18:10:00.420084Z","shell.execute_reply":"2025-03-06T18:10:04.421721Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-19-fd5c10b25dba>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-813b0de7c609>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdesired_positive_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_class_balance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_accepts_loss_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3655\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3707\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   2315\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-45d67cba96ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         outputs = self.llada(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/GSAI-ML/LLaDA-8B-Base/ce71e3c2523f535e022bccedbda192eb8869fd44/modeling_llada.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         outputs = self.model.forward(\n\u001b[0m\u001b[1;32m   1419\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0minput_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/GSAI-ML/LLaDA-8B-Base/ce71e3c2523f535e022bccedbda192eb8869fd44/modeling_llada.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, past_key_values, use_cache, last_logits_only, output_hidden_states)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 988.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 355.12 MiB is free. Process 3052 has 15.54 GiB memory in use. Of the allocated memory 14.87 GiB is allocated by PyTorch, and 391.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 988.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 355.12 MiB is free. Process 3052 has 15.54 GiB memory in use. Of the allocated memory 14.87 GiB is allocated by PyTorch, and 391.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":21},{"id":"64d34282-f37f-4616-b7b9-8430f4bd970d","cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.424034Z","iopub.status.idle":"2025-03-06T18:10:04.424379Z","shell.execute_reply":"2025-03-06T18:10:04.424253Z"}},"outputs":[],"execution_count":null},{"id":"a1e54265-24e4-4445-abf5-1fd43614e6fb","cell_type":"code","source":"import torch\nfrom transformers import Gemma2ForTokenClassification, BitsAndBytesConfig\nfrom peft import PeftModel, get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n\n\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=False,\n   bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = LLadaForTokenClassification(\n    pretrained_model_name=PRETRAINED_MODEL,\n    num_labels=2,\n    quantization_config=nf4_config,\n    device_map=\"cuda:0\",\n    id2label=id2label,\n    label2id=label2id\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\nmodel = PeftModel.from_pretrained(model, \"./model_checkpoints_llada-8b-seq-trunc/checkpoint-600/\")\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.425264Z","iopub.status.idle":"2025-03-06T18:10:04.425672Z","shell.execute_reply":"2025-03-06T18:10:04.425481Z"}},"outputs":[],"execution_count":null},{"id":"d3913ada-b31b-404f-9745-7babf156ec94","cell_type":"code","source":"trainer.model = model.cuda().eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.426455Z","iopub.status.idle":"2025-03-06T18:10:04.426881Z","shell.execute_reply":"2025-03-06T18:10:04.426697Z"}},"outputs":[],"execution_count":null},{"id":"4800edb9-d35d-4289-ad21-b0672254edd2","cell_type":"code","source":"valid_preds = trainer.predict(ds_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.427545Z","iopub.status.idle":"2025-03-06T18:10:04.427975Z","shell.execute_reply":"2025-03-06T18:10:04.427799Z"}},"outputs":[],"execution_count":null},{"id":"92bd64bc-afd9-4a8a-8de4-514824f6d4a1","cell_type":"code","source":"valid_preds.predictions.shape, valid_preds.label_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.429132Z","iopub.status.idle":"2025-03-06T18:10:04.429570Z","shell.execute_reply":"2025-03-06T18:10:04.429386Z"}},"outputs":[],"execution_count":null},{"id":"fc734f81-5211-4312-b248-f1da909f69d6","cell_type":"code","source":"trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.430465Z","iopub.status.idle":"2025-03-06T18:10:04.430916Z","shell.execute_reply":"2025-03-06T18:10:04.430720Z"}},"outputs":[],"execution_count":null},{"id":"dfdd22ce-5c37-41b6-bf6d-ab1e50791687","cell_type":"code","source":"test_preds = trainer.predict(ds_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.431691Z","iopub.status.idle":"2025-03-06T18:10:04.432143Z","shell.execute_reply":"2025-03-06T18:10:04.431953Z"}},"outputs":[],"execution_count":null},{"id":"23204c4d-70aa-4b44-9edd-a62794e4be1b","cell_type":"code","source":"test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.433265Z","iopub.status.idle":"2025-03-06T18:10:04.433733Z","shell.execute_reply":"2025-03-06T18:10:04.433520Z"}},"outputs":[],"execution_count":null},{"id":"e950513f-2a4d-4e6b-ac59-6e27836023a3","cell_type":"code","source":"trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.434502Z","iopub.status.idle":"2025-03-06T18:10:04.434949Z","shell.execute_reply":"2025-03-06T18:10:04.434748Z"}},"outputs":[],"execution_count":null},{"id":"d9bec8dd-19c1-441b-b467-aa0238dc1af5","cell_type":"code","source":"final_th = (0.405959+0.366363)/2\nfinal_th","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.435854Z","iopub.status.idle":"2025-03-06T18:10:04.436307Z","shell.execute_reply":"2025-03-06T18:10:04.436090Z"}},"outputs":[],"execution_count":null},{"id":"042dc4ff-378c-46ef-bbe4-7613de11f19c","cell_type":"code","source":"def inference_aggregation(probabilities, labels, offset_mappings, thold):\n    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n    ]\n    pred_spans_all = []\n    for pred, offsets in zip(true_predictions, offset_mappings):\n        samplewise_spans = []\n        current_span = None\n        for token_label, span in zip(pred, offsets):\n            if token_label == 1:  # If the current token is labeled as an entity (1)\n                if current_span is None:\n                    current_span = [span[0], span[1]]  # Start a new span\n                else:\n                    current_span[1] = span[1]  # Extend the span to include the current token\n            else:  # If token_label == 0 (not an entity)\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))  # Save completed span\n                    current_span = None  # Reset for the next entity\n        \n                    # If the last token was part of a span, save it\n        if current_span is not None:\n            samplewise_spans.append(tuple(current_span))\n        \n        pred_spans_all.append(samplewise_spans)\n    return [str(row) for row in pred_spans_all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.437100Z","iopub.status.idle":"2025-03-06T18:10:04.437458Z","shell.execute_reply":"2025-03-06T18:10:04.437338Z"}},"outputs":[],"execution_count":null},{"id":"537572ee-be33-4911-bc82-8abd6b9d2823","cell_type":"code","source":"valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\nvalid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.438225Z","iopub.status.idle":"2025-03-06T18:10:04.438636Z","shell.execute_reply":"2025-03-06T18:10:04.438456Z"}},"outputs":[],"execution_count":null},{"id":"bdc10345-650a-4e78-8e7b-2f923faefe8b","cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.439579Z","iopub.status.idle":"2025-03-06T18:10:04.440011Z","shell.execute_reply":"2025-03-06T18:10:04.439833Z"}},"outputs":[],"execution_count":null},{"id":"ffbb019a-659b-4bd8-a84f-b1c3ceb5698f","cell_type":"code","source":"from copy import deepcopy\n\ndf_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\ndf_pred = deepcopy(df_gt)\ndf_pred['trigger_words'] = valid_results\nscore(df_gt, df_pred, row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.441038Z","iopub.status.idle":"2025-03-06T18:10:04.441489Z","shell.execute_reply":"2025-03-06T18:10:04.441298Z"}},"outputs":[],"execution_count":null},{"id":"b4a952c8-9492-4b3a-819b-a061f03657d4","cell_type":"code","source":"test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.442516Z","iopub.status.idle":"2025-03-06T18:10:04.442938Z","shell.execute_reply":"2025-03-06T18:10:04.442740Z"}},"outputs":[],"execution_count":null},{"id":"e2f59c7e-a282-4609-921e-b6879978818f","cell_type":"code","source":"ss = pd.read_csv(\"/kaggle/input/unlp-2025-shared-task-span-identification/sample_submission.csv\")#'sample_submission.csv')\nss['trigger_words'] = test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.443729Z","iopub.status.idle":"2025-03-06T18:10:04.444170Z","shell.execute_reply":"2025-03-06T18:10:04.443977Z"}},"outputs":[],"execution_count":null},{"id":"5e832f9a-ef93-4400-9b67-5bf843a90205","cell_type":"code","source":"ss.to_csv('unsloth-full-seq-gemma2-2b-binary-cv0.593.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.445120Z","iopub.status.idle":"2025-03-06T18:10:04.445591Z","shell.execute_reply":"2025-03-06T18:10:04.445393Z"}},"outputs":[],"execution_count":null},{"id":"43edf7bc-5a95-4174-984b-4cfabce5252d","cell_type":"code","source":"import pickle\n\npickle.dump(valid_preds, open('valid_preds_gemma2_binary.pkl', 'wb'))\npickle.dump(test_preds, open('test_preds_gemma2_binary.pkl', 'wb'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:10:04.446696Z","iopub.status.idle":"2025-03-06T18:10:04.447158Z","shell.execute_reply":"2025-03-06T18:10:04.446957Z"}},"outputs":[],"execution_count":null},{"id":"dc896de7-aa34-430b-82d4-d6b103cb720d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}