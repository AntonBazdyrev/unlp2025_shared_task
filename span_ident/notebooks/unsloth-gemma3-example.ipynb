{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10664686,"sourceType":"datasetVersion","datasetId":6604871}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d60b7330-f0f2-418f-a6d8-055ca6e82d53","cell_type":"code","source":"!pip install -U -q bitsandbytes accelerate peft git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:54:34.460195Z","iopub.execute_input":"2025-03-17T19:54:34.460524Z","iopub.status.idle":"2025-03-17T19:55:10.899656Z","shell.execute_reply.started":"2025-03-17T19:54:34.460471Z","shell.execute_reply":"2025-03-17T19:55:10.898553Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"id":"2a31f4b5-454e-4fad-94fe-2612578f11de","cell_type":"code","source":"import os\nimport copy\nimport random\nfrom dataclasses import dataclass\nfrom tqdm.autonotebook import tqdm\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    Gemma2ForTokenClassification,\n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:10.900691Z","iopub.execute_input":"2025-03-17T19:55:10.900960Z","iopub.status.idle":"2025-03-17T19:55:32.949756Z","shell.execute_reply.started":"2025-03-17T19:55:10.900939Z","shell.execute_reply":"2025-03-17T19:55:32.948877Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-2242a67acc80>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}],"execution_count":2},{"id":"192f7edd-5297-49cc-aec3-c3e94791eaff","cell_type":"markdown","source":"# Configurations","metadata":{}},{"id":"84cf43aa-6e13-4b33-8940-2eb5009bb598","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n@dataclass\nclass Config:\n    data_path: str = '/kaggle/input/unlp-2025-shared-task-span-identification/'\n    cv_path: str = \"/kaggle/input/unlp25-cross-validation-split/cv_split.csv\"\n    \n    # pretrained: str = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n    pretrained: str = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\"\n    max_length: int = 512\n\n    hugginface_key: str = user_secrets.get_secret(\"hugginface_key\")\n    wandb_key: str = user_secrets.get_secret(\"wandb_key\")\n    wandb_init_args = {\n        'project': \"unlp-span-ident-task\",\n        'entity': \"ivan-havlytskyiz\",\n        'name': \"unsloth-gemma-3-4b-it-unsloth-bnb-4bit\"\n    }\n\n    lora_args = {\n        'r': 16,\n        'bias': \"none\",\n        'lora_alpha': 32,\n        'lora_dropout': 0.05,\n        # 'layers_to_transform': list(range(16, 42))\n    }\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:32.950634Z","iopub.execute_input":"2025-03-17T19:55:32.951228Z","iopub.status.idle":"2025-03-17T19:55:33.265740Z","shell.execute_reply.started":"2025-03-17T19:55:32.951198Z","shell.execute_reply":"2025-03-17T19:55:33.265090Z"}},"outputs":[],"execution_count":3},{"id":"3dc1afcf-b343-4038-b61e-aef2384e86e8","cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)\ntqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:33.266503Z","iopub.execute_input":"2025-03-17T19:55:33.266717Z","iopub.status.idle":"2025-03-17T19:55:33.339583Z","shell.execute_reply.started":"2025-03-17T19:55:33.266698Z","shell.execute_reply":"2025-03-17T19:55:33.338949Z"}},"outputs":[],"execution_count":4},{"id":"27b6b2e2-24af-4985-b18e-de7202fed5d9","cell_type":"code","source":"import huggingface_hub\nimport wandb\n\nhuggingface_hub.login(token=config.hugginface_key)\nwandb.login(key=config.wandb_key)\n\nwandb.init(**config.wandb_init_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:33.340185Z","iopub.execute_input":"2025-03-17T19:55:33.340376Z","iopub.status.idle":"2025-03-17T19:55:47.879211Z","shell.execute_reply.started":"2025-03-17T19:55:33.340359Z","shell.execute_reply":"2025-03-17T19:55:47.878515Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-havlytskyi\u001b[0m (\u001b[33mivan-havlytskyiz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250317_195541-w6mrzkou</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/w6mrzkou' target=\"_blank\">unsloth-gemma-3-4b-it-unsloth-bnb-4bit</a></strong> to <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/w6mrzkou' target=\"_blank\">https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/w6mrzkou</a>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/w6mrzkou?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7de4037e53c0>"},"metadata":{}}],"execution_count":5},{"id":"e9b0a9ab-1388-47e8-b169-0440b14bdce4","cell_type":"markdown","source":"# Training Arguments","metadata":{}},{"id":"5a5d1f7a-52b6-427a-89b5-c363d857db62","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'./model_checkpoints_{config.wandb_init_args[\"name\"]}',\n    logging_dir=f'./model_logs_{config.wandb_init_args[\"name\"]}',\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.0,\n    num_train_epochs=5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    # bf16=True,\n    report_to=\"wandb\",\n    optim='adamw_8bit',\n    eval_strategy='steps',\n    save_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=20,\n    save_steps=200,\n    save_total_limit=10,\n    metric_for_best_model='eval_f1',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:47.882534Z","iopub.execute_input":"2025-03-17T19:55:47.882722Z","iopub.status.idle":"2025-03-17T19:55:47.936730Z","shell.execute_reply.started":"2025-03-17T19:55:47.882705Z","shell.execute_reply":"2025-03-17T19:55:47.936128Z"}},"outputs":[],"execution_count":6},{"id":"11588d69-5334-4c5b-a1a7-d460ba57bd79","cell_type":"markdown","source":"# LoRA config","metadata":{}},{"id":"79735da9-8cef-42ba-adcf-c715af343db6","cell_type":"code","source":"lora_config = LoraConfig(\n    **config.lora_args,\n    # only target self-attention\n    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\"],\n    # target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\"], #\"gate_proj\", \"down_proj\", \"up_proj\"]\n    task_type=TaskType.TOKEN_CLS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:47.937443Z","iopub.execute_input":"2025-03-17T19:55:47.937653Z","iopub.status.idle":"2025-03-17T19:55:47.941736Z","shell.execute_reply.started":"2025-03-17T19:55:47.937633Z","shell.execute_reply":"2025-03-17T19:55:47.941004Z"}},"outputs":[],"execution_count":7},{"id":"e3c6e19e-3769-434e-845b-d364b48915c1","cell_type":"markdown","source":"# Instantiate the tokenizer & model","metadata":{}},{"id":"409ea5fb-ceee-4dde-8cb6-2c82fb91e81d","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.pretrained)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:47.942675Z","iopub.execute_input":"2025-03-17T19:55:47.942946Z","iopub.status.idle":"2025-03-17T19:55:51.757187Z","shell.execute_reply.started":"2025-03-17T19:55:47.942913Z","shell.execute_reply":"2025-03-17T19:55:51.756272Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60faf6e962e243d2b798d6fcee39db5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939fcc37a3ab41d59d3ac6d891df814b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45345f20db41480ca0a7a880609d568e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24bd06aad8a40b192c07e6e4d39eb0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8786c77d12b74f5a93d9d1bfd64198ee"}},"metadata":{}}],"execution_count":8},{"id":"44eacb41-3e7f-410f-a36b-e67a3410b545","cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom typing import Optional, List, Tuple, Union\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.gemma3 import Gemma3PreTrainedModel, Gemma3ForConditionalGeneration\n\nclass Gemma3ForTokenClassification(Gemma3PreTrainedModel):\n    def __init__(self, model_name, num_labels):\n        gemma_model = Gemma3ForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"cuda\",\n        )\n        super().__init__(gemma_model.language_model.model.config)\n        \n        self.gemma_model = gemma_model.language_model.model\n        hidden_size = self.gemma_model.config.hidden_size\n\n        self.dropout = nn.Dropout(0.1)\n        self.ner_head = nn.Linear(hidden_size, num_labels)\n\n        del gemma_model\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.gemma_model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.ner_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_function(logits, labels, self.config)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            # hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:55:57.285329Z","iopub.execute_input":"2025-03-17T19:55:57.285681Z","iopub.status.idle":"2025-03-17T19:55:57.309967Z","shell.execute_reply.started":"2025-03-17T19:55:57.285656Z","shell.execute_reply":"2025-03-17T19:55:57.309099Z"}},"outputs":[],"execution_count":9},{"id":"86c2a72a-5f50-44fa-916e-d5fd280038e2","cell_type":"code","source":"model = Gemma3ForTokenClassification(\n    config.pretrained,\n    num_labels=2\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:56:09.951540Z","iopub.execute_input":"2025-03-17T19:56:09.951865Z","iopub.status.idle":"2025-03-17T19:56:42.603362Z","shell.execute_reply.started":"2025-03-17T19:56:09.951838Z","shell.execute_reply":"2025-03-17T19:56:42.602604Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/5.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e016b7ee5d843cf964cb57a2964a4e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3860cfa5cc347228c765af56597754c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7ce302e83b4581a9097023dbcf7e29"}},"metadata":{}},{"name":"stdout","text":"trainable params: 15,876,096 || all params: 3,896,144,386 || trainable%: 0.4075\n","output_type":"stream"}],"execution_count":10},{"id":"c14e2e83-495a-43c1-a08a-964815fd0a78","cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters())\npytorch_total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:56:46.188303Z","iopub.execute_input":"2025-03-17T19:56:46.188662Z","iopub.status.idle":"2025-03-17T19:56:46.199893Z","shell.execute_reply.started":"2025-03-17T19:56:46.188633Z","shell.execute_reply":"2025-03-17T19:56:46.199043Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"2496295426"},"metadata":{}}],"execution_count":11},{"id":"5b717a13-bafe-49a1-a8dd-2ec3604860d9","cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\npytorch_total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:56:47.338991Z","iopub.execute_input":"2025-03-17T19:56:47.339282Z","iopub.status.idle":"2025-03-17T19:56:47.350836Z","shell.execute_reply.started":"2025-03-17T19:56:47.339260Z","shell.execute_reply":"2025-03-17T19:56:47.349985Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"15876096"},"metadata":{}}],"execution_count":12},{"id":"20da2803-1e64-4cb0-a943-6ec021d30e80","cell_type":"code","source":"print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:56:59.800570Z","iopub.execute_input":"2025-03-17T19:56:59.800888Z","iopub.status.idle":"2025-03-17T19:56:59.806748Z","shell.execute_reply.started":"2025-03-17T19:56:59.800864Z","shell.execute_reply":"2025-03-17T19:56:59.805916Z"}},"outputs":[{"name":"stdout","text":"torch.cuda.memory_allocated: 5.448019GB\n","output_type":"stream"}],"execution_count":13},{"id":"d44ba491-f4c9-40f4-84c8-0f2fec080ce9","cell_type":"markdown","source":"# Data","metadata":{}},{"id":"01d1c0dd-e022-4b0a-ae48-1bf4915e3e03","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(config.data_path + \"train.parquet\")\ncv = pd.read_csv(config.cv_path)\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(config.data_path + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:11.712853Z","iopub.execute_input":"2025-03-17T19:57:11.713183Z","iopub.status.idle":"2025-03-17T19:57:12.084600Z","shell.execute_reply.started":"2025-03-17T19:57:11.713154Z","shell.execute_reply":"2025-03-17T19:57:12.083671Z"},"trusted":true},"outputs":[],"execution_count":14},{"id":"020ee767-721c-4ae4-867d-f880cb02d398","cell_type":"code","source":"def convert_to_seq_labeling(text, tokenizer, max_length=None, trigger_spans=None):\n    tokenized_output = tokenizer(\n        text,\n        return_offsets_mapping=True,\n        add_special_tokens=True,\n        \n        max_length=max_length,\n        truncation=(max_length is not None),\n        padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n\n    # Get subword tokenized versions of the text\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    \n    # Initialize labels as 'O'\n    labels = [0] * len(tokens)\n\n    if trigger_spans is not None:\n        # Assign 'TRIGGER' to overlapping tokens\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n                    labels[i] = 1\n\n    tokenized_output['labels'] = labels\n    return tokenized_output\n\n\ndef preprocess_df(df, max_length):\n    \"\"\"Modified processing incorporating trigger span handling\"\"\"\n    tqdm.pandas()\n    \n    df['seq_labels'] = df.progress_apply(\n        lambda row: convert_to_seq_labeling(\n            text=row['content'],\n            tokenizer=tokenizer,\n            trigger_spans=row.get('trigger_words', None),  # Handle both validation and test cases\n            max_length=max_length\n        ),\n        axis=1\n    )\n    \n    # Extract all tokenizer outputs\n    for column in df.seq_labels.iloc[0].keys():\n        df[column] = df.seq_labels.apply(lambda x: x.get(column))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:14.315440Z","iopub.execute_input":"2025-03-17T19:57:14.315781Z","iopub.status.idle":"2025-03-17T19:57:14.323451Z","shell.execute_reply.started":"2025-03-17T19:57:14.315750Z","shell.execute_reply":"2025-03-17T19:57:14.322614Z"},"trusted":true},"outputs":[],"execution_count":15},{"id":"c668d782-c0ef-42ae-a366-c6a2fe032d25","cell_type":"code","source":"df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)\n\nis_valid_mask = (df.fold == 4)\ndf_train = df[~is_valid_mask].copy()\ndf_valid = df[is_valid_mask].copy()\n\n\ndf_train = preprocess_df(df_train, max_length=512)\ndf_valid = preprocess_df(df_valid, max_length=None)\ndf_test = preprocess_df(df_test, max_length=None)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:16.361219Z","iopub.execute_input":"2025-03-17T19:57:16.361559Z","iopub.status.idle":"2025-03-17T19:57:23.719153Z","shell.execute_reply.started":"2025-03-17T19:57:16.361531Z","shell.execute_reply":"2025-03-17T19:57:23.718200Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3058 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b06138e9fb4c8684e098d3b358559f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/764 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f1f4850c3c431d9702cf0152186590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"254f6c7b11944619bcabda6931c4791e"}},"metadata":{}}],"execution_count":16},{"id":"3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f","cell_type":"code","source":"train_columns = list(df_train.seq_labels.iloc[0].keys()) +\\\n                ['content', 'trigger_words']\ntest_columns = list(df_train.seq_labels.iloc[0].keys()) + ['content']\n\nds_train = Dataset.from_pandas(df_train[train_columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(df_valid[train_columns].reset_index(drop=True))\nds_test = Dataset.from_pandas(df_test[test_columns].reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:25.583708Z","iopub.execute_input":"2025-03-17T19:57:25.584023Z","iopub.status.idle":"2025-03-17T19:57:27.298550Z","shell.execute_reply.started":"2025-03-17T19:57:25.584000Z","shell.execute_reply":"2025-03-17T19:57:27.297866Z"},"trusted":true},"outputs":[],"execution_count":17},{"id":"9a72c32e-e752-4c16-be89-cb06bfbd1dd3","cell_type":"markdown","source":"# Custom Trainer","metadata":{}},{"id":"dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5","cell_type":"code","source":"from itertools import chain\n\ntrain_labels = df_train.labels.tolist() + df_valid.labels.tolist()\npositive_class_balance = pd.Series(list(chain(*train_labels))).mean()\npositive_class_balance","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:37.320958Z","iopub.execute_input":"2025-03-17T19:57:37.321274Z","iopub.status.idle":"2025-03-17T19:57:37.494771Z","shell.execute_reply.started":"2025-03-17T19:57:37.321245Z","shell.execute_reply":"2025-03-17T19:57:37.493944Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.2324692813832274"},"metadata":{}}],"execution_count":18},{"id":"d7abe3ad-0619-40d3-90cc-70fd52617d8b","cell_type":"code","source":"import math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\nclass SpanEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        desired_positive_ratio: float = 0.25,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.desired_positive_ratio = desired_positive_ratio\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.desired_positive_ratio)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th\n        \n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n        eval_dataset = self.eval_dataset\n        logits, labels = eval_pred\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:41.229253Z","iopub.execute_input":"2025-03-17T19:57:41.229620Z","iopub.status.idle":"2025-03-17T19:57:41.366740Z","shell.execute_reply.started":"2025-03-17T19:57:41.229588Z","shell.execute_reply":"2025-03-17T19:57:41.366014Z"},"trusted":true},"outputs":[],"execution_count":19},{"id":"e2e33e84-5eac-48fe-948e-f53acd597002","cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = SpanEvaluationTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    desired_positive_ratio=positive_class_balance\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:57:45.854675Z","iopub.execute_input":"2025-03-17T19:57:45.854979Z","iopub.status.idle":"2025-03-17T19:57:45.888201Z","shell.execute_reply.started":"2025-03-17T19:57:45.854956Z","shell.execute_reply":"2025-03-17T19:57:45.887317Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-19-1175221e7f1c>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":20},{"id":"9835e231-e15e-4299-b4b1-b47d464db8c0","cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:58:11.331936Z","iopub.execute_input":"2025-03-17T19:58:11.332223Z","iopub.status.idle":"2025-03-17T20:25:32.706756Z","shell.execute_reply.started":"2025-03-17T19:58:11.332201Z","shell.execute_reply":"2025-03-17T20:25:32.705520Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='215' max='1910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 215/1910 27:10 < 3:36:16, 0.13 it/s, Epoch 0.56/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Thold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.507200</td>\n      <td>0.516125</td>\n      <td>0.530492</td>\n      <td>0.524388</td>\n      <td>0.527422</td>\n      <td>0.237677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fecc6099d6644e2ea3d542c12fe0b535"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2248\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2250\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2251\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m                     )\n\u001b[1;32m   2560\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3751\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":21},{"id":"b9e4dbe9-e9bb-45cf-b13c-bb19bff01d58","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"a5ed6002-164d-421a-8278-c049501a684c","cell_type":"markdown","source":"# Predict","metadata":{}},{"id":"63083323-8398-4291-8f8a-f4cfa54dfe4c","cell_type":"code","source":"FINETUNED_MODEL = ''","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:45:43.239374Z","iopub.status.busy":"2025-02-27T20:45:43.239080Z","iopub.status.idle":"2025-02-27T20:45:43.243807Z","shell.execute_reply":"2025-02-27T20:45:43.242955Z","shell.execute_reply.started":"2025-02-27T20:45:43.239352Z"}},"outputs":[],"execution_count":67},{"id":"a5b811ad-7174-443e-8534-ec52139812fc","cell_type":"code","source":"trainer._load_from_checkpoint(FINETUNED_MODEL)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:45:43.894410Z","iopub.status.busy":"2025-02-27T20:45:43.894094Z","iopub.status.idle":"2025-02-27T20:45:43.976599Z","shell.execute_reply":"2025-02-27T20:45:43.975722Z","shell.execute_reply.started":"2025-02-27T20:45:43.894385Z"}},"outputs":[],"execution_count":68},{"id":"f5c56b18-9259-4fe4-b58b-d687f4c9c5c1","cell_type":"code","source":"valid_preds = trainer.predict(ds_valid)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:45:46.616863Z","iopub.status.busy":"2025-02-27T20:45:46.616480Z","iopub.status.idle":"2025-02-27T20:49:31.426042Z","shell.execute_reply":"2025-02-27T20:49:31.425337Z","shell.execute_reply.started":"2025-02-27T20:45:46.616833Z"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6642452c60204c5c8a4f89b7f25e3152","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":69},{"id":"bc10ce25-7d51-44f6-88ae-ae44b8a88128","cell_type":"code","source":"trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:49:31.427328Z","iopub.status.busy":"2025-02-27T20:49:31.427056Z","iopub.status.idle":"2025-02-27T20:49:49.972503Z","shell.execute_reply":"2025-02-27T20:49:49.971687Z","shell.execute_reply.started":"2025-02-27T20:49:31.427306Z"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d7812bd795a4a078afb1791a775c127","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'precision': 0.5896525335557485,\n"," 'recall': 0.58191198683027,\n"," 'f1': 0.5857566892310321,\n"," 'thold': 0.5841414141414141}"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"execution_count":70},{"id":"2f9febbe-afc8-4b6e-b8c5-2ecdf0980e74","cell_type":"code","source":"test_preds = trainer.predict(ds_test)\ntest_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()\n\ntrainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:49:49.974004Z","iopub.status.busy":"2025-02-27T20:49:49.973790Z","iopub.status.idle":"2025-02-27T21:22:38.771245Z","shell.execute_reply":"2025-02-27T21:22:38.770608Z","shell.execute_reply.started":"2025-02-27T20:49:49.973985Z"}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f79c54e041e6436a801c29d63a02425d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0.5544444444444444"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"execution_count":71},{"id":"dee0aa45-02a6-48e0-975d-5aea7d33ca07","cell_type":"code","source":"final_th = (0.4752+0.4257575)/2 - 0.15\nfinal_th","metadata":{"execution":{"iopub.execute_input":"2025-02-27T21:22:38.772673Z","iopub.status.busy":"2025-02-27T21:22:38.772295Z","iopub.status.idle":"2025-02-27T21:22:38.777892Z","shell.execute_reply":"2025-02-27T21:22:38.777105Z","shell.execute_reply.started":"2025-02-27T21:22:38.772577Z"}},"outputs":[{"data":{"text/plain":["0.30047875"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"execution_count":72},{"id":"4e31a1a1-7efa-467e-ab31-34969164856b","cell_type":"markdown","source":"## Metric","metadata":{}},{"id":"9a61b2a0-dd05-4581-b7e6-bd1b6b0a07f0","cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:28:44.678119Z","iopub.status.busy":"2025-02-27T20:28:44.677847Z","iopub.status.idle":"2025-02-27T20:28:44.687692Z","shell.execute_reply":"2025-02-27T20:28:44.686882Z","shell.execute_reply.started":"2025-02-27T20:28:44.678098Z"}},"outputs":[],"execution_count":45},{"id":"f8417a1b-b292-4b9b-b6d6-a986c5fb7089","cell_type":"markdown","source":"## Span level","metadata":{}},{"id":"2dcb4987-376b-4905-9b1f-7c217b164fdf","cell_type":"code","source":"def inference_aggregation(probabilities, labels, offset_mappings, thold):\n    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n    ]\n    pred_spans_all = []\n    for pred, offsets in zip(true_predictions, offset_mappings):\n        samplewise_spans = []\n        current_span = None\n        for token_label, span in zip(pred, offsets):\n            if token_label == 1:  # If the current token is labeled as an entity (1)\n                if current_span is None:\n                    current_span = [span[0], span[1]]  # Start a new span\n                else:\n                    current_span[1] = span[1]  # Extend the span to include the current token\n            else:  # If token_label == 0 (not an entity)\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))  # Save completed span\n                    current_span = None  # Reset for the next entity\n        \n                    # If the last token was part of a span, save it\n        if current_span is not None:\n            samplewise_spans.append(tuple(current_span))\n        \n        pred_spans_all.append(samplewise_spans)\n    return [str(row) for row in pred_spans_all]","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:28:47.304495Z","iopub.status.busy":"2025-02-27T20:28:47.304197Z","iopub.status.idle":"2025-02-27T20:28:47.311230Z","shell.execute_reply":"2025-02-27T20:28:47.310564Z","shell.execute_reply.started":"2025-02-27T20:28:47.304469Z"}},"outputs":[],"execution_count":46},{"id":"dda5c4cb-c27e-472e-80c5-116f9786b65b","cell_type":"code","source":"valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\nvalid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:28:47.557315Z","iopub.status.busy":"2025-02-27T20:28:47.557091Z","iopub.status.idle":"2025-02-27T20:28:48.614688Z","shell.execute_reply":"2025-02-27T20:28:48.613814Z","shell.execute_reply.started":"2025-02-27T20:28:47.557297Z"}},"outputs":[],"execution_count":47},{"id":"0a85dff8-7d21-464a-8f82-86b0c2627c7f","cell_type":"code","source":"from copy import deepcopy\n\ndf_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\ndf_pred = deepcopy(df_gt)\ndf_pred['trigger_words'] = valid_results\nscore(df_gt, df_pred, row_id_column_name='id')","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:28:48.616316Z","iopub.status.busy":"2025-02-27T20:28:48.616017Z","iopub.status.idle":"2025-02-27T20:28:48.711144Z","shell.execute_reply":"2025-02-27T20:28:48.710510Z","shell.execute_reply.started":"2025-02-27T20:28:48.616284Z"}},"outputs":[{"data":{"text/plain":["0.6120617208613024"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"execution_count":48},{"id":"94451835-b216-4ffd-b95c-02139747fd30","cell_type":"code","source":"# df_pred.to_csv(\"unsloth-full-seq-gemma2-2b-binary-cv0.612.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:45:12.920102Z","iopub.status.busy":"2025-02-27T20:45:12.919772Z","iopub.status.idle":"2025-02-27T20:45:12.928806Z","shell.execute_reply":"2025-02-27T20:45:12.927870Z","shell.execute_reply.started":"2025-02-27T20:45:12.920080Z"}},"outputs":[],"execution_count":66},{"id":"d42d55a4-01fb-42ff-81bf-c27849905067","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"e51e76f0-6884-4573-8a86-96e9be48df13","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"d6a8b969-cea9-4da0-a0b3-d919f48d4659","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"24d22f8a-b542-4ecd-87fb-b30c1d465e04","cell_type":"markdown","source":"## Char level","metadata":{}},{"id":"ea1a6079-4277-42a0-9b00-bdfaa624a19b","cell_type":"code","source":"def inference_to_char_level(probabilities, labels, offset_mappings, ids):\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(probabilities[:, :, 1], labels)\n    ]\n    \n    all_data = []\n    \n    for sample_id, pred, offsets, probs in zip(ids, true_predictions, offset_mappings, probabilities[:, :, 1]):\n        for token_label, span, proba in zip(pred, offsets, probs):\n            for char_index in range(span[0], span[1]):\n                all_data.append({\"id\": sample_id, \"char_index\": char_index, \"proba\": proba})\n\n    output_df = pd.DataFrame(all_data)\n\n    # something was wrong, got some duplicates, mb because of out of vocab tokens\n    output_df = output_df.groupby(['id', 'char_index'], as_index=False)['proba'].mean()\n\n    return output_df\n\n\ndef char_level_to_spans(df, thold):\n    spans_all = []\n    for sample_id, group in df.groupby(\"id\"):\n        spans = []\n        current_span = None\n        for _, row in group.iterrows():\n            if row[\"proba\"] >= thold:\n                if current_span is None:\n                    current_span = [row[\"char_index\"], row[\"char_index\"] + 1]\n                else:\n                    current_span[1] = row[\"char_index\"] + 1\n            else:\n                if current_span is not None:\n                    spans.append(tuple(current_span))\n                    current_span = None\n        if current_span is not None:\n            spans.append(tuple(current_span))\n        spans_all.append({\"id\": sample_id, \"trigger_words\": spans})\n    return pd.DataFrame(spans_all)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:29:38.590677Z","iopub.status.busy":"2025-02-27T20:29:38.590355Z","iopub.status.idle":"2025-02-27T20:29:38.598619Z","shell.execute_reply":"2025-02-27T20:29:38.597729Z","shell.execute_reply.started":"2025-02-27T20:29:38.590652Z"}},"outputs":[],"execution_count":50},{"id":"8dad26af-19d4-4e06-a049-a1973e1427f4","cell_type":"code","source":"ids = df.loc[df.fold==4, 'id'].values\nvalid_results_df = inference_to_char_level(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], ids)","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:29:39.343288Z","iopub.status.busy":"2025-02-27T20:29:39.343005Z","iopub.status.idle":"2025-02-27T20:29:40.602053Z","shell.execute_reply":"2025-02-27T20:29:40.601382Z","shell.execute_reply.started":"2025-02-27T20:29:39.343263Z"}},"outputs":[],"execution_count":51},{"id":"2f2017dd-2b9e-40b7-935c-b76bd7c93522","cell_type":"code","source":"score(df_gt, char_level_to_spans(valid_results_df, final_th), row_id_column_name='id')","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:31:47.326106Z","iopub.status.busy":"2025-02-27T20:31:47.325753Z","iopub.status.idle":"2025-02-27T20:32:04.750087Z","shell.execute_reply":"2025-02-27T20:32:04.749196Z","shell.execute_reply.started":"2025-02-27T20:31:47.326080Z"}},"outputs":[{"data":{"text/plain":["0.6120442270208827"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"execution_count":55},{"id":"b0707674-64bf-4874-ae3d-02555cc613c3","cell_type":"code","source":"valid_results_df.to_csv(f\"char_cv_preds_{config.wandb_init_args['name']}.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:34:42.214434Z","iopub.status.busy":"2025-02-27T20:34:42.214118Z","iopub.status.idle":"2025-02-27T20:34:43.220709Z","shell.execute_reply":"2025-02-27T20:34:43.220039Z","shell.execute_reply.started":"2025-02-27T20:34:42.214406Z"}},"outputs":[],"execution_count":58},{"id":"3fd739bd-66be-4780-98a7-4565f5bc3ebb","cell_type":"code","source":"ds_test","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:39:09.284247Z","iopub.status.busy":"2025-02-27T20:39:09.283954Z","iopub.status.idle":"2025-02-27T20:39:09.289976Z","shell.execute_reply":"2025-02-27T20:39:09.289072Z","shell.execute_reply.started":"2025-02-27T20:39:09.284227Z"}},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'attention_mask', 'offset_mapping', 'labels', 'content'],\n","    num_rows: 5735\n","})"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"execution_count":62},{"id":"8754e93b-567d-4011-9f9e-c7acc687d594","cell_type":"code","source":"ids = df_test[\"id\"]\n\ntest_char_results_df = inference_to_char_level(\n    test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], ids\n)\n\ntest_char_results_df.shape","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:39:20.004922Z","iopub.status.busy":"2025-02-27T20:39:20.004576Z","iopub.status.idle":"2025-02-27T20:39:33.267166Z","shell.execute_reply":"2025-02-27T20:39:33.266501Z","shell.execute_reply.started":"2025-02-27T20:39:20.004896Z"}},"outputs":[{"data":{"text/plain":["(3383472, 3)"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"execution_count":63},{"id":"fa24b598-1976-428d-b4dd-d8e5fe29d520","cell_type":"code","source":"test_char_results_df.to_csv(f\"char_test_preds_{config.wandb_init_args['name']}.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-02-27T20:41:31.793513Z","iopub.status.busy":"2025-02-27T20:41:31.793179Z","iopub.status.idle":"2025-02-27T20:41:39.538825Z","shell.execute_reply":"2025-02-27T20:41:39.538121Z","shell.execute_reply.started":"2025-02-27T20:41:31.793479Z"}},"outputs":[],"execution_count":64},{"id":"ce6b5317-2803-48e7-8360-f1a2263804ae","cell_type":"code","source":"output = tokenizer(\n        '🪃',\n        return_offsets_mapping=True,\n        add_special_tokens=True,\n        max_length=None,\n        padding=False\n    )\n\ntokenizer.convert_ids_to_tokens(output['input_ids'])","metadata":{"execution":{"iopub.execute_input":"2025-02-27T00:41:34.369681Z","iopub.status.busy":"2025-02-27T00:41:34.369112Z","iopub.status.idle":"2025-02-27T00:41:34.377863Z","shell.execute_reply":"2025-02-27T00:41:34.376997Z","shell.execute_reply.started":"2025-02-27T00:41:34.369631Z"}},"outputs":[{"data":{"text/plain":["['<bos>', '<0xF0>', '<0x9F>', '<0xAA>', '<0x83>', '<eos>']"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"execution_count":114}]}