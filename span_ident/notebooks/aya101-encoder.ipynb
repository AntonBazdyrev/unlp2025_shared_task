{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T01:33:51.885896Z",
     "iopub.status.busy": "2025-03-30T01:33:51.885569Z",
     "iopub.status.idle": "2025-03-30T01:33:58.760457Z",
     "shell.execute_reply": "2025-03-30T01:33:58.759616Z",
     "shell.execute_reply.started": "2025-03-30T01:33:51.885865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers bitsandbytes accelerate datasets peft scipy wandb scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"$HUGGINGFACE_TOKEN\"\n",
    "!wandb login \"$WANDB_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T01:33:58.762073Z",
     "iopub.status.busy": "2025-03-30T01:33:58.761746Z",
     "iopub.status.idle": "2025-03-30T01:34:24.766220Z",
     "shell.execute_reply": "2025-03-30T01:34:24.765480Z",
     "shell.execute_reply.started": "2025-03-30T01:33:58.762041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ef305aa39f81>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T01:34:24.768058Z",
     "iopub.status.busy": "2025-03-30T01:34:24.767462Z",
     "iopub.status.idle": "2025-03-30T01:34:25.133536Z",
     "shell.execute_reply": "2025-03-30T01:34:25.132857Z",
     "shell.execute_reply.started": "2025-03-30T01:34:24.768033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_path: str = '/kaggle/input/unlp-2025-shared-task-span-identification/'\n",
    "    cv_path: str = \"/kaggle/input/unlp25-cross-validation-split/cv_split.csv\"\n",
    "    \n",
    "    pretrained: str = \"CohereForAI/aya-101\"\n",
    "    max_length: int = 2048\n",
    "\n",
    "    hugginface_key: str = user_secrets.get_secret(\"hugginface_key\")\n",
    "    wandb_key: str = user_secrets.get_secret(\"wandb_key\")\n",
    "    wandb_init_args = {\n",
    "        'project': \"unlp-span-ident-task\",\n",
    "        'entity': \"ivan-havlytskyiz\",\n",
    "        'name': \"aya-101-encoder-a100\"\n",
    "    }\n",
    "\n",
    "    lora_args = {\n",
    "        'r': 16,\n",
    "        'bias': \"none\",\n",
    "        'lora_alpha': 32,\n",
    "        'lora_dropout': 0.05,\n",
    "        # 'layers_to_transform': list(range(16, 42))\n",
    "    }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T01:34:25.134629Z",
     "iopub.status.busy": "2025-03-30T01:34:25.134421Z",
     "iopub.status.idle": "2025-03-30T01:34:25.198725Z",
     "shell.execute_reply": "2025-03-30T01:34:25.197860Z",
     "shell.execute_reply.started": "2025-03-30T01:34:25.134611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T01:34:25.199943Z",
     "iopub.status.busy": "2025-03-30T01:34:25.199640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import wandb\n",
    "\n",
    "wandb.init(**config.wandb_init_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./model_checkpoints_{config.wandb_init_args[\"name\"]}',\n",
    "    logging_dir=f'./model_logs_{config.wandb_init_args[\"name\"]}',\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_8bit',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.164354Z",
     "iopub.status.idle": "2025-03-30T01:32:51.164631Z",
     "shell.execute_reply": "2025-03-30T01:32:51.164521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    **config.lora_args,\n",
    "    # only target self-attention\n",
    "    target_modules=['o', 'v', \"q\", \"k\", \"wi_0\"],\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.165748Z",
     "iopub.status.idle": "2025-03-30T01:32:51.166166Z",
     "shell.execute_reply": "2025-03-30T01:32:51.165995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.pretrained)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.168030Z",
     "iopub.status.idle": "2025-03-30T01:32:51.168325Z",
     "shell.execute_reply": "2025-03-30T01:32:51.168170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.t5.modeling_t5 import T5ForTokenClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model = T5ForTokenClassification.from_pretrained(\n",
    "    config.pretrained,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=nf4_config\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.169128Z",
     "iopub.status.idle": "2025-03-30T01:32:51.169455Z",
     "shell.execute_reply": "2025-03-30T01:32:51.169344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(config.data_path + \"train.parquet\")\n",
    "cv = pd.read_csv(config.cv_path)\n",
    "df = df.merge(cv, on='id', how='left')\n",
    "\n",
    "df_test = pd.read_csv(config.data_path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.169999Z",
     "iopub.status.idle": "2025-03-30T01:32:51.170237Z",
     "shell.execute_reply": "2025-03-30T01:32:51.170140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_seq_labeling(text, tokenizer, max_length=None, trigger_spans=None):\n",
    "    tokenized_output = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=True,\n",
    "        \n",
    "        max_length=max_length,\n",
    "        truncation=(max_length is not None),\n",
    "        padding=False\n",
    "    )\n",
    "    tokens = tokenized_output[\"input_ids\"]\n",
    "    offsets = tokenized_output[\"offset_mapping\"]\n",
    "\n",
    "    # Get subword tokenized versions of the text\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    \n",
    "    # Initialize labels as 'O'\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    if trigger_spans is not None:\n",
    "        # Assign 'TRIGGER' to overlapping tokens\n",
    "        for start, end in trigger_spans:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == 0 and tok_end == 0:\n",
    "                    continue\n",
    "                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n",
    "                    labels[i] = 1\n",
    "\n",
    "    tokenized_output['labels'] = labels\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "def preprocess_df(df, max_length):\n",
    "    \"\"\"Modified processing incorporating trigger span handling\"\"\"\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    df['seq_labels'] = df.progress_apply(\n",
    "        lambda row: convert_to_seq_labeling(\n",
    "            text=row['content'],\n",
    "            tokenizer=tokenizer,\n",
    "            trigger_spans=row.get('trigger_words', None),  # Handle both validation and test cases\n",
    "            max_length=max_length\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract all tokenizer outputs\n",
    "    for column in df.seq_labels.iloc[0].keys():\n",
    "        df[column] = df.seq_labels.apply(lambda x: x.get(column))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.171008Z",
     "iopub.status.idle": "2025-03-30T01:32:51.171243Z",
     "shell.execute_reply": "2025-03-30T01:32:51.171147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)\n",
    "\n",
    "is_valid_mask = (df.fold == 4)\n",
    "df_train = df[~is_valid_mask].copy()\n",
    "df_valid = df[is_valid_mask].copy()\n",
    "\n",
    "\n",
    "df_train = preprocess_df(df_train, max_length=config.max_length)\n",
    "df_valid = preprocess_df(df_valid, max_length=None)\n",
    "df_test = preprocess_df(df_test, max_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.171791Z",
     "iopub.status.idle": "2025-03-30T01:32:51.172122Z",
     "shell.execute_reply": "2025-03-30T01:32:51.172005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_columns = list(df_train.seq_labels.iloc[0].keys()) +\\\n",
    "                ['content', 'trigger_words']\n",
    "test_columns = list(df_train.seq_labels.iloc[0].keys()) + ['content']\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train[train_columns].reset_index(drop=True))\n",
    "ds_valid = Dataset.from_pandas(df_valid[train_columns].reset_index(drop=True))\n",
    "ds_test = Dataset.from_pandas(df_test[test_columns].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.172672Z",
     "iopub.status.idle": "2025-03-30T01:32:51.172943Z",
     "shell.execute_reply": "2025-03-30T01:32:51.172806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "train_labels = df_train.labels.tolist() + df_valid.labels.tolist()\n",
    "positive_class_balance = pd.Series(list(chain(*train_labels))).mean()\n",
    "positive_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.179317Z",
     "iopub.status.idle": "2025-03-30T01:32:51.179614Z",
     "shell.execute_reply": "2025-03-30T01:32:51.179484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "def extract_chars_from_spans(spans):\n",
    "    \"\"\"\n",
    "    Given a list of spans (each a tuple (start, end)),\n",
    "    return a set of character indices for all spans.\n",
    "    \"\"\"\n",
    "    char_set = set()\n",
    "    for start, end in spans:\n",
    "        # Each span covers positions start, start+1, ..., end-1.\n",
    "        char_set.update(range(start, end))\n",
    "    return char_set\n",
    "\n",
    "class SpanEvaluationTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any = None,\n",
    "        args: TrainingArguments = None,\n",
    "        data_collator: Any = None,\n",
    "        train_dataset: Any = None,\n",
    "        eval_dataset: Any = None,\n",
    "        tokenizer: Any = None,\n",
    "        desired_positive_ratio: float = 0.25,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer with our custom compute_metrics.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.desired_positive_ratio = desired_positive_ratio\n",
    "\n",
    "    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n",
    "        total_true_chars = 0\n",
    "        total_pred_chars = 0\n",
    "        total_overlap_chars = 0\n",
    "        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n",
    "            if isinstance(true_spans, str):\n",
    "                try:\n",
    "                    true_spans = eval(true_spans)\n",
    "                except Exception:\n",
    "                    true_spans = []\n",
    "                    \n",
    "            # Convert spans to sets of character indices.\n",
    "            true_chars = extract_chars_from_spans(true_spans)\n",
    "            pred_chars = extract_chars_from_spans(pred_spans)\n",
    "            \n",
    "            total_true_chars += len(true_chars)\n",
    "            total_pred_chars += len(pred_chars)\n",
    "            total_overlap_chars += len(true_chars.intersection(pred_chars))\n",
    "            \n",
    "            union_chars = true_chars.union(pred_chars)\n",
    "            \n",
    "        # Compute precision, recall, and F1.\n",
    "        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n",
    "        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _find_optimal_threshold(self, probabilities, labels):\n",
    "        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n",
    "        best_th = 0.5  # Default starting point\n",
    "        best_diff = float(\"inf\")\n",
    "        optimal_th = best_th\n",
    "        \n",
    "        for thold in np.linspace(0.01, 0.99, num=100):\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n",
    "            total = sum([len(prediction) for prediction in true_predictions])\n",
    "            \n",
    "            positive_ratio = total_pos / total if total > 0 else 0\n",
    "            \n",
    "            diff = abs(positive_ratio - self.desired_positive_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                optimal_th = thold\n",
    "        \n",
    "        return optimal_th\n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n",
    "        eval_dataset = self.eval_dataset\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "    \n",
    "        #thresholds = np.linspace(0.1, 0.5, num=41)\n",
    "        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n",
    "        results = []\n",
    "        best_f1 = -1\n",
    "        best_th = 0\n",
    "        best_metrics = None\n",
    "    \n",
    "        for thold in tqdm(thresholds):\n",
    "            # Apply thresholding instead of argmax\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    \n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    \n",
    "            pred_spans_all = []\n",
    "            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n",
    "                samplewise_spans = []\n",
    "                current_span = None\n",
    "                for token_label, span in zip(pred, offsets):\n",
    "                    if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                        if current_span is None:\n",
    "                            current_span = [span[0], span[1]]  # Start a new span\n",
    "                        else:\n",
    "                            current_span[1] = span[1]  # Extend the span to include the current token\n",
    "                    else:  # If token_label == 0 (not an entity)\n",
    "                        if current_span is not None:\n",
    "                            samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                            current_span = None  # Reset for the next entity\n",
    "    \n",
    "                # If the last token was part of a span, save it\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))\n",
    "    \n",
    "                pred_spans_all.append(samplewise_spans)\n",
    "    \n",
    "            # Store results for this threshold\n",
    "            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n",
    "            if current_metrics['f1'] >= best_f1:\n",
    "                best_f1 = current_metrics['f1']\n",
    "                best_th = thold\n",
    "                best_metrics = current_metrics\n",
    "                best_metrics['thold'] = thold\n",
    "                \n",
    "            \n",
    "            results.append(current_metrics)\n",
    "        return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.181203Z",
     "iopub.status.idle": "2025-03-30T01:32:51.181602Z",
     "shell.execute_reply": "2025-03-30T01:32:51.181431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SpanEvaluationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    desired_positive_ratio=positive_class_balance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T01:32:51.182460Z",
     "iopub.status.idle": "2025-03-30T01:32:51.182823Z",
     "shell.execute_reply": "2025-03-30T01:32:51.182661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:45:43.239374Z",
     "iopub.status.busy": "2025-02-27T20:45:43.239080Z",
     "iopub.status.idle": "2025-02-27T20:45:43.243807Z",
     "shell.execute_reply": "2025-02-27T20:45:43.242955Z",
     "shell.execute_reply.started": "2025-02-27T20:45:43.239352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FINETUNED_MODEL = f'./model_checkpoints_{config.wandb_init_args[\"name\"]}/checkpoint-600'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:45:43.894410Z",
     "iopub.status.busy": "2025-02-27T20:45:43.894094Z",
     "iopub.status.idle": "2025-02-27T20:45:43.976599Z",
     "shell.execute_reply": "2025-02-27T20:45:43.975722Z",
     "shell.execute_reply.started": "2025-02-27T20:45:43.894385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer._load_from_checkpoint(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:45:46.616863Z",
     "iopub.status.busy": "2025-02-27T20:45:46.616480Z",
     "iopub.status.idle": "2025-02-27T20:49:31.426042Z",
     "shell.execute_reply": "2025-02-27T20:49:31.425337Z",
     "shell.execute_reply.started": "2025-02-27T20:45:46.616833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_preds = trainer.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:49:31.427328Z",
     "iopub.status.busy": "2025-02-27T20:49:31.427056Z",
     "iopub.status.idle": "2025-02-27T20:49:49.972503Z",
     "shell.execute_reply": "2025-02-27T20:49:49.971687Z",
     "shell.execute_reply.started": "2025-02-27T20:49:31.427306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_th = trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))['thold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:49:49.974004Z",
     "iopub.status.busy": "2025-02-27T20:49:49.973790Z",
     "iopub.status.idle": "2025-02-27T21:22:38.771245Z",
     "shell.execute_reply": "2025-02-27T21:22:38.770608Z",
     "shell.execute_reply.started": "2025-02-27T20:49:49.973985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_preds = trainer.predict(ds_test)\n",
    "test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()\n",
    "\n",
    "test_th = trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T21:22:38.772673Z",
     "iopub.status.busy": "2025-02-27T21:22:38.772295Z",
     "iopub.status.idle": "2025-02-27T21:22:38.777892Z",
     "shell.execute_reply": "2025-02-27T21:22:38.777105Z",
     "shell.execute_reply.started": "2025-02-27T21:22:38.772577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_th = (val_th+test_th)/2 - 0.15\n",
    "final_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:28:44.678119Z",
     "iopub.status.busy": "2025-02-27T20:28:44.677847Z",
     "iopub.status.idle": "2025-02-27T20:28:44.687692Z",
     "shell.execute_reply": "2025-02-27T20:28:44.686882Z",
     "shell.execute_reply.started": "2025-02-27T20:28:44.678098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Custom exception for participant-visible errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute span-level F1 score based on overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n",
    "    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n",
    "    - row_id_column_name (str): Column name for the row identifier.\n",
    "\n",
    "    Returns:\n",
    "    - float: The token-level weighted F1 score.\n",
    "\n",
    "    Example:\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n",
    "    ... })\n",
    "    >>> score(solution, submission, \"id\")\n",
    "    0.16296296296296295\n",
    "    \"\"\"\n",
    "    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    \n",
    "    def safe_parse_spans(trigger_words):\n",
    "        if isinstance(trigger_words, str):\n",
    "            try:\n",
    "                return ast.literal_eval(trigger_words)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n",
    "            return trigger_words\n",
    "        return []\n",
    "\n",
    "    def extract_tokens_from_spans(spans):\n",
    "        tokens = set()\n",
    "        for start, end in spans:\n",
    "            tokens.update(range(start, end))\n",
    "        return tokens\n",
    "    \n",
    "    solution = solution.copy()\n",
    "    submission = submission.copy()\n",
    "\n",
    "    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n",
    "    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n",
    "\n",
    "    merged = pd.merge(\n",
    "        solution,\n",
    "        submission,\n",
    "        on=\"id\",\n",
    "        suffixes=(\"_solution\", \"_submission\")\n",
    "    )\n",
    "\n",
    "    total_true_tokens = 0\n",
    "    total_pred_tokens = 0\n",
    "    overlapping_tokens = 0\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        true_spans = row[\"trigger_words_solution\"]\n",
    "        pred_spans = row[\"trigger_words_submission\"]\n",
    "\n",
    "        true_tokens = extract_tokens_from_spans(true_spans)\n",
    "        pred_tokens = extract_tokens_from_spans(pred_spans)\n",
    "\n",
    "        total_true_tokens += len(true_tokens)\n",
    "        total_pred_tokens += len(pred_tokens)\n",
    "        overlapping_tokens += len(true_tokens & pred_tokens)\n",
    "\n",
    "    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n",
    "    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:28:47.304495Z",
     "iopub.status.busy": "2025-02-27T20:28:47.304197Z",
     "iopub.status.idle": "2025-02-27T20:28:47.311230Z",
     "shell.execute_reply": "2025-02-27T20:28:47.310564Z",
     "shell.execute_reply.started": "2025-02-27T20:28:47.304469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference_aggregation(probabilities, labels, offset_mappings, thold):\n",
    "    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    pred_spans_all = []\n",
    "    for pred, offsets in zip(true_predictions, offset_mappings):\n",
    "        samplewise_spans = []\n",
    "        current_span = None\n",
    "        for token_label, span in zip(pred, offsets):\n",
    "            if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                if current_span is None:\n",
    "                    current_span = [span[0], span[1]]  # Start a new span\n",
    "                else:\n",
    "                    current_span[1] = span[1]  # Extend the span to include the current token\n",
    "            else:  # If token_label == 0 (not an entity)\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                    current_span = None  # Reset for the next entity\n",
    "        \n",
    "                    # If the last token was part of a span, save it\n",
    "        if current_span is not None:\n",
    "            samplewise_spans.append(tuple(current_span))\n",
    "        \n",
    "        pred_spans_all.append(samplewise_spans)\n",
    "    return [str(row) for row in pred_spans_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:28:47.557315Z",
     "iopub.status.busy": "2025-02-27T20:28:47.557091Z",
     "iopub.status.idle": "2025-02-27T20:28:48.614688Z",
     "shell.execute_reply": "2025-02-27T20:28:48.613814Z",
     "shell.execute_reply.started": "2025-02-27T20:28:47.557297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:28:48.616316Z",
     "iopub.status.busy": "2025-02-27T20:28:48.616017Z",
     "iopub.status.idle": "2025-02-27T20:28:48.711144Z",
     "shell.execute_reply": "2025-02-27T20:28:48.710510Z",
     "shell.execute_reply.started": "2025-02-27T20:28:48.616284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_pred = deepcopy(df_gt)\n",
    "df_pred['trigger_words'] = valid_results\n",
    "cv_f1 = score(df_gt, df_pred, row_id_column_name='id')\n",
    "\n",
    "cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ss = pd.read_csv('/kaggle/input/unlp-2025-shared-task-span-identification/sample_submission.csv')\n",
    "ss['trigger_words'] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T20:45:12.920102Z",
     "iopub.status.busy": "2025-02-27T20:45:12.919772Z",
     "iopub.status.idle": "2025-02-27T20:45:12.928806Z",
     "shell.execute_reply": "2025-02-27T20:45:12.927870Z",
     "shell.execute_reply.started": "2025-02-27T20:45:12.920080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_pred.to_csv(f\"{config.wandb_init_args[\"name\"]}-cv{cv_f1:.2f}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10931355,
     "sourceId": 89664,
     "sourceType": "competition"
    },
    {
     "datasetId": 6604871,
     "sourceId": 10664686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
