{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10664686,"sourceType":"datasetVersion","datasetId":6604871}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)","metadata":{"_uuid":"1dfe9b25-83da-47f0-9209-18dbd2b54ad4","_cell_guid":"cee816df-1067-4736-9235-308def338c0b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:25.515827Z","iopub.execute_input":"2025-02-04T23:02:25.516177Z","iopub.status.idle":"2025-02-04T23:02:28.536476Z","shell.execute_reply.started":"2025-02-04T23:02:25.516149Z","shell.execute_reply":"2025-02-04T23:02:28.535577Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPRETRAINED_MODEL = \"bert-base-multilingual-cased\"","metadata":{"_uuid":"0111bc09-0f06-44ec-bbb1-87ba0c16c05a","_cell_guid":"99ddbffc-acb9-4df4-a4de-4cbb505cce27","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:29.904769Z","iopub.execute_input":"2025-02-04T23:02:29.905221Z","iopub.status.idle":"2025-02-04T23:02:29.909271Z","shell.execute_reply.started":"2025-02-04T23:02:29.905192Z","shell.execute_reply":"2025-02-04T23:02:29.908234Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data","metadata":{"_uuid":"d15467c8-74a0-4a4e-9538-fc45f7b93fde","_cell_guid":"c345ddbe-6098-44f7-8c0d-f5452f2acbc7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\n\nkaggle_path = '/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet'\ndf = pd.read_parquet(kaggle_path) #pd.read_parquet('train.parquet')","metadata":{"_uuid":"fd1a8000-7ba9-436e-b753-7bf6d6a410ef","_cell_guid":"5a3b5d33-3dbd-4b3e-acd2-7b87358e2ee9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:31.506787Z","iopub.execute_input":"2025-02-04T23:02:31.507111Z","iopub.status.idle":"2025-02-04T23:02:32.071231Z","shell.execute_reply.started":"2025-02-04T23:02:31.507084Z","shell.execute_reply":"2025-02-04T23:02:32.070546Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"cv_split = pd.read_csv(\"/kaggle/input/unlp25-cross-validation-split/cv_split.csv\")\n\ndf = df.merge(cv_split, on=\"id\")\ndf['is_valid'] = (df['fold'] == 4).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:02:33.011952Z","iopub.execute_input":"2025-02-04T23:02:33.012596Z","iopub.status.idle":"2025-02-04T23:02:33.051868Z","shell.execute_reply.started":"2025-02-04T23:02:33.012567Z","shell.execute_reply":"2025-02-04T23:02:33.051187Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"36de962c-497e-4ed1-b5af-f2dc26594442","_cell_guid":"f96bb3b2-7c58-4782-9444-e2b5674bb5bc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:33.904746Z","iopub.execute_input":"2025-02-04T23:02:33.905086Z","iopub.status.idle":"2025-02-04T23:02:33.925509Z","shell.execute_reply.started":"2025-02-04T23:02:33.905057Z","shell.execute_reply":"2025-02-04T23:02:33.924895Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  fold  is_valid  \n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1         0  \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3         0  \n2                                    [[55, 100]]     1         0  \n3                                           None     2         0  \n4                                   [[114, 144]]     2         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n      <th>is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[loaded_language, euphoria]</td>\n      <td>[[55, 100]]</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>uk</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language]</td>\n      <td>[[114, 144]]</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Targets Prep","metadata":{"_uuid":"d705e9d3-0e60-486d-b19f-1e3c4b6c5a29","_cell_guid":"bbe093d1-722e-471e-b742-35ae296e7b60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Classification","metadata":{"_uuid":"59ec63a9-088c-45fc-a8f3-245ebef7266e","_cell_guid":"ce4b51ec-d63b-438c-ae23-c45e8cc7a642","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from collections.abc import Iterable\n\ntechniques = ['straw_man', 'appeal_to_fear', 'fud', 'bandwagon', 'whataboutism', 'loaded_language', 'glittering_generalities', 'euphoria', 'cherry_picking', 'cliche']\n\nfor col in techniques:\n    df[col] = 0\n\nimport numpy as np\nfor ind, row in df.iterrows():\n    if isinstance(row['techniques'], Iterable):\n        for t in row['techniques']:\n            df.loc[ind, t] = 1\n\ndf['clf_labels'] = list(df[techniques].values)\ndf.drop(columns=techniques, inplace=True)","metadata":{"_uuid":"552a2940-f222-48e3-9975-93caae44b4d3","_cell_guid":"7018698c-346e-48a7-a9fe-bb625be626e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:39.115686Z","iopub.execute_input":"2025-02-04T23:02:39.116020Z","iopub.status.idle":"2025-02-04T23:02:40.152555Z","shell.execute_reply.started":"2025-02-04T23:02:39.115991Z","shell.execute_reply":"2025-02-04T23:02:40.151894Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"22fed7b3-8d7f-4f2f-8a00-08011d18f74e","_cell_guid":"60512e36-a2b1-467e-8bcc-0e86a0ba24b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:41.868573Z","iopub.execute_input":"2025-02-04T23:02:41.868937Z","iopub.status.idle":"2025-02-04T23:02:41.885615Z","shell.execute_reply.started":"2025-02-04T23:02:41.868907Z","shell.execute_reply":"2025-02-04T23:02:41.884733Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  fold  is_valid  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1         0   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3         0   \n2                                    [[55, 100]]     1         0   \n3                                           None     2         0   \n4                                   [[114, 144]]     2         0   \n\n                       clf_labels  \n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]  \n2  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n      <th>is_valid</th>\n      <th>clf_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[loaded_language, euphoria]</td>\n      <td>[[55, 100]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>uk</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>2</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language]</td>\n      <td>[[114, 144]]</td>\n      <td>2</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Span","metadata":{"_uuid":"cfb25aac-35f8-453a-ab30-46ae2efd94fb","_cell_guid":"53e697bd-47d3-432b-8032-e927a3f7aa79","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import spacy\n\nfrom spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\nfrom tqdm.autonotebook import tqdm\ntqdm.pandas()\n\ndf.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)\ndf['target'] = df.trigger_words.apply(lambda x: [[y[0], y[1], 'TRIGGER'] for y in x])\n\ndef resolve_overlapping_spans(spans):\n    if not spans:\n        return []\n    spans = sorted(spans, key=lambda x: x[0])  # Sort by start index\n    resolved = [spans[0]]\n    for current in spans[1:]:\n        last = resolved[-1]\n        if current[0] < last[1]:  # Overlap\n            new_span = (last[0], max(last[1], current[1]), 'TRIGGER')\n            resolved[-1] = new_span\n            print('resolved')\n        else:\n            resolved.append(current)\n    return resolved\n\ndf['target'] = df.target.apply(resolve_overlapping_spans)\n\nnlp = spacy.blank(\"xx\")\n\ndef convert_to_conll(row):\n    data = {\n        \"text\": row['content'],\n        \"label\": row['target']\n    }\n    doc = nlp(data[\"text\"])\n    ents = []\n    for start, end, label in data[\"label\"]:\n        span = doc.char_span(start, end, label=label)\n        if span is not None:\n            ents.append(span)\n        else:\n            pass\n        #TODO fix not align to token case\n        '''\n            print(\n                \"Skipping span (does not align to tokens):\",\n                start,\n                end,\n                label,\n                doc.text[start:end],\n            )\n        '''\n    doc.ents = ents\n    return {\n        'tokens': list([t.text for t in doc]),\n        'labels': list(biluo_to_iob(doc_to_biluo_tags(doc)))\n    }\n\ndf['conll'] = df.progress_apply(convert_to_conll, axis=1)","metadata":{"_uuid":"8ffec65a-6d51-44b7-89f1-3ba78e7ab1dc","_cell_guid":"1558919e-d269-47b4-a28d-135b3b5aed40","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:43.748246Z","iopub.execute_input":"2025-02-04T23:02:43.748544Z","iopub.status.idle":"2025-02-04T23:02:53.331868Z","shell.execute_reply.started":"2025-02-04T23:02:43.748516Z","shell.execute_reply":"2025-02-04T23:02:53.330856Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"resolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\nresolved\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3822 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8318b49eca4fa486a2cc421b1e61c2"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"label2id = {'O': 0, 'B-TRIGGER': 1, 'I-TRIGGER': 2}\n\ndf['tokens'] = df.conll.str['tokens']\ndf['ner_tags'] = df.conll.str['labels'].apply(lambda x: [label2id[t] for t in x])\n\ndf_train = df[df.is_valid == 0]\ndf_valid = df[df.is_valid == 1]","metadata":{"_uuid":"cbb23bb3-5eda-443c-9418-e71932fe2eec","_cell_guid":"83c514c1-9a08-43db-b6a8-a5bf3514e1fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:56.904182Z","iopub.execute_input":"2025-02-04T23:02:56.904682Z","iopub.status.idle":"2025-02-04T23:02:56.949930Z","shell.execute_reply.started":"2025-02-04T23:02:56.904651Z","shell.execute_reply":"2025-02-04T23:02:56.949290Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train.head()","metadata":{"_uuid":"68e64a3c-0781-464d-bb88-c73f099c4a98","_cell_guid":"dfd286ad-5857-4340-aa7c-9c049b02708f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:02:58.100659Z","iopub.execute_input":"2025-02-04T23:02:58.100977Z","iopub.status.idle":"2025-02-04T23:02:58.133891Z","shell.execute_reply.started":"2025-02-04T23:02:58.100950Z","shell.execute_reply":"2025-02-04T23:02:58.132831Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  fold  is_valid  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1         0   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3         0   \n2                                    [[55, 100]]     1         0   \n3                                             []     2         0   \n4                                   [[114, 144]]     2         0   \n\n                       clf_labels  \\\n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]   \n2  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n\n                                              target  \\\n0  [[27, 63, TRIGGER], [65, 88, TRIGGER], [90, 18...   \n1  [[0, 40, TRIGGER], [123, 137, TRIGGER], [180, ...   \n2                               [[55, 100, TRIGGER]]   \n3                                                 []   \n4                              [[114, 144, TRIGGER]]   \n\n                                               conll  \\\n0  {'tokens': ['–ù–æ–≤–∏–π', '–æ–≥–ª—è–¥', '–º–∞–ø–∏', 'DeepSta...   \n1  {'tokens': ['–ù–µ–¥–∞–≤–Ω–æ', '95', '–∫–≤–∞—Ä—Ç–∞–ª', '–∂—ë—Å—Ç–∫...   \n2  {'tokens': ['ü§©', '\n', '–¢–∏–º', '—á–∞—Å–æ–º', '–π–¥–µ', '...   \n3  {'tokens': ['–í', '–£–∫—Ä–∞—ó–Ω—ñ', '–Ω–∞–π–±–ª–∏–∂—á–∏–º', '—á–∞—Å...   \n4  {'tokens': ['–†–∞—Å—á—ë—Ç—ã', '122-–º–º', '–°–ê–£', '2–°1',...   \n\n                                              tokens  \\\n0  [–ù–æ–≤–∏–π, –æ–≥–ª—è–¥, –º–∞–ø–∏, DeepState, –≤—ñ–¥, —Ä–æ—Å—ñ–π—Å—å–∫–æ...   \n1  [–ù–µ–¥–∞–≤–Ω–æ, 95, –∫–≤–∞—Ä—Ç–∞–ª, –∂—ë—Å—Ç–∫–æ, –ø–æ–≥–ª—É–º–∏–ª—Å—è, –Ω–∞–¥...   \n2  [ü§©, \\n, –¢–∏–º, —á–∞—Å–æ–º, –π–¥–µ, –µ–≤–∞–∫—É–∞—Ü—ñ—è, –ë—î–ª–≥–æ—Ä–æ–¥—Å—å...   \n3  [–í, –£–∫—Ä–∞—ó–Ω—ñ, –Ω–∞–π–±–ª–∏–∂—á–∏–º, —á–∞—Å–æ–º, –º–∞—é—Ç—å, –Ω–∞–º—ñ—Ä, ...   \n4  [–†–∞—Å—á—ë—Ç—ã, 122-–º–º, –°–ê–£, 2–°1, \", –ì–≤–æ–∑–¥–∏–∫–∞, \", 13...   \n\n                                            ner_tags  \n0  [0, 0, 0, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 1, ...  \n1  [1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n      <th>is_valid</th>\n      <th>clf_labels</th>\n      <th>target</th>\n      <th>conll</th>\n      <th>tokens</th>\n      <th>ner_tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n      <td>[[27, 63, TRIGGER], [65, 88, TRIGGER], [90, 18...</td>\n      <td>{'tokens': ['–ù–æ–≤–∏–π', '–æ–≥–ª—è–¥', '–º–∞–ø–∏', 'DeepSta...</td>\n      <td>[–ù–æ–≤–∏–π, –æ–≥–ª—è–¥, –º–∞–ø–∏, DeepState, –≤—ñ–¥, —Ä–æ—Å—ñ–π—Å—å–∫–æ...</td>\n      <td>[0, 0, 0, 0, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n      <td>[[0, 40, TRIGGER], [123, 137, TRIGGER], [180, ...</td>\n      <td>{'tokens': ['–ù–µ–¥–∞–≤–Ω–æ', '95', '–∫–≤–∞—Ä—Ç–∞–ª', '–∂—ë—Å—Ç–∫...</td>\n      <td>[–ù–µ–¥–∞–≤–Ω–æ, 95, –∫–≤–∞—Ä—Ç–∞–ª, –∂—ë—Å—Ç–∫–æ, –ø–æ–≥–ª—É–º–∏–ª—Å—è, –Ω–∞–¥...</td>\n      <td>[1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[loaded_language, euphoria]</td>\n      <td>[[55, 100]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n      <td>[[55, 100, TRIGGER]]</td>\n      <td>{'tokens': ['ü§©', '\n', '–¢–∏–º', '—á–∞—Å–æ–º', '–π–¥–µ', '...</td>\n      <td>[ü§©, \\n, –¢–∏–º, —á–∞—Å–æ–º, –π–¥–µ, –µ–≤–∞–∫—É–∞—Ü—ñ—è, –ë—î–ª–≥–æ—Ä–æ–¥—Å—å...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>uk</td>\n      <td>False</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>2</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[]</td>\n      <td>{'tokens': ['–í', '–£–∫—Ä–∞—ó–Ω—ñ', '–Ω–∞–π–±–ª–∏–∂—á–∏–º', '—á–∞—Å...</td>\n      <td>[–í, –£–∫—Ä–∞—ó–Ω—ñ, –Ω–∞–π–±–ª–∏–∂—á–∏–º, —á–∞—Å–æ–º, –º–∞—é—Ç—å, –Ω–∞–º—ñ—Ä, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language]</td>\n      <td>[[114, 144]]</td>\n      <td>2</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n      <td>[[114, 144, TRIGGER]]</td>\n      <td>{'tokens': ['–†–∞—Å—á—ë—Ç—ã', '122-–º–º', '–°–ê–£', '2–°1',...</td>\n      <td>[–†–∞—Å—á—ë—Ç—ã, 122-–º–º, –°–ê–£, 2–°1, \", –ì–≤–æ–∑–¥–∏–∫–∞, \", 13...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Datasets","metadata":{"_uuid":"b5bfb354-29d5-4ccd-9cf9-6bddbeda581c","_cell_guid":"652ffd3a-60ba-4f46-b7cc-ca69a5f17e77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nos.makedirs('data', exist_ok=True)\n\ndf_train[['tokens', 'clf_labels', 'ner_tags']].to_json(\n    './data/train_processed.json', orient='records', lines=True)\ndf_valid[['tokens', 'clf_labels', 'ner_tags']].to_json(\n    './data/valid_processed.json', orient='records', lines=True)","metadata":{"_uuid":"4e1de90e-1fb1-42ef-9a5b-8037469210fe","_cell_guid":"1386e6d2-4c0a-46af-b666-ce2e253fba4f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:00.423294Z","iopub.execute_input":"2025-02-04T23:03:00.423625Z","iopub.status.idle":"2025-02-04T23:03:00.658152Z","shell.execute_reply.started":"2025-02-04T23:03:00.423593Z","shell.execute_reply":"2025-02-04T23:03:00.657451Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets_ua = load_dataset(\n    \"json\",\n    data_files={\n        'train': './data/train_processed.json',\n        'val': './data/valid_processed.json'\n    }\n)\nraw_datasets_ua","metadata":{"_uuid":"3a3bb6af-f65b-4675-8ef4-c02676550bf7","_cell_guid":"08edad22-fba4-4c5c-b676-365cfaec9db9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:01.704074Z","iopub.execute_input":"2025-02-04T23:03:01.704372Z","iopub.status.idle":"2025-02-04T23:03:03.170330Z","shell.execute_reply.started":"2025-02-04T23:03:01.704348Z","shell.execute_reply":"2025-02-04T23:03:03.169640Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faeb775834b1491c9faec916faca751a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2b4400bb5747de829bff6ff2fb4c2b"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'clf_labels', 'ner_tags'],\n        num_rows: 3058\n    })\n    val: Dataset({\n        features: ['tokens', 'clf_labels', 'ner_tags'],\n        num_rows: 764\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Tokenization","metadata":{"_uuid":"33490ea5-f38a-4ca5-adee-e9ca529c7198","_cell_guid":"395ea214-7e95-4cbe-924f-c3d216fae8c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n","metadata":{"_uuid":"07b4f9ce-f0af-46df-af9a-370ce37c85dd","_cell_guid":"7352fd82-12d8-48c7-8aa7-1b8000cc9396","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:06.196241Z","iopub.execute_input":"2025-02-04T23:03:06.196782Z","iopub.status.idle":"2025-02-04T23:03:12.790722Z","shell.execute_reply.started":"2025-02-04T23:03:06.196753Z","shell.execute_reply":"2025-02-04T23:03:12.789921Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf59ff1fa064da99dbc453d1a5f69e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a0d076a9d144cabacfb5019b853fd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5a65a11c104e00b3061fb720339287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ce3b8f6f604f7b8f730de1de75f254"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_token_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_token_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_token_labels\n    tokenized_inputs[\"sequence_labels\"] = examples[\"clf_labels\"]\n    \n    return tokenized_inputs","metadata":{"_uuid":"18b295d9-6ec9-4280-b684-0027a08f1dfa","_cell_guid":"76743308-3c7e-4ea9-b127-ab71d78ebfb0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:16.408004Z","iopub.execute_input":"2025-02-04T23:03:16.408567Z","iopub.status.idle":"2025-02-04T23:03:16.414757Z","shell.execute_reply.started":"2025-02-04T23:03:16.408539Z","shell.execute_reply":"2025-02-04T23:03:16.413888Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tokenized_datasets_ua = raw_datasets_ua.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets_ua[\"train\"].column_names,\n)","metadata":{"_uuid":"fda1b658-9fa9-4e44-a8c0-66f01734e1d0","_cell_guid":"0a128975-aaf4-435d-8b7d-a2ef121f2984","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:18.001884Z","iopub.execute_input":"2025-02-04T23:03:18.002221Z","iopub.status.idle":"2025-02-04T23:03:20.501060Z","shell.execute_reply.started":"2025-02-04T23:03:18.002188Z","shell.execute_reply":"2025-02-04T23:03:20.500194Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3058 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088fc455ee3b4f08b0449eea56912fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/764 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c533eecb6c437593d1f91c0aa5da9a"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"tokenized_datasets_ua['train'].to_pandas().head()","metadata":{"_uuid":"88d24273-636c-4bfa-9bc3-0f87c1f4f47e","_cell_guid":"e68c5615-d2fd-4500-bb33-96dd1d1c19d4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:22.187551Z","iopub.execute_input":"2025-02-04T23:03:22.187931Z","iopub.status.idle":"2025-02-04T23:03:22.223914Z","shell.execute_reply.started":"2025-02-04T23:03:22.187899Z","shell.execute_reply":"2025-02-04T23:03:22.223224Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0  [101, 100325, 555, 41824, 97744, 20785, 18891,...   \n1  [101, 21124, 95227, 11978, 69055, 50680, 10517...   \n2  [101, 100, 61059, 60019, 550, 12265, 546, 1085...   \n3  [101, 511, 21567, 15861, 61394, 10191, 12025, ...   \n4  [101, 525, 18291, 56604, 10292, 17484, 118, 14...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                              labels  \\\n0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, ...   \n1  [-100, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                  sequence_labels  \n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]  \n2  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>sequence_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[101, 100325, 555, 41824, 97744, 20785, 18891,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[101, 21124, 95227, 11978, 69055, 50680, 10517...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[101, 100, 61059, 60019, 550, 12265, 546, 1085...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[101, 511, 21567, 15861, 61394, 10191, 12025, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[101, 525, 18291, 56604, 10292, 17484, 118, 14...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"14bdff46-04eb-4dcc-9b7c-04ad3fa91db2","_cell_guid":"49877ab0-b991-4efe-82b0-5d48877c2032","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\nfrom transformers import BertModel, BertPreTrainedModel","metadata":{"_uuid":"01428bd6-c61c-4af6-861e-bee770f2104d","_cell_guid":"4161e2e5-0143-4869-b756-b5efdf457f26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:29.563395Z","iopub.execute_input":"2025-02-04T23:03:29.563697Z","iopub.status.idle":"2025-02-04T23:03:41.327418Z","shell.execute_reply.started":"2025-02-04T23:03:29.563671Z","shell.execute_reply":"2025-02-04T23:03:41.326723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class BertForTokenSequenceClassification(BertPreTrainedModel):\n    def __init__(self, model_name, num_token_labels, num_sequence_labels):\n        bert_model = BertModel.from_pretrained(model_name)\n        super().__init__(bert_model.config)\n        self.bert = bert_model\n        hidden_size = self.config.hidden_size\n\n        # Token Classification Head\n        self.token_classifier = nn.Linear(hidden_size, num_token_labels)\n\n        # Sequence Classification Head\n        self.sequence_classifier = nn.Linear(hidden_size, num_sequence_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(self, input_ids, attention_mask, labels=None, sequence_labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state  # Shape: (batch, seq_len, hidden)\n\n        # Token Classification Output (Apply to each token)\n        token_logits = self.token_classifier(sequence_output)  # (batch, seq_len, num_token_labels)\n\n        # Sequence Classification Output (Use [CLS] token's representation)\n        cls_output = sequence_output[:, 0, :]  # Take first token (CLS)\n        sequence_logits = self.sequence_classifier(cls_output)  # (batch, num_sequence_labels)\n\n        loss = None\n        if labels is not None and sequence_labels is not None:\n            token_loss_fn = nn.CrossEntropyLoss()\n            seq_loss_fn = nn.BCEWithLogitsLoss()  # For multi-label classification\n\n            token_loss = token_loss_fn(token_logits.view(-1, token_logits.shape[-1]), labels.view(-1))\n            seq_loss = seq_loss_fn(sequence_logits, sequence_labels.float())\n\n            loss = token_loss + seq_loss  # Combine losses\n\n        return {\n            \"loss\": loss,\n            \"token_logits\": token_logits,\n            \"sequence_logits\": sequence_logits,\n        }","metadata":{"_uuid":"9936ba82-7e0e-4653-8d2a-e6b152d38ba5","_cell_guid":"5ff939da-38dc-41b4-97eb-5c3d4771c110","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:43.709490Z","iopub.execute_input":"2025-02-04T23:03:43.710152Z","iopub.status.idle":"2025-02-04T23:03:43.716629Z","shell.execute_reply.started":"2025-02-04T23:03:43.710120Z","shell.execute_reply":"2025-02-04T23:03:43.715662Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Init and Test","metadata":{"_uuid":"300483b3-fecb-442d-b346-fffcd5bf751e","_cell_guid":"697962c3-ce08-4fa0-9019-7d17ce437c0e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sample = tokenized_datasets_ua['train'][0]\n\n# Convert input to batch format (add batch dimension)\ninput_ids = torch.tensor([sample[\"input_ids\"]])\nattention_mask = torch.tensor([sample[\"attention_mask\"]])\ntoken_labels = torch.tensor([sample[\"labels\"]])\nsequence_labels = torch.tensor([sample[\"sequence_labels\"]])","metadata":{"_uuid":"07084858-848a-462a-894b-6c37a6b7bbb2","_cell_guid":"2268950a-7760-411f-860a-949178a5c112","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:44.549959Z","iopub.execute_input":"2025-02-04T23:03:44.550249Z","iopub.status.idle":"2025-02-04T23:03:44.567172Z","shell.execute_reply.started":"2025-02-04T23:03:44.550226Z","shell.execute_reply":"2025-02-04T23:03:44.566454Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model = BertForTokenSequenceClassification(\n    model_name=PRETRAINED_MODEL,\n    num_token_labels=3,\n    num_sequence_labels=10\n)\n\nmodel","metadata":{"_uuid":"2569323d-c395-4134-9b52-886bc339308d","_cell_guid":"94ea0525-f66c-4750-9183-ecbb883ca9e0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:46.096136Z","iopub.execute_input":"2025-02-04T23:03:46.096429Z","iopub.status.idle":"2025-02-04T23:03:50.012294Z","shell.execute_reply.started":"2025-02-04T23:03:46.096405Z","shell.execute_reply":"2025-02-04T23:03:50.011428Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a8dbf9df03458e962b093f84b86a99"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"BertForTokenSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (token_classifier): Linear(in_features=768, out_features=3, bias=True)\n  (sequence_classifier): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Metrics","metadata":{"_uuid":"8ed49dfa-9f75-405e-bcbc-e74ac2377cad","_cell_guid":"1d3e034e-8cfe-482f-a689-af35ca573a2f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install evaluate seqeval","metadata":{"_uuid":"d53367c6-c066-435d-86c3-90f8fa7fbf58","_cell_guid":"173172bf-3d13-4f29-829d-61ab5519ea55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:03:52.019596Z","iopub.execute_input":"2025-02-04T23:03:52.019960Z","iopub.status.idle":"2025-02-04T23:04:01.783094Z","shell.execute_reply.started":"2025-02-04T23:03:52.019927Z","shell.execute_reply":"2025-02-04T23:04:01.781985Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m26.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=433a1b3828c92a35aeedda0f209e10a78ee8d9c29c9540be961856e758045f5d\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval, evaluate\nSuccessfully installed evaluate-0.4.3 seqeval-1.2.2\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom sklearn.metrics import f1_score\nseqeval = evaluate.load(\"seqeval\")\n\nlabel_names = list(label2id.keys())\n\ndef compute_metrics(eval_pred):\n    # print(eval_pred.predictions[0].shape)\n    # print(eval_pred.predictions[1].shape)\n    \n    token_logits, sequence_logits = eval_pred.predictions\n    token_labels, sequence_labels = eval_pred.label_ids\n\n    # Token classification metrics\n    token_metrics = compute_token_metrics(token_logits, token_labels)\n    \n    # Sequence classification metrics (multi-label)\n    sequence_metrics = compute_sequence_metrics(sequence_logits, sequence_labels)\n\n    return {\n        **{f\"sequence_{key}\": value for key, value in sequence_metrics.items()},\n        **{f\"token_{key}\": value for key, value in token_metrics.items()}\n    }\n\n\ndef compute_token_metrics(logits, labels):\n    predictions = np.argmax(logits, axis=-1)\n\n    # Remove ignored index (special tokens) and convert to labels\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": all_metrics[\"overall_precision\"],\n        \"recall\": all_metrics[\"overall_recall\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n    }\n    \n\ndef compute_sequence_metrics(logits, labels):\n    predictions = (logits >= 0.0).astype(int)\n\n    return {\n        \"f1\": f1_score(labels, predictions, average=\"macro\")\n    }","metadata":{"_uuid":"e9c5802f-e69b-4d26-b337-1b6f4f3e43fb","_cell_guid":"ea58abdf-7305-4796-9366-5d9ca45d8cb9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:04:04.212285Z","iopub.execute_input":"2025-02-04T23:04:04.212636Z","iopub.status.idle":"2025-02-04T23:04:05.328294Z","shell.execute_reply.started":"2025-02-04T23:04:04.212603Z","shell.execute_reply":"2025-02-04T23:04:05.327645Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4853eb958249368f3783af2242643c"}},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Train","metadata":{"_uuid":"f6e9954c-d610-4acc-92af-647aa1299f72","_cell_guid":"c103352a-8cf6-442d-b13c-aaa4b97f6af8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"EPOCHS = 5","metadata":{"_uuid":"c0df1238-cb4a-4d68-9954-0d6cd01152c7","_cell_guid":"e437c553-d5bf-493a-808c-6e31f86a1e5e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:04:05.329197Z","iopub.execute_input":"2025-02-04T23:04:05.329430Z","iopub.status.idle":"2025-02-04T23:04:05.332816Z","shell.execute_reply.started":"2025-02-04T23:04:05.329396Z","shell.execute_reply":"2025-02-04T23:04:05.332143Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\nclass CustomDataCollator(DataCollatorForTokenClassification):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n    \n    def __call__(self, features):\n        # Separate token-level and sequence-level labels\n        sequence_labels = [f.pop(\"sequence_labels\") for f in features]\n        \n        # Use Hugging Face's built-in collator for token classification\n        batch = super().torch_call(features)\n        \n        # Convert sequence labels to tensor\n        batch[\"sequence_labels\"] = torch.tensor(sequence_labels, dtype=torch.int64)\n        \n        return batch\n\n# Use the custom data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)","metadata":{"_uuid":"4dba46cf-1458-48ea-8e01-6fd8b44828ab","_cell_guid":"2b300f6e-5b6c-4b54-afe0-b428b8d22f64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:04:07.467043Z","iopub.execute_input":"2025-02-04T23:04:07.467337Z","iopub.status.idle":"2025-02-04T23:04:07.472675Z","shell.execute_reply.started":"2025-02-04T23:04:07.467315Z","shell.execute_reply":"2025-02-04T23:04:07.471754Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW([\n    {'params': list(model.bert.parameters()), 'lr': 2e-5},\n    {'params': list(model.token_classifier.parameters()), 'lr': 1e-4},\n    {'params': list(model.sequence_classifier.parameters()), 'lr': 1e-4}\n])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0.1*EPOCHS*(tokenized_datasets_ua['train'].num_rows/16),\n    num_training_steps=EPOCHS*(tokenized_datasets_ua['train'].num_rows/16)\n)","metadata":{"_uuid":"d06fb774-8183-4478-aa4e-a7251d953493","_cell_guid":"d873684b-08ac-4bb7-85e0-b89cec712e68","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:04:11.132980Z","iopub.execute_input":"2025-02-04T23:04:11.133318Z","iopub.status.idle":"2025-02-04T23:04:11.148119Z","shell.execute_reply.started":"2025-02-04T23:04:11.133287Z","shell.execute_reply":"2025-02-04T23:04:11.147341Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=EPOCHS,\n    \n    output_dir=\"./results\",\n    logging_strategy=\"steps\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets_ua[\"train\"],\n    eval_dataset=tokenized_datasets_ua[\"val\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, scheduler),\n)\n\ntrainer.train()","metadata":{"_uuid":"227045e6-6321-4b21-881e-68c2fe25457d","_cell_guid":"05530542-136f-4239-ae55-7be22d786c74","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-04T23:04:19.731800Z","iopub.execute_input":"2025-02-04T23:04:19.732111Z","iopub.status.idle":"2025-02-04T23:18:56.744812Z","shell.execute_reply.started":"2025-02-04T23:04:19.732088Z","shell.execute_reply":"2025-02-04T23:18:56.743514Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-26-08bede53136e>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [960/960 14:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Sequence F1</th>\n      <th>Token Precision</th>\n      <th>Token Recall</th>\n      <th>Token F1</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.664400</td>\n      <td>0.749648</td>\n      <td>0.116178</td>\n      <td>0.012184</td>\n      <td>0.024149</td>\n      <td>0.016196</td>\n      <td>0.781049</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.733200</td>\n      <td>0.730565</td>\n      <td>0.169089</td>\n      <td>0.017191</td>\n      <td>0.025387</td>\n      <td>0.020500</td>\n      <td>0.785329</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.635300</td>\n      <td>0.725438</td>\n      <td>0.235562</td>\n      <td>0.021934</td>\n      <td>0.047059</td>\n      <td>0.029921</td>\n      <td>0.780908</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.559600</td>\n      <td>0.825035</td>\n      <td>0.230990</td>\n      <td>0.026801</td>\n      <td>0.029721</td>\n      <td>0.028186</td>\n      <td>0.784698</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.379900</td>\n      <td>0.810022</td>\n      <td>0.235821</td>\n      <td>0.032787</td>\n      <td>0.048297</td>\n      <td>0.039059</td>\n      <td>0.782384</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=960, training_loss=0.6700409390032291, metrics={'train_runtime': 874.7457, 'train_samples_per_second': 17.479, 'train_steps_per_second': 1.097, 'total_flos': 3685202400667932.0, 'train_loss': 0.6700409390032291, 'epoch': 5.0})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Replace this with your own checkpoint\nmodel_checkpoint = \"/kaggle/working/results/checkpoint-384\"\n# model_checkpoint = \"/kaggle/working/results/checkpoint-576\"\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:01.719363Z","iopub.execute_input":"2025-02-04T23:50:01.719661Z","iopub.status.idle":"2025-02-04T23:50:02.182461Z","shell.execute_reply.started":"2025-02-04T23:50:01.719636Z","shell.execute_reply":"2025-02-04T23:50:02.181749Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at /kaggle/working/results/checkpoint-384 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use cuda:0\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"preds = token_classifier.predict(df_valid.content.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:02.183825Z","iopub.execute_input":"2025-02-04T23:50:02.184101Z","iopub.status.idle":"2025-02-04T23:50:17.122459Z","shell.execute_reply.started":"2025-02-04T23:50:02.184079Z","shell.execute_reply":"2025-02-04T23:50:17.121724Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"val_sub = [str([(p['start'], p['end']) for p in row]) for row in preds]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:17.124042Z","iopub.execute_input":"2025-02-04T23:50:17.124323Z","iopub.status.idle":"2025-02-04T23:50:17.143959Z","shell.execute_reply.started":"2025-02-04T23:50:17.124300Z","shell.execute_reply":"2025-02-04T23:50:17.143134Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from copy import deepcopy\n\ndef safe_string(row):\n    if row is None:\n        return '[]'\n    else:\n        return str([(s[0], s[1]) for s in row])\n\nvalid_sub = deepcopy(df_valid)\nvalid_sub['trigger_words'] = valid_sub.trigger_words.apply(safe_string)\nvalid_sub_gt = deepcopy(valid_sub[['id', 'trigger_words']])\nvalid_sub_hat = deepcopy(valid_sub[['id', 'trigger_words']])\nvalid_sub_hat['trigger_words'] = val_sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:17.145272Z","iopub.execute_input":"2025-02-04T23:50:17.145540Z","iopub.status.idle":"2025-02-04T23:50:17.166899Z","shell.execute_reply.started":"2025-02-04T23:50:17.145519Z","shell.execute_reply":"2025-02-04T23:50:17.166097Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:17.167717Z","iopub.execute_input":"2025-02-04T23:50:17.168030Z","iopub.status.idle":"2025-02-04T23:50:17.181552Z","shell.execute_reply.started":"2025-02-04T23:50:17.167999Z","shell.execute_reply":"2025-02-04T23:50:17.180860Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"score(solution=valid_sub_gt, submission=valid_sub_hat, row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T23:50:17.182487Z","iopub.execute_input":"2025-02-04T23:50:17.182811Z","iopub.status.idle":"2025-02-04T23:50:17.416282Z","shell.execute_reply.started":"2025-02-04T23:50:17.182777Z","shell.execute_reply":"2025-02-04T23:50:17.415549Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0.40750596564023467"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}