{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9283525b-ba75-467a-8b90-9c190ce7f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('train.parquet')\n",
    "cv = pd.read_csv('cv_split.csv')\n",
    "df = df.merge(cv, on='id', how='left')\n",
    "\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443e4c9f-7dba-4634-a8b0-5b65f78044b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1020177/2364685263.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de39a37-a1fb-49a7-af42-f78be7037717",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = 'microsoft/mdeberta-v3-base'\n",
    "MAX_LEN = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0d2f0e-3850-4750-b6e0-d8ff71910992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abazdyrev/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020ee767-721c-4ae4-867d-f880cb02d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abazdyrev/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n",
    "    tokenized_output = tokenizer(\n",
    "        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n",
    "        truncation=True, padding=False\n",
    "    )\n",
    "    tokens = tokenized_output[\"input_ids\"]\n",
    "    offsets = tokenized_output[\"offset_mapping\"]\n",
    "\n",
    "    # Get subword tokenized versions of the text\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    \n",
    "    # Initialize labels as 'O'\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    if trigger_spans is not None:\n",
    "        # Assign 'TRIGGER' to overlapping tokens\n",
    "        for start, end in trigger_spans:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == 0 and tok_end == 0:\n",
    "                    continue\n",
    "                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n",
    "                    labels[i] = 1\n",
    "\n",
    "    tokenized_output['labels'] = labels\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c668d782-c0ef-42ae-a366-c6a2fe032d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a7128a0b9c41d385c983f6f4586b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774f6e9925ae4a0f9938ca5a013ed680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\n",
    "for column in df.seq_labels.iloc[0].keys():\n",
    "    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n",
    "\n",
    "df_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\n",
    "for column in df_test.seq_labels.iloc[0].keys():\n",
    "    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "df['is_valid'] = df.fold == 4\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\n",
    "ds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\n",
    "ds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content']\n",
    "ds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a1c206-05d8-4fa6-8735-65827a485fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/abazdyrev/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    id2label={0: 0, 1: 1},\n",
    "    label2id={0: 0, 1: 1},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fcc28d-281d-4a1f-931e-a88378bc6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e3b6fad-eacf-427f-b17b-ff9a4224e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='model_checkpoints_mdebertav3_binary',\n",
    "    logging_dir='./model_logs_mdebertav3_binary',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    #bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_torch',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "006bb1d7-e8cf-4b56-8323-14b40552494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbazdyrev99\u001b[0m (\u001b[33mbazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abazdyrev/comps/span-ident/wandb/run-20250203_183000-ur5x5r7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/ur5x5r7g' target=\"_blank\">mdebertav3-binary</a></strong> to <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/ur5x5r7g' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/ur5x5r7g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/ur5x5r7g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x74476da201a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(\n",
    "    project=\"unlp-span-ident-task\",\n",
    "    entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\", \n",
    "    name='mdebertav3-binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2453814229012187"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "positive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\n",
    "positive_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7abe3ad-0619-40d3-90cc-70fd52617d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "def extract_chars_from_spans(spans):\n",
    "    \"\"\"\n",
    "    Given a list of spans (each a tuple (start, end)),\n",
    "    return a set of character indices for all spans.\n",
    "    \"\"\"\n",
    "    char_set = set()\n",
    "    for start, end in spans:\n",
    "        # Each span covers positions start, start+1, ..., end-1.\n",
    "        char_set.update(range(start, end))\n",
    "    return char_set\n",
    "\n",
    "class SpanEvaluationTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any = None,\n",
    "        args: TrainingArguments = None,\n",
    "        data_collator: Any = None,\n",
    "        train_dataset: Any = None,\n",
    "        eval_dataset: Any = None,\n",
    "        tokenizer: Any = None,\n",
    "        desired_positive_ratio: float = 0.25,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer with our custom compute_metrics.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.desired_positive_ratio = desired_positive_ratio\n",
    "\n",
    "    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n",
    "        total_true_chars = 0\n",
    "        total_pred_chars = 0\n",
    "        total_overlap_chars = 0\n",
    "        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n",
    "            if isinstance(true_spans, str):\n",
    "                try:\n",
    "                    true_spans = eval(true_spans)\n",
    "                except Exception:\n",
    "                    true_spans = []\n",
    "                    \n",
    "            # Convert spans to sets of character indices.\n",
    "            true_chars = extract_chars_from_spans(true_spans)\n",
    "            pred_chars = extract_chars_from_spans(pred_spans)\n",
    "            \n",
    "            total_true_chars += len(true_chars)\n",
    "            total_pred_chars += len(pred_chars)\n",
    "            total_overlap_chars += len(true_chars.intersection(pred_chars))\n",
    "            \n",
    "            union_chars = true_chars.union(pred_chars)\n",
    "            \n",
    "        # Compute precision, recall, and F1.\n",
    "        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n",
    "        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _find_optimal_threshold(self, probabilities, labels):\n",
    "        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n",
    "        best_th = 0.5  # Default starting point\n",
    "        best_diff = float(\"inf\")\n",
    "        optimal_th = best_th\n",
    "        \n",
    "        for thold in np.linspace(0.01, 0.99, num=100):\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n",
    "            total = sum([len(prediction) for prediction in true_predictions])\n",
    "            \n",
    "            positive_ratio = total_pos / total if total > 0 else 0\n",
    "            \n",
    "            diff = abs(positive_ratio - self.desired_positive_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                optimal_th = thold\n",
    "        \n",
    "        return optimal_th\n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n",
    "        eval_dataset = self.eval_dataset\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "    \n",
    "        #thresholds = np.linspace(0.1, 0.5, num=41)\n",
    "        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n",
    "        results = []\n",
    "        best_f1 = -1\n",
    "        best_th = 0\n",
    "        best_metrics = None\n",
    "    \n",
    "        for thold in tqdm(thresholds):\n",
    "            # Apply thresholding instead of argmax\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    \n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    \n",
    "            pred_spans_all = []\n",
    "            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n",
    "                samplewise_spans = []\n",
    "                current_span = None\n",
    "                for token_label, span in zip(pred, offsets):\n",
    "                    if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                        if current_span is None:\n",
    "                            current_span = [span[0], span[1]]  # Start a new span\n",
    "                        else:\n",
    "                            current_span[1] = span[1]  # Extend the span to include the current token\n",
    "                    else:  # If token_label == 0 (not an entity)\n",
    "                        if current_span is not None:\n",
    "                            samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                            current_span = None  # Reset for the next entity\n",
    "    \n",
    "                # If the last token was part of a span, save it\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))\n",
    "    \n",
    "                pred_spans_all.append(samplewise_spans)\n",
    "    \n",
    "            # Store results for this threshold\n",
    "            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n",
    "            if current_metrics['f1'] >= best_f1:\n",
    "                best_f1 = current_metrics['f1']\n",
    "                best_th = thold\n",
    "                best_metrics = current_metrics\n",
    "                best_metrics['thold'] = thold\n",
    "                \n",
    "            \n",
    "            results.append(current_metrics)\n",
    "        return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e33e84-5eac-48fe-948e-f53acd597002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1020177/3170772390.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/955 09:01 < 08:12, 0.92 it/s, Epoch 2.61/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Thold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.569776</td>\n",
       "      <td>0.594370</td>\n",
       "      <td>0.573655</td>\n",
       "      <td>0.583829</td>\n",
       "      <td>0.118889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.442804</td>\n",
       "      <td>0.611672</td>\n",
       "      <td>0.587546</td>\n",
       "      <td>0.599366</td>\n",
       "      <td>0.346566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.416488</td>\n",
       "      <td>0.611471</td>\n",
       "      <td>0.597373</td>\n",
       "      <td>0.604340</td>\n",
       "      <td>0.415859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.470250</td>\n",
       "      <td>0.610776</td>\n",
       "      <td>0.597135</td>\n",
       "      <td>0.603879</td>\n",
       "      <td>0.396061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.469184</td>\n",
       "      <td>0.602244</td>\n",
       "      <td>0.585620</td>\n",
       "      <td>0.593816</td>\n",
       "      <td>0.514848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f91d9dddd34baf8f88fc108a27d0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca26cc6a6214786ad4503faf69bca2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85db8736d0142819a952f56ca8e1fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d6d6e1ef0045ca80ecbed731f46e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570baa5f04564ba4b2afd212ca5e08af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SpanEvaluationTrainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2589\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(\n\u001b[1;32m   2590\u001b[0m         tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3054\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3051\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3054\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial)\n\u001b[1;32m   3055\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3186\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3184\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3185\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3189\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3806\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(output_dir)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3910\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3908\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\n\u001b[1;32m   3911\u001b[0m         output_dir, state_dict\u001b[38;5;241m=\u001b[39mstate_dict, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors\n\u001b[1;32m   3912\u001b[0m     )\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3915\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3034\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3029\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   3031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 3034\u001b[0m     safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3036\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SpanEvaluationTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    desired_positive_ratio=positive_class_balance\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae539e8e-d6d5-469d-bf7b-5bcacb0eb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_MODEL = 'model_checkpoints_mdebertav3_binary/checkpoint-300'\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    FINETUNED_MODEL,\n",
    "    id2label={0: 0, 1: 1},\n",
    "    label2id={0: 0, 1: 1},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3913ada-b31b-404f-9745-7babf156ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4800edb9-d35d-4289-ad21-b0672254edd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b2fc23488a4f2fb45bcdb8e96393ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_preds = trainer.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92bd64bc-afd9-4a8a-8de4-514824f6d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((764, 1516, 2), (764, 1516))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.predictions.shape, valid_preds.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc734f81-5211-4312-b248-f1da909f69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3032d660cf348b88b3092a7621013fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6114706112274058,\n",
       " 'recall': 0.5973728425232931,\n",
       " 'f1': 0.6043395214078764,\n",
       " 'thold': 0.41585858585858587}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b673f-e45c-4cbc-abf9-3aac3528c101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19d3bd-9b97-43b7-baad-35aabe26476f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24406db-2045-40a2-b3bf-e4ba50ae18ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdd22ce-5c37-41b6-bf6d-ab1e50791687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49683be1982459c94df025ef121ad91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23204c4d-70aa-4b44-9edd-a62794e4be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e950513f-2a4d-4e6b-ac59-6e27836023a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40595959595959596"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9bec8dd-19c1-441b-b467-aa0238dc1af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.410875"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_th = (0.4158+0.40595)/2\n",
    "final_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "042dc4ff-378c-46ef-bbe4-7613de11f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_aggregation(probabilities, labels, offset_mappings, thold):\n",
    "    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    pred_spans_all = []\n",
    "    for pred, offsets in zip(true_predictions, offset_mappings):\n",
    "        samplewise_spans = []\n",
    "        current_span = None\n",
    "        for token_label, span in zip(pred, offsets):\n",
    "            if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                if current_span is None:\n",
    "                    current_span = [span[0], span[1]]  # Start a new span\n",
    "                else:\n",
    "                    current_span[1] = span[1]  # Extend the span to include the current token\n",
    "            else:  # If token_label == 0 (not an entity)\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                    current_span = None  # Reset for the next entity\n",
    "        \n",
    "                    # If the last token was part of a span, save it\n",
    "        if current_span is not None:\n",
    "            samplewise_spans.append(tuple(current_span))\n",
    "        \n",
    "        pred_spans_all.append(samplewise_spans)\n",
    "    return [str(row) for row in pred_spans_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "537572ee-be33-4911-bc82-8abd6b9d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdc10345-650a-4e78-8e7b-2f923faefe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Custom exception for participant-visible errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute span-level F1 score based on overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n",
    "    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n",
    "    - row_id_column_name (str): Column name for the row identifier.\n",
    "\n",
    "    Returns:\n",
    "    - float: The token-level weighted F1 score.\n",
    "\n",
    "    Example:\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n",
    "    ... })\n",
    "    >>> score(solution, submission, \"id\")\n",
    "    0.16296296296296295\n",
    "    \"\"\"\n",
    "    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    \n",
    "    def safe_parse_spans(trigger_words):\n",
    "        if isinstance(trigger_words, str):\n",
    "            try:\n",
    "                return ast.literal_eval(trigger_words)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n",
    "            return trigger_words\n",
    "        return []\n",
    "\n",
    "    def extract_tokens_from_spans(spans):\n",
    "        tokens = set()\n",
    "        for start, end in spans:\n",
    "            tokens.update(range(start, end))\n",
    "        return tokens\n",
    "    \n",
    "    solution = solution.copy()\n",
    "    submission = submission.copy()\n",
    "\n",
    "    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n",
    "    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n",
    "\n",
    "    merged = pd.merge(\n",
    "        solution,\n",
    "        submission,\n",
    "        on=\"id\",\n",
    "        suffixes=(\"_solution\", \"_submission\")\n",
    "    )\n",
    "\n",
    "    total_true_tokens = 0\n",
    "    total_pred_tokens = 0\n",
    "    overlapping_tokens = 0\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        true_spans = row[\"trigger_words_solution\"]\n",
    "        pred_spans = row[\"trigger_words_submission\"]\n",
    "\n",
    "        true_tokens = extract_tokens_from_spans(true_spans)\n",
    "        pred_tokens = extract_tokens_from_spans(pred_spans)\n",
    "\n",
    "        total_true_tokens += len(true_tokens)\n",
    "        total_pred_tokens += len(pred_tokens)\n",
    "        overlapping_tokens += len(true_tokens & pred_tokens)\n",
    "\n",
    "    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n",
    "    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffbb019a-659b-4bd8-a84f-b1c3ceb5698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6054884124996806"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_pred = deepcopy(df_gt)\n",
    "df_pred['trigger_words'] = valid_results\n",
    "score(df_gt, df_pred, row_id_column_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4a952c8-9492-4b3a-819b-a061f03657d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2f59c7e-a282-4609-921e-b6879978818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('sample_submission.csv')\n",
    "ss['trigger_words'] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e832f9a-ef93-4400-9b67-5bf843a90205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submissions/mdebertav3-binary-cv0.605.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fa661-1641-42f0-8752-8ed8ab006bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
