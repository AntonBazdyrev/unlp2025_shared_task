{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9283525b-ba75-467a-8b90-9c190ce7f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('train.parquet')\n",
    "cv = pd.read_csv('cv_split.csv')\n",
    "df = df.merge(cv, on='id', how='left')\n",
    "\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443e4c9f-7dba-4634-a8b0-5b65f78044b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1349145/2364685263.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de39a37-a1fb-49a7-af42-f78be7037717",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked'\n",
    "MAX_LEN = 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0d2f0e-3850-4750-b6e0-d8ff71910992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020ee767-721c-4ae4-867d-f880cb02d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n",
    "    tokenized_output = tokenizer(\n",
    "        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n",
    "        truncation=True, padding=False\n",
    "    )\n",
    "    tokens = tokenized_output[\"input_ids\"]\n",
    "    offsets = tokenized_output[\"offset_mapping\"]\n",
    "\n",
    "    # Get subword tokenized versions of the text\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    \n",
    "    # Initialize labels as 'O'\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    if trigger_spans is not None:\n",
    "        # Assign 'TRIGGER' to overlapping tokens\n",
    "        for start, end in trigger_spans:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == 0 and tok_end == 0:\n",
    "                    continue\n",
    "                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n",
    "                    labels[i] = 1\n",
    "\n",
    "    tokenized_output['labels'] = labels\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c668d782-c0ef-42ae-a366-c6a2fe032d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b985c162b6464eaf5d1d2b278b06f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d646c04a3f4c71878584c76ad5fc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\n",
    "for column in df.seq_labels.iloc[0].keys():\n",
    "    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n",
    "\n",
    "df_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\n",
    "for column in df_test.seq_labels.iloc[0].keys():\n",
    "    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "df['is_valid'] = df.fold == 4\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\n",
    "ds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\n",
    "ds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content']\n",
    "ds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438d0ad1-c4d7-4026-9ca4-632d8ac659a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7607dac0dbca4228ac88933df4c259d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at /home/abazdyrev/pretrained_models/gemma-2-27b-it/ and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 456,729,602 || all params: 27,683,867,140 || trainable%: 1.6498\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Gemma2ForTokenClassification, LlamaForTokenClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "id2label = {0: 0, 1: 1}\n",
    "label2id = {0: 0, 1: 1}\n",
    "\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Gemma2ForTokenClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=nf4_config\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=32, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout=0.05, \n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ") \n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fcc28d-281d-4a1f-931e-a88378bc6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3b6fad-eacf-427f-b17b-ff9a4224e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='model_checkpoints_gemma2_27b_binary',\n",
    "    logging_dir='./model_logs_gemma2_27b_binary',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_8bit',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006bb1d7-e8cf-4b56-8323-14b40552494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/7tduj0x1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7df72919e4e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(\n",
    "    project=\"unlp-span-ident-task\",\n",
    "    entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\", \n",
    "    name='gemma2-27b-binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22925695654588976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "positive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\n",
    "positive_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7abe3ad-0619-40d3-90cc-70fd52617d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "def extract_chars_from_spans(spans):\n",
    "    \"\"\"\n",
    "    Given a list of spans (each a tuple (start, end)),\n",
    "    return a set of character indices for all spans.\n",
    "    \"\"\"\n",
    "    char_set = set()\n",
    "    for start, end in spans:\n",
    "        # Each span covers positions start, start+1, ..., end-1.\n",
    "        char_set.update(range(start, end))\n",
    "    return char_set\n",
    "\n",
    "class SpanEvaluationTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any = None,\n",
    "        args: TrainingArguments = None,\n",
    "        data_collator: Any = None,\n",
    "        train_dataset: Any = None,\n",
    "        eval_dataset: Any = None,\n",
    "        tokenizer: Any = None,\n",
    "        desired_positive_ratio: float = 0.25,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer with our custom compute_metrics.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.desired_positive_ratio = desired_positive_ratio\n",
    "\n",
    "    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n",
    "        total_true_chars = 0\n",
    "        total_pred_chars = 0\n",
    "        total_overlap_chars = 0\n",
    "        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n",
    "            if isinstance(true_spans, str):\n",
    "                try:\n",
    "                    true_spans = eval(true_spans)\n",
    "                except Exception:\n",
    "                    true_spans = []\n",
    "                    \n",
    "            # Convert spans to sets of character indices.\n",
    "            true_chars = extract_chars_from_spans(true_spans)\n",
    "            pred_chars = extract_chars_from_spans(pred_spans)\n",
    "            \n",
    "            total_true_chars += len(true_chars)\n",
    "            total_pred_chars += len(pred_chars)\n",
    "            total_overlap_chars += len(true_chars.intersection(pred_chars))\n",
    "            \n",
    "            union_chars = true_chars.union(pred_chars)\n",
    "            \n",
    "        # Compute precision, recall, and F1.\n",
    "        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n",
    "        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _find_optimal_threshold(self, probabilities, labels):\n",
    "        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n",
    "        best_th = 0.5  # Default starting point\n",
    "        best_diff = float(\"inf\")\n",
    "        optimal_th = best_th\n",
    "        \n",
    "        for thold in np.linspace(0.01, 0.99, num=100):\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n",
    "            total = sum([len(prediction) for prediction in true_predictions])\n",
    "            \n",
    "            positive_ratio = total_pos / total if total > 0 else 0\n",
    "            \n",
    "            diff = abs(positive_ratio - self.desired_positive_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                optimal_th = thold\n",
    "        \n",
    "        return optimal_th\n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n",
    "        eval_dataset = self.eval_dataset\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "    \n",
    "        #thresholds = np.linspace(0.1, 0.5, num=41)\n",
    "        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n",
    "        results = []\n",
    "        best_f1 = -1\n",
    "        best_th = 0\n",
    "        best_metrics = None\n",
    "    \n",
    "        for thold in tqdm(thresholds):\n",
    "            # Apply thresholding instead of argmax\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    \n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    \n",
    "            pred_spans_all = []\n",
    "            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n",
    "                samplewise_spans = []\n",
    "                current_span = None\n",
    "                for token_label, span in zip(pred, offsets):\n",
    "                    if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                        if current_span is None:\n",
    "                            current_span = [span[0], span[1]]  # Start a new span\n",
    "                        else:\n",
    "                            current_span[1] = span[1]  # Extend the span to include the current token\n",
    "                    else:  # If token_label == 0 (not an entity)\n",
    "                        if current_span is not None:\n",
    "                            samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                            current_span = None  # Reset for the next entity\n",
    "    \n",
    "                # If the last token was part of a span, save it\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))\n",
    "    \n",
    "                pred_spans_all.append(samplewise_spans)\n",
    "    \n",
    "            # Store results for this threshold\n",
    "            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n",
    "            if current_metrics['f1'] >= best_f1:\n",
    "                best_f1 = current_metrics['f1']\n",
    "                best_th = thold\n",
    "                best_metrics = current_metrics\n",
    "                best_metrics['thold'] = thold\n",
    "                \n",
    "            \n",
    "            results.append(current_metrics)\n",
    "        return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e33e84-5eac-48fe-948e-f53acd597002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051059/3170772390.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SpanEvaluationTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    desired_positive_ratio=positive_class_balance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835e231-e15e-4299-b4b1-b47d464db8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4dbe9-e9bb-45cf-b13c-bb19bff01d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e54265-24e4-4445-abf5-1fd43614e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c372773dced4153b4ff43ad0e41790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at /home/abazdyrev/pretrained_models/gemma-2-27b-it/ and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,218 || all params: 27,683,867,140 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Gemma2ForTokenClassification, BitsAndBytesConfig\n",
    "from peft import PeftModel, get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Gemma2ForTokenClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=nf4_config\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./model_checkpoints_gemma2_27b_binary/checkpoint-600/\")\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3913ada-b31b-404f-9745-7babf156ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4800edb9-d35d-4289-ad21-b0672254edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6e4ef72f204107916f07e68012925e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    }
   ],
   "source": [
    "valid_preds = trainer.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92bd64bc-afd9-4a8a-8de4-514824f6d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((764, 1440, 2), (764, 1440))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.predictions.shape, valid_preds.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc734f81-5211-4312-b248-f1da909f69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec683a0c15134ce59fa7e83ca0db9a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6157670394598016,\n",
       " 'recall': 0.6190536802267366,\n",
       " 'f1': 0.6174059859259228,\n",
       " 'thold': 0.45545454545454545}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfdd22ce-5c37-41b6-bf6d-ab1e50791687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd8b4a213eb453e9fac7c00ccaed30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23204c4d-70aa-4b44-9edd-a62794e4be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e950513f-2a4d-4e6b-ac59-6e27836023a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4257575757575757"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9bec8dd-19c1-441b-b467-aa0238dc1af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29057875"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_th = (0.4554+0.4257575)/2 - 0.15\n",
    "final_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "042dc4ff-378c-46ef-bbe4-7613de11f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_aggregation(probabilities, labels, offset_mappings, thold):\n",
    "    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    pred_spans_all = []\n",
    "    for pred, offsets in zip(true_predictions, offset_mappings):\n",
    "        samplewise_spans = []\n",
    "        current_span = None\n",
    "        for token_label, span in zip(pred, offsets):\n",
    "            if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                if current_span is None:\n",
    "                    current_span = [span[0], span[1]]  # Start a new span\n",
    "                else:\n",
    "                    current_span[1] = span[1]  # Extend the span to include the current token\n",
    "            else:  # If token_label == 0 (not an entity)\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                    current_span = None  # Reset for the next entity\n",
    "        \n",
    "                    # If the last token was part of a span, save it\n",
    "        if current_span is not None:\n",
    "            samplewise_spans.append(tuple(current_span))\n",
    "        \n",
    "        pred_spans_all.append(samplewise_spans)\n",
    "    return [str(row) for row in pred_spans_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "537572ee-be33-4911-bc82-8abd6b9d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdc10345-650a-4e78-8e7b-2f923faefe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Custom exception for participant-visible errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute span-level F1 score based on overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n",
    "    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n",
    "    - row_id_column_name (str): Column name for the row identifier.\n",
    "\n",
    "    Returns:\n",
    "    - float: The token-level weighted F1 score.\n",
    "\n",
    "    Example:\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n",
    "    ... })\n",
    "    >>> score(solution, submission, \"id\")\n",
    "    0.16296296296296295\n",
    "    \"\"\"\n",
    "    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    \n",
    "    def safe_parse_spans(trigger_words):\n",
    "        if isinstance(trigger_words, str):\n",
    "            try:\n",
    "                return ast.literal_eval(trigger_words)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n",
    "            return trigger_words\n",
    "        return []\n",
    "\n",
    "    def extract_tokens_from_spans(spans):\n",
    "        tokens = set()\n",
    "        for start, end in spans:\n",
    "            tokens.update(range(start, end))\n",
    "        return tokens\n",
    "    \n",
    "    solution = solution.copy()\n",
    "    submission = submission.copy()\n",
    "\n",
    "    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n",
    "    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n",
    "\n",
    "    merged = pd.merge(\n",
    "        solution,\n",
    "        submission,\n",
    "        on=\"id\",\n",
    "        suffixes=(\"_solution\", \"_submission\")\n",
    "    )\n",
    "\n",
    "    total_true_tokens = 0\n",
    "    total_pred_tokens = 0\n",
    "    overlapping_tokens = 0\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        true_spans = row[\"trigger_words_solution\"]\n",
    "        pred_spans = row[\"trigger_words_submission\"]\n",
    "\n",
    "        true_tokens = extract_tokens_from_spans(true_spans)\n",
    "        pred_tokens = extract_tokens_from_spans(pred_spans)\n",
    "\n",
    "        total_true_tokens += len(true_tokens)\n",
    "        total_pred_tokens += len(pred_tokens)\n",
    "        overlapping_tokens += len(true_tokens & pred_tokens)\n",
    "\n",
    "    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n",
    "    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffbb019a-659b-4bd8-a84f-b1c3ceb5698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627367884288067"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_pred = deepcopy(df_gt)\n",
    "df_pred['trigger_words'] = valid_results\n",
    "score(df_gt, df_pred, row_id_column_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4a952c8-9492-4b3a-819b-a061f03657d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2f59c7e-a282-4609-921e-b6879978818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('sample_submission.csv')\n",
    "ss['trigger_words'] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e832f9a-ef93-4400-9b67-5bf843a90205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submissions/gemma2-27b-binary-cv0.627blackmagic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fa661-1641-42f0-8752-8ed8ab006bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43edf7bc-5a95-4174-984b-4cfabce5252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(valid_preds, open('local_preds/valid_preds_gemma2_binary.pkl', 'wb'))\n",
    "pickle.dump(test_preds, open('local_preds/test_preds_gemma2_binary.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63083323-8398-4291-8f8a-f4cfa54dfe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
