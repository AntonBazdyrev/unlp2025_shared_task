{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10664686,"sourceType":"datasetVersion","datasetId":6604871}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d60b7330-f0f2-418f-a6d8-055ca6e82d53","cell_type":"code","source":"!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:46:42.556198Z","iopub.execute_input":"2025-02-27T16:46:42.556486Z","iopub.status.idle":"2025-02-27T16:46:57.945987Z","shell.execute_reply.started":"2025-02-27T16:46:42.556454Z","shell.execute_reply":"2025-02-27T16:46:57.945128Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers>=4.42.3 in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers>=4.42.3\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting accelerate\n  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.3) (4.67.1)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.42.3) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.42.3) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.42.3) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.3) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.3) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.3) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.42.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.42.3) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.42.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.42.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers>=4.42.3) (2024.2.0)\nDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, accelerate, bitsandbytes\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.4.0 bitsandbytes-0.45.3 transformers-4.49.0\n","output_type":"stream"}],"execution_count":1},{"id":"2a31f4b5-454e-4fad-94fe-2612578f11de","cell_type":"code","source":"import os\nimport copy\nimport random\nfrom dataclasses import dataclass\nfrom tqdm.autonotebook import tqdm\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    \n    AutoTokenizer, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:46:57.947018Z","iopub.execute_input":"2025-02-27T16:46:57.947340Z","iopub.status.idle":"2025-02-27T16:47:19.768424Z","shell.execute_reply.started":"2025-02-27T16:46:57.947310Z","shell.execute_reply":"2025-02-27T16:47:19.767571Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-115343807469>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}],"execution_count":2},{"id":"192f7edd-5297-49cc-aec3-c3e94791eaff","cell_type":"markdown","source":"# Configurations","metadata":{}},{"id":"84cf43aa-6e13-4b33-8940-2eb5009bb598","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n@dataclass\nclass Config:\n    data_path: str = '/kaggle/input/unlp-2025-shared-task-span-identification/'\n    cv_path: str = \"/kaggle/input/unlp25-cross-validation-split/cv_split.csv\"\n    \n    pretrained: str = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n    max_length: int = 2048\n\n    hugginface_key: str = user_secrets.get_secret(\"hugginface_key\")\n    wandb_key: str = user_secrets.get_secret(\"wandb_key\")\n    wandb_init_args = {\n        'project': \"unlp-span-ident-task\",\n        'entity': \"ivan-havlytskyiz\",\n        'name': \"unsloth-gemma-2-2b-it-bnb-4bit\"\n    }\n\n    lora_args = {\n        'r': 16,\n        'bias': \"none\",\n        'lora_alpha': 32,\n        'lora_dropout': 0.05,\n        # 'layers_to_transform': list(range(16, 42))\n    }\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:19.769331Z","iopub.execute_input":"2025-02-27T16:47:19.769915Z","iopub.status.idle":"2025-02-27T16:47:20.125335Z","shell.execute_reply.started":"2025-02-27T16:47:19.769881Z","shell.execute_reply":"2025-02-27T16:47:20.124507Z"}},"outputs":[],"execution_count":3},{"id":"3dc1afcf-b343-4038-b61e-aef2384e86e8","cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)\ntqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:20.126247Z","iopub.execute_input":"2025-02-27T16:47:20.126550Z","iopub.status.idle":"2025-02-27T16:47:20.198000Z","shell.execute_reply.started":"2025-02-27T16:47:20.126522Z","shell.execute_reply":"2025-02-27T16:47:20.197390Z"}},"outputs":[],"execution_count":4},{"id":"27b6b2e2-24af-4985-b18e-de7202fed5d9","cell_type":"code","source":"import huggingface_hub\nimport wandb\n\nhuggingface_hub.login(token=config.hugginface_key)\nwandb.login(key=config.wandb_key)\n\nwandb.init(**config.wandb_init_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:20.198748Z","iopub.execute_input":"2025-02-27T16:47:20.199046Z","iopub.status.idle":"2025-02-27T16:47:36.116942Z","shell.execute_reply.started":"2025-02-27T16:47:20.199018Z","shell.execute_reply":"2025-02-27T16:47:36.116083Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-havlytskyi\u001b[0m (\u001b[33mivan-havlytskyiz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250227_164729-hwr3mzp4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/hwr3mzp4' target=\"_blank\">unsloth-gemma-2-2b-it-bnb-4bit</a></strong> to <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/hwr3mzp4' target=\"_blank\">https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/hwr3mzp4</a>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ivan-havlytskyiz/unlp-span-ident-task/runs/hwr3mzp4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78395751b880>"},"metadata":{}}],"execution_count":5},{"id":"e9b0a9ab-1388-47e8-b169-0440b14bdce4","cell_type":"markdown","source":"# Training Arguments","metadata":{}},{"id":"5a5d1f7a-52b6-427a-89b5-c363d857db62","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'./model_checkpoints_{config.wandb_init_args[\"name\"]}',\n    logging_dir=f'./model_logs_{config.wandb_init_args[\"name\"]}',\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.0,\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    # bf16=True,\n    report_to=\"wandb\",\n    optim='adamw_8bit',\n    eval_strategy='steps',\n    save_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=20,\n    save_steps=200,\n    save_total_limit=10,\n    metric_for_best_model='eval_f1',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:36.119382Z","iopub.execute_input":"2025-02-27T16:47:36.119619Z","iopub.status.idle":"2025-02-27T16:47:36.174149Z","shell.execute_reply.started":"2025-02-27T16:47:36.119600Z","shell.execute_reply":"2025-02-27T16:47:36.173541Z"}},"outputs":[],"execution_count":6},{"id":"11588d69-5334-4c5b-a1a7-d460ba57bd79","cell_type":"markdown","source":"# LoRA config","metadata":{}},{"id":"79735da9-8cef-42ba-adcf-c715af343db6","cell_type":"code","source":"lora_config = LoraConfig(\n    **config.lora_args,\n    # only target self-attention\n    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\"],\n    task_type=TaskType.TOKEN_CLS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:36.177664Z","iopub.execute_input":"2025-02-27T16:47:36.177953Z","iopub.status.idle":"2025-02-27T16:47:36.182309Z","shell.execute_reply.started":"2025-02-27T16:47:36.177924Z","shell.execute_reply":"2025-02-27T16:47:36.181458Z"}},"outputs":[],"execution_count":7},{"id":"e3c6e19e-3769-434e-845b-d364b48915c1","cell_type":"markdown","source":"# Instantiate the tokenizer & model","metadata":{}},{"id":"409ea5fb-ceee-4dde-8cb6-2c82fb91e81d","cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.pretrained)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:36.183111Z","iopub.execute_input":"2025-02-27T16:47:36.183334Z","iopub.status.idle":"2025-02-27T16:47:38.554316Z","shell.execute_reply.started":"2025-02-27T16:47:36.183315Z","shell.execute_reply":"2025-02-27T16:47:38.553573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1b5d6c6ad74d09b16ea9f5ec2eee2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee0fb3ad0b54e7d963e90cfca2a83c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2b9fc842e14b409cb7f4b58da61999"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da674372e40a48d695e3aba0d492ce36"}},"metadata":{}}],"execution_count":8},{"id":"86c2a72a-5f50-44fa-916e-d5fd280038e2","cell_type":"code","source":"model = Gemma2ForTokenClassification.from_pretrained(\n    config.pretrained,\n    num_labels=2,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:38.555292Z","iopub.execute_input":"2025-02-27T16:47:38.555614Z","iopub.status.idle":"2025-02-27T16:47:52.821078Z","shell.execute_reply.started":"2025-02-27T16:47:38.555567Z","shell.execute_reply":"2025-02-27T16:47:52.820319Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4828b538f543798294db1e09ef6722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918c0d12dadf4c0ca44163d473f6a630"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at unsloth/gemma-2-2b-it-bnb-4bit and are newly initialized: ['score.bias', 'score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 11,186,690 || all params: 2,625,533,188 || trainable%: 0.4261\n","output_type":"stream"}],"execution_count":9},{"id":"d44ba491-f4c9-40f4-84c8-0f2fec080ce9","cell_type":"markdown","source":"# Data","metadata":{}},{"id":"01d1c0dd-e022-4b0a-ae48-1bf4915e3e03","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(config.data_path + \"train.parquet\")\ncv = pd.read_csv(config.cv_path)\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(config.data_path + \"test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:52.821972Z","iopub.execute_input":"2025-02-27T16:47:52.822268Z","iopub.status.idle":"2025-02-27T16:47:53.149738Z","shell.execute_reply.started":"2025-02-27T16:47:52.822238Z","shell.execute_reply":"2025-02-27T16:47:53.148794Z"}},"outputs":[],"execution_count":10},{"id":"020ee767-721c-4ae4-867d-f880cb02d398","cell_type":"code","source":"def convert_to_seq_labeling(text, tokenizer, max_length=None, trigger_spans=None):\n    tokenized_output = tokenizer(\n        text,\n        return_offsets_mapping=True,\n        add_special_tokens=True,\n        \n        max_length=max_length,\n        truncation=(max_length is not None),\n        padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n\n    # Get subword tokenized versions of the text\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    \n    # Initialize labels as 'O'\n    labels = [0] * len(tokens)\n\n    if trigger_spans is not None:\n        # Assign 'TRIGGER' to overlapping tokens\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n                    labels[i] = 1\n\n    tokenized_output['labels'] = labels\n    return tokenized_output\n\n\ndef preprocess_df(df, max_length):\n    \"\"\"Modified processing incorporating trigger span handling\"\"\"\n    tqdm.pandas()\n    \n    df['seq_labels'] = df.progress_apply(\n        lambda row: convert_to_seq_labeling(\n            text=row['content'],\n            tokenizer=tokenizer,\n            trigger_spans=row.get('trigger_words', None),  # Handle both validation and test cases\n            max_length=max_length\n        ),\n        axis=1\n    )\n    \n    # Extract all tokenizer outputs\n    for column in df.seq_labels.iloc[0].keys():\n        df[column] = df.seq_labels.apply(lambda x: x.get(column))\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:53.150560Z","iopub.execute_input":"2025-02-27T16:47:53.150838Z","iopub.status.idle":"2025-02-27T16:47:53.158666Z","shell.execute_reply.started":"2025-02-27T16:47:53.150816Z","shell.execute_reply":"2025-02-27T16:47:53.157549Z"}},"outputs":[],"execution_count":11},{"id":"c668d782-c0ef-42ae-a366-c6a2fe032d25","cell_type":"code","source":"df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)\n\nis_valid_mask = (df.fold == 4)\ndf_train = df[~is_valid_mask].copy()\ndf_valid = df[is_valid_mask].copy()\n\n\ndf_train = preprocess_df(df_train, max_length=config.max_length)\ndf_valid = preprocess_df(df_valid, max_length=None)\ndf_test = preprocess_df(df_test, max_length=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:47:53.159485Z","iopub.execute_input":"2025-02-27T16:47:53.159753Z","iopub.status.idle":"2025-02-27T16:48:01.482719Z","shell.execute_reply.started":"2025-02-27T16:47:53.159721Z","shell.execute_reply":"2025-02-27T16:48:01.481784Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3058 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b4c96de4c0842bc82cb0e570df697f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/764 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1126c810fa4f48bdb64a792c1a72164d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e269596f924eaeabdb2f15d6f8c26c"}},"metadata":{}}],"execution_count":12},{"id":"3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f","cell_type":"code","source":"train_columns = list(df_train.seq_labels.iloc[0].keys()) +\\\n                ['content', 'trigger_words']\ntest_columns = list(df_train.seq_labels.iloc[0].keys()) + ['content']\n\nds_train = Dataset.from_pandas(df_train[train_columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(df_valid[train_columns].reset_index(drop=True))\nds_test = Dataset.from_pandas(df_test[test_columns].reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:48:01.483528Z","iopub.execute_input":"2025-02-27T16:48:01.483873Z","iopub.status.idle":"2025-02-27T16:48:03.292514Z","shell.execute_reply.started":"2025-02-27T16:48:01.483837Z","shell.execute_reply":"2025-02-27T16:48:03.291113Z"}},"outputs":[],"execution_count":13},{"id":"9a72c32e-e752-4c16-be89-cb06bfbd1dd3","cell_type":"markdown","source":"# Custom Trainer","metadata":{}},{"id":"dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5","cell_type":"code","source":"from itertools import chain\n\ntrain_labels = df_train.labels.tolist() + df_valid.labels.tolist()\npositive_class_balance = pd.Series(list(chain(*train_labels))).mean()\npositive_class_balance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:48:03.293311Z","iopub.execute_input":"2025-02-27T16:48:03.293655Z","iopub.status.idle":"2025-02-27T16:48:05.560207Z","shell.execute_reply.started":"2025-02-27T16:48:03.293625Z","shell.execute_reply":"2025-02-27T16:48:05.559491Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0.22813063581288748"},"metadata":{}}],"execution_count":14},{"id":"d7abe3ad-0619-40d3-90cc-70fd52617d8b","cell_type":"code","source":"import math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\nclass SpanEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        desired_positive_ratio: float = 0.25,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.desired_positive_ratio = desired_positive_ratio\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.desired_positive_ratio)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th\n        \n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n        eval_dataset = self.eval_dataset\n        logits, labels = eval_pred\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:48:05.561044Z","iopub.execute_input":"2025-02-27T16:48:05.561290Z","iopub.status.idle":"2025-02-27T16:48:05.694963Z","shell.execute_reply.started":"2025-02-27T16:48:05.561269Z","shell.execute_reply":"2025-02-27T16:48:05.694298Z"}},"outputs":[],"execution_count":15},{"id":"e2e33e84-5eac-48fe-948e-f53acd597002","cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = SpanEvaluationTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    desired_positive_ratio=positive_class_balance\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:48:05.695758Z","iopub.execute_input":"2025-02-27T16:48:05.695971Z","iopub.status.idle":"2025-02-27T16:48:05.712530Z","shell.execute_reply.started":"2025-02-27T16:48:05.695952Z","shell.execute_reply":"2025-02-27T16:48:05.711938Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-1175221e7f1c>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":16},{"id":"9835e231-e15e-4299-b4b1-b47d464db8c0","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:48:05.713267Z","iopub.execute_input":"2025-02-27T16:48:05.713557Z","iopub.status.idle":"2025-02-27T19:41:44.734295Z","shell.execute_reply.started":"2025-02-27T16:48:05.713526Z","shell.execute_reply":"2025-02-27T19:41:44.732851Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1217' max='1910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1217/1910 2:53:27 < 1:38:56, 0.12 it/s, Epoch 3.18/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Thold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.721400</td>\n      <td>0.566400</td>\n      <td>0.547641</td>\n      <td>0.518643</td>\n      <td>0.532748</td>\n      <td>0.178283</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.389100</td>\n      <td>0.411841</td>\n      <td>0.578913</td>\n      <td>0.570940</td>\n      <td>0.574899</td>\n      <td>0.445556</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.415600</td>\n      <td>0.423681</td>\n      <td>0.589653</td>\n      <td>0.581912</td>\n      <td>0.585757</td>\n      <td>0.584141</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.426600</td>\n      <td>0.406309</td>\n      <td>0.588484</td>\n      <td>0.581997</td>\n      <td>0.585222</td>\n      <td>0.475253</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.391300</td>\n      <td>0.411222</td>\n      <td>0.592728</td>\n      <td>0.574063</td>\n      <td>0.583246</td>\n      <td>0.534646</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.358100</td>\n      <td>0.430200</td>\n      <td>0.586475</td>\n      <td>0.572680</td>\n      <td>0.579495</td>\n      <td>0.455455</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1434/1434 25:37]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f17d13a2eb049a999132356573e22a3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c70d5e283ec4faa81354f1e6b437d99"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5bae141da62405fb6b3a831b882e0c0"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f6359a5d50c42029774847faadbefd6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8414b65fc51d43799bbbcd3a24df798d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47dceb29dddb4e1dae3999d32491c365"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3738\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"id":"b9e4dbe9-e9bb-45cf-b13c-bb19bff01d58","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a5ed6002-164d-421a-8278-c049501a684c","cell_type":"markdown","source":"# Predict","metadata":{}},{"id":"63083323-8398-4291-8f8a-f4cfa54dfe4c","cell_type":"code","source":"FINETUNED_MODEL = '/kaggle/working/model_checkpoints_unsloth-gemma-2-2b-it-bnb-4bit/checkpoint-600'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:45:43.239080Z","iopub.execute_input":"2025-02-27T20:45:43.239374Z","iopub.status.idle":"2025-02-27T20:45:43.243807Z","shell.execute_reply.started":"2025-02-27T20:45:43.239352Z","shell.execute_reply":"2025-02-27T20:45:43.242955Z"}},"outputs":[],"execution_count":67},{"id":"a5b811ad-7174-443e-8534-ec52139812fc","cell_type":"code","source":"trainer._load_from_checkpoint(FINETUNED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:45:43.894094Z","iopub.execute_input":"2025-02-27T20:45:43.894410Z","iopub.status.idle":"2025-02-27T20:45:43.976599Z","shell.execute_reply.started":"2025-02-27T20:45:43.894385Z","shell.execute_reply":"2025-02-27T20:45:43.975722Z"}},"outputs":[],"execution_count":68},{"id":"f5c56b18-9259-4fe4-b58b-d687f4c9c5c1","cell_type":"code","source":"valid_preds = trainer.predict(ds_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:45:46.616480Z","iopub.execute_input":"2025-02-27T20:45:46.616863Z","iopub.status.idle":"2025-02-27T20:49:31.426042Z","shell.execute_reply.started":"2025-02-27T20:45:46.616833Z","shell.execute_reply":"2025-02-27T20:49:31.425337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6642452c60204c5c8a4f89b7f25e3152"}},"metadata":{}}],"execution_count":69},{"id":"bc10ce25-7d51-44f6-88ae-ae44b8a88128","cell_type":"code","source":"trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:49:31.427056Z","iopub.execute_input":"2025-02-27T20:49:31.427328Z","iopub.status.idle":"2025-02-27T20:49:49.972503Z","shell.execute_reply.started":"2025-02-27T20:49:31.427306Z","shell.execute_reply":"2025-02-27T20:49:49.971687Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7812bd795a4a078afb1791a775c127"}},"metadata":{}},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"{'precision': 0.5896525335557485,\n 'recall': 0.58191198683027,\n 'f1': 0.5857566892310321,\n 'thold': 0.5841414141414141}"},"metadata":{}}],"execution_count":70},{"id":"2f9febbe-afc8-4b6e-b8c5-2ecdf0980e74","cell_type":"code","source":"test_preds = trainer.predict(ds_test)\ntest_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()\n\ntrainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:49:49.973790Z","iopub.execute_input":"2025-02-27T20:49:49.974004Z","iopub.status.idle":"2025-02-27T21:22:38.771245Z","shell.execute_reply.started":"2025-02-27T20:49:49.973985Z","shell.execute_reply":"2025-02-27T21:22:38.770608Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79c54e041e6436a801c29d63a02425d"}},"metadata":{}},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"0.5544444444444444"},"metadata":{}}],"execution_count":71},{"id":"dee0aa45-02a6-48e0-975d-5aea7d33ca07","cell_type":"code","source":"final_th = (0.4752+0.4257575)/2 - 0.15\nfinal_th","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T21:22:38.772295Z","iopub.execute_input":"2025-02-27T21:22:38.772673Z","iopub.status.idle":"2025-02-27T21:22:38.777892Z","shell.execute_reply.started":"2025-02-27T21:22:38.772577Z","shell.execute_reply":"2025-02-27T21:22:38.777105Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"0.30047875"},"metadata":{}}],"execution_count":72},{"id":"4e31a1a1-7efa-467e-ab31-34969164856b","cell_type":"markdown","source":"## Metric","metadata":{}},{"id":"9a61b2a0-dd05-4581-b7e6-bd1b6b0a07f0","cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:28:44.677847Z","iopub.execute_input":"2025-02-27T20:28:44.678119Z","iopub.status.idle":"2025-02-27T20:28:44.687692Z","shell.execute_reply.started":"2025-02-27T20:28:44.678098Z","shell.execute_reply":"2025-02-27T20:28:44.686882Z"}},"outputs":[],"execution_count":45},{"id":"f8417a1b-b292-4b9b-b6d6-a986c5fb7089","cell_type":"markdown","source":"## Span level","metadata":{}},{"id":"2dcb4987-376b-4905-9b1f-7c217b164fdf","cell_type":"code","source":"def inference_aggregation(probabilities, labels, offset_mappings, thold):\n    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n    ]\n    pred_spans_all = []\n    for pred, offsets in zip(true_predictions, offset_mappings):\n        samplewise_spans = []\n        current_span = None\n        for token_label, span in zip(pred, offsets):\n            if token_label == 1:  # If the current token is labeled as an entity (1)\n                if current_span is None:\n                    current_span = [span[0], span[1]]  # Start a new span\n                else:\n                    current_span[1] = span[1]  # Extend the span to include the current token\n            else:  # If token_label == 0 (not an entity)\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))  # Save completed span\n                    current_span = None  # Reset for the next entity\n        \n                    # If the last token was part of a span, save it\n        if current_span is not None:\n            samplewise_spans.append(tuple(current_span))\n        \n        pred_spans_all.append(samplewise_spans)\n    return [str(row) for row in pred_spans_all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:28:47.304197Z","iopub.execute_input":"2025-02-27T20:28:47.304495Z","iopub.status.idle":"2025-02-27T20:28:47.311230Z","shell.execute_reply.started":"2025-02-27T20:28:47.304469Z","shell.execute_reply":"2025-02-27T20:28:47.310564Z"}},"outputs":[],"execution_count":46},{"id":"dda5c4cb-c27e-472e-80c5-116f9786b65b","cell_type":"code","source":"valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\nvalid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:28:47.557091Z","iopub.execute_input":"2025-02-27T20:28:47.557315Z","iopub.status.idle":"2025-02-27T20:28:48.614688Z","shell.execute_reply.started":"2025-02-27T20:28:47.557297Z","shell.execute_reply":"2025-02-27T20:28:48.613814Z"}},"outputs":[],"execution_count":47},{"id":"0a85dff8-7d21-464a-8f82-86b0c2627c7f","cell_type":"code","source":"from copy import deepcopy\n\ndf_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\ndf_pred = deepcopy(df_gt)\ndf_pred['trigger_words'] = valid_results\nscore(df_gt, df_pred, row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:28:48.616017Z","iopub.execute_input":"2025-02-27T20:28:48.616316Z","iopub.status.idle":"2025-02-27T20:28:48.711144Z","shell.execute_reply.started":"2025-02-27T20:28:48.616284Z","shell.execute_reply":"2025-02-27T20:28:48.710510Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0.6120617208613024"},"metadata":{}}],"execution_count":48},{"id":"94451835-b216-4ffd-b95c-02139747fd30","cell_type":"code","source":"# df_pred.to_csv(\"unsloth-full-seq-gemma2-2b-binary-cv0.612.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:45:12.919772Z","iopub.execute_input":"2025-02-27T20:45:12.920102Z","iopub.status.idle":"2025-02-27T20:45:12.928806Z","shell.execute_reply.started":"2025-02-27T20:45:12.920080Z","shell.execute_reply":"2025-02-27T20:45:12.927870Z"}},"outputs":[],"execution_count":66},{"id":"d42d55a4-01fb-42ff-81bf-c27849905067","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e51e76f0-6884-4573-8a86-96e9be48df13","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d6a8b969-cea9-4da0-a0b3-d919f48d4659","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"24d22f8a-b542-4ecd-87fb-b30c1d465e04","cell_type":"markdown","source":"## Char level","metadata":{}},{"id":"ea1a6079-4277-42a0-9b00-bdfaa624a19b","cell_type":"code","source":"def inference_to_char_level(probabilities, labels, offset_mappings, ids):\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(probabilities[:, :, 1], labels)\n    ]\n    \n    all_data = []\n    \n    for sample_id, pred, offsets, probs in zip(ids, true_predictions, offset_mappings, probabilities[:, :, 1]):\n        for token_label, span, proba in zip(pred, offsets, probs):\n            for char_index in range(span[0], span[1]):\n                all_data.append({\"id\": sample_id, \"char_index\": char_index, \"proba\": proba})\n\n    output_df = pd.DataFrame(all_data)\n\n    # something was wrong, got some duplicates, mb because of out of vocab tokens\n    output_df = output_df.groupby(['id', 'char_index'], as_index=False)['proba'].mean()\n\n    return output_df\n\n\ndef char_level_to_spans(df, thold):\n    spans_all = []\n    for sample_id, group in df.groupby(\"id\"):\n        spans = []\n        current_span = None\n        for _, row in group.iterrows():\n            if row[\"proba\"] >= thold:\n                if current_span is None:\n                    current_span = [row[\"char_index\"], row[\"char_index\"] + 1]\n                else:\n                    current_span[1] = row[\"char_index\"] + 1\n            else:\n                if current_span is not None:\n                    spans.append(tuple(current_span))\n                    current_span = None\n        if current_span is not None:\n            spans.append(tuple(current_span))\n        spans_all.append({\"id\": sample_id, \"trigger_words\": spans})\n    return pd.DataFrame(spans_all)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:29:38.590355Z","iopub.execute_input":"2025-02-27T20:29:38.590677Z","iopub.status.idle":"2025-02-27T20:29:38.598619Z","shell.execute_reply.started":"2025-02-27T20:29:38.590652Z","shell.execute_reply":"2025-02-27T20:29:38.597729Z"}},"outputs":[],"execution_count":50},{"id":"8dad26af-19d4-4e06-a049-a1973e1427f4","cell_type":"code","source":"ids = df.loc[df.fold==4, 'id'].values\nvalid_results_df = inference_to_char_level(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:29:39.343005Z","iopub.execute_input":"2025-02-27T20:29:39.343288Z","iopub.status.idle":"2025-02-27T20:29:40.602053Z","shell.execute_reply.started":"2025-02-27T20:29:39.343263Z","shell.execute_reply":"2025-02-27T20:29:40.601382Z"}},"outputs":[],"execution_count":51},{"id":"2f2017dd-2b9e-40b7-935c-b76bd7c93522","cell_type":"code","source":"score(df_gt, char_level_to_spans(valid_results_df, final_th), row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:31:47.325753Z","iopub.execute_input":"2025-02-27T20:31:47.326106Z","iopub.status.idle":"2025-02-27T20:32:04.750087Z","shell.execute_reply.started":"2025-02-27T20:31:47.326080Z","shell.execute_reply":"2025-02-27T20:32:04.749196Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"0.6120442270208827"},"metadata":{}}],"execution_count":55},{"id":"b0707674-64bf-4874-ae3d-02555cc613c3","cell_type":"code","source":"valid_results_df.to_csv(f\"char_cv_preds_{config.wandb_init_args['name']}.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:34:42.214118Z","iopub.execute_input":"2025-02-27T20:34:42.214434Z","iopub.status.idle":"2025-02-27T20:34:43.220709Z","shell.execute_reply.started":"2025-02-27T20:34:42.214406Z","shell.execute_reply":"2025-02-27T20:34:43.220039Z"}},"outputs":[],"execution_count":58},{"id":"3fd739bd-66be-4780-98a7-4565f5bc3ebb","cell_type":"code","source":"ds_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:39:09.283954Z","iopub.execute_input":"2025-02-27T20:39:09.284247Z","iopub.status.idle":"2025-02-27T20:39:09.289976Z","shell.execute_reply.started":"2025-02-27T20:39:09.284227Z","shell.execute_reply":"2025-02-27T20:39:09.289072Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'offset_mapping', 'labels', 'content'],\n    num_rows: 5735\n})"},"metadata":{}}],"execution_count":62},{"id":"8754e93b-567d-4011-9f9e-c7acc687d594","cell_type":"code","source":"ids = df_test[\"id\"]\n\ntest_char_results_df = inference_to_char_level(\n    test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], ids\n)\n\ntest_char_results_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:39:20.004576Z","iopub.execute_input":"2025-02-27T20:39:20.004922Z","iopub.status.idle":"2025-02-27T20:39:33.267166Z","shell.execute_reply.started":"2025-02-27T20:39:20.004896Z","shell.execute_reply":"2025-02-27T20:39:33.266501Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"(3383472, 3)"},"metadata":{}}],"execution_count":63},{"id":"fa24b598-1976-428d-b4dd-d8e5fe29d520","cell_type":"code","source":"test_char_results_df.to_csv(f\"char_test_preds_{config.wandb_init_args['name']}.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T20:41:31.793179Z","iopub.execute_input":"2025-02-27T20:41:31.793513Z","iopub.status.idle":"2025-02-27T20:41:39.538825Z","shell.execute_reply.started":"2025-02-27T20:41:31.793479Z","shell.execute_reply":"2025-02-27T20:41:39.538121Z"}},"outputs":[],"execution_count":64},{"id":"ce6b5317-2803-48e7-8360-f1a2263804ae","cell_type":"code","source":"output = tokenizer(\n        '🪃',\n        return_offsets_mapping=True,\n        add_special_tokens=True,\n        max_length=None,\n        padding=False\n    )\n\ntokenizer.convert_ids_to_tokens(output['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:41:34.369112Z","iopub.execute_input":"2025-02-27T00:41:34.369681Z","iopub.status.idle":"2025-02-27T00:41:34.377863Z","shell.execute_reply.started":"2025-02-27T00:41:34.369631Z","shell.execute_reply":"2025-02-27T00:41:34.376997Z"}},"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"['<bos>', '<0xF0>', '<0x9F>', '<0xAA>', '<0x83>', '<eos>']"},"metadata":{}}],"execution_count":114}]}