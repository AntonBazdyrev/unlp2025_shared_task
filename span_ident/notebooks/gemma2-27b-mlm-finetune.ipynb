{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9283525b-ba75-467a-8b90-9c190ce7f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('train.parquet')\n",
    "cv = pd.read_csv('cv_split.csv')\n",
    "df = df.merge(cv, on='id', how='left')\n",
    "\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443e4c9f-7dba-4634-a8b0-5b65f78044b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1354428/2364685263.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de39a37-a1fb-49a7-af42-f78be7037717",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = '/home/abazdyrev/pretrained_models/gemma2-27b-it-unmasked'\n",
    "MAX_LEN = 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0d2f0e-3850-4750-b6e0-d8ff71910992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020ee767-721c-4ae4-867d-f880cb02d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n",
    "    tokenized_output = tokenizer(\n",
    "        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n",
    "        truncation=True, padding=False\n",
    "    )\n",
    "    tokens = tokenized_output[\"input_ids\"]\n",
    "    offsets = tokenized_output[\"offset_mapping\"]\n",
    "\n",
    "    # Get subword tokenized versions of the text\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    \n",
    "    # Initialize labels as 'O'\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    if trigger_spans is not None:\n",
    "        # Assign 'TRIGGER' to overlapping tokens\n",
    "        for start, end in trigger_spans:\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_start == 0 and tok_end == 0:\n",
    "                    continue\n",
    "                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n",
    "                    labels[i] = 1\n",
    "\n",
    "    tokenized_output['labels'] = labels\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c668d782-c0ef-42ae-a366-c6a2fe032d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07edad2db2004a3a913ddb8efcc8f616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493ad7e9184649a88ad609650c5da144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\n",
    "for column in df.seq_labels.iloc[0].keys():\n",
    "    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n",
    "\n",
    "df_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\n",
    "for column in df_test.seq_labels.iloc[0].keys():\n",
    "    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "df['is_valid'] = df.fold == 4\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\n",
    "ds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\n",
    "ds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n",
    "\n",
    "columns = list(df.seq_labels.iloc[0].keys()) + ['content']\n",
    "ds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b3f770-62e4-4d3c-b472-5db1377ad0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from packaging import version\n",
    "import importlib.metadata\n",
    "\n",
    "from transformers import Gemma2Model, Gemma2ForCausalLM, Gemma2PreTrainedModel, Gemma2Config\n",
    "from transformers.models.gemma2.modeling_gemma2 import (\n",
    "    Gemma2DecoderLayer,\n",
    "    Gemma2Attention,\n",
    "    Gemma2FlashAttention2,\n",
    "    Gemma2SdpaAttention,\n",
    "    Gemma2MLP,\n",
    "    Gemma2RMSNorm,\n",
    ")\n",
    "\n",
    "from torch import nn\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import AttentionMaskConverter\n",
    "from transformers.utils.import_utils import _is_package_available\n",
    "from transformers.cache_utils import Cache, StaticCache\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers.models.gemma2.modeling_gemma2 import *\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def is_transformers_attn_greater_or_equal_4_41():\n",
    "    if not _is_package_available(\"transformers\"):\n",
    "        return False\n",
    "\n",
    "    return version.parse(importlib.metadata.version(\"transformers\")) >= version.parse(\n",
    "        \"4.41.0\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ModifiedGemma2Attention(Gemma2Attention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedGemma2FlashAttention2(Gemma2FlashAttention2):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedGemma2SdpaAttention(Gemma2SdpaAttention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "GEMMA2_ATTENTION_CLASSES = {\n",
    "    \"eager\": ModifiedGemma2Attention,\n",
    "    \"flash_attention_2\": ModifiedGemma2FlashAttention2,\n",
    "    \"sdpa\": ModifiedGemma2SdpaAttention,\n",
    "}\n",
    "\n",
    "\n",
    "class ModifiedGemma2DecoderLayer(Gemma2DecoderLayer):\n",
    "    def __init__(self, config: Gemma2Config, layer_idx: int):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = GEMMA2_ATTENTION_CLASSES[config._attn_implementation](\n",
    "            config=config, layer_idx=layer_idx\n",
    "        )\n",
    "\n",
    "        self.mlp = Gemma2MLP(config)\n",
    "        self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.is_sliding = not bool(layer_idx % 2)\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.sliding_window = config.sliding_window\n",
    "\n",
    "\n",
    "\n",
    "class Gemma2BiModel(Gemma2Model):\n",
    "    _no_split_modules = [\"ModifiedGemma2DecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: Gemma2Config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ModifiedGemma2DecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if past_key_values is not None:\n",
    "            target_length = past_key_values.get_max_length()\n",
    "        else:\n",
    "            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n",
    "\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n",
    "            if attention_mask.max() != 0:\n",
    "                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            causal_mask = torch.zeros(\n",
    "                (sequence_length, target_length), dtype=dtype, device=device\n",
    "            )\n",
    "            # causal_mask = torch.full(\n",
    "            #     (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            # )\n",
    "            # if sequence_length != 1:\n",
    "            #     causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                    padding_mask, min_dtype\n",
    "                )\n",
    "        return causal_mask\n",
    "\n",
    "\n",
    "class Gemma2BiForMNTP(Gemma2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.model = Gemma2BiModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # getter for PEFT model\n",
    "    def get_model_for_peft(self):\n",
    "        return self.model\n",
    "\n",
    "    # setter for PEFT model\n",
    "    def set_model_for_peft(self, model: PeftModel):\n",
    "        self.model = model\n",
    "\n",
    "    # save the PEFT model\n",
    "    def save_peft_model(self, path):\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "\n",
    "class Gemma2BiMLM(Gemma2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        Gemma2PreTrainedModel.__init__(self, config)\n",
    "        self.model = Gemma2BiModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[HybridCache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **loss_kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(prediction_scores.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    # getter for PEFT model\n",
    "    def get_model_for_peft(self):\n",
    "        return self.model\n",
    "\n",
    "    # setter for PEFT model\n",
    "    def set_model_for_peft(self, model: PeftModel):\n",
    "        self.model = model\n",
    "\n",
    "    # save the PEFT model\n",
    "    def save_peft_model(self, path):\n",
    "        self.model.save_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86956468-d181-4f13-a58a-6079d55700d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c82c346424c01b7005082e4e5a3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForMaskedLM, AutoModelForMaskedLM, Gemma2ForCausalLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import Gemma2ForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "backbone_model = Gemma2BiModel.from_pretrained(\n",
    "    PRETRAINED_MODEL, torch_dtype=torch.bfloat16,\n",
    "    quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a1d355-4f2a-4188-88c6-1db76b03f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c38f7ce6d74712b5703dccd25626ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at /home/abazdyrev/pretrained_models/gemma-2-27b-it and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Gemma2ForTokenClassification, LlamaForTokenClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "id2label = {0: 0, 1: 1}\n",
    "label2id = {0: 0, 1: 1}\n",
    "\n",
    "from peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Gemma2ForTokenClassification.from_pretrained(\n",
    "    '/home/abazdyrev/pretrained_models/gemma-2-27b-it',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f6bb75-ff07-4e72-a3dc-ac6b0f01be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = backbone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324e0449-43dd-477a-b5b1-9c086894df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52936bac-dc4b-4d99-bd85-513a529fa4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForTokenClassification(\n",
       "  (model): Gemma2BiModel(\n",
       "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-45): 46 x ModifiedGemma2DecoderLayer(\n",
       "        (self_attn): ModifiedGemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=4608, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4608, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=36864, out_features=4608, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (score): Linear(in_features=4608, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "438d0ad1-c4d7-4026-9ca4-632d8ac659a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 456,729,602 || all params: 27,683,867,140 || trainable%: 1.6498\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=32, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout=0.05, \n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ") \n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fcc28d-281d-4a1f-931e-a88378bc6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "\n",
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3b6fad-eacf-427f-b17b-ff9a4224e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='model_checkpoints_gemma2_27b_mlm_finetune',\n",
    "    logging_dir='./model_logs_gemma2_27b_mlm_finetune',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_8bit',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006bb1d7-e8cf-4b56-8323-14b40552494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbazdyrev99\u001b[0m (\u001b[33mIASA-BA-Diploma-Ivan-Bashtovyi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abazdyrev/repos/unlp2025_shared_task/span_ident/notebooks/wandb/run-20250329_112739-siefmwhk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/siefmwhk' target=\"_blank\">gemma2-27b-mlm-finetune</a></strong> to <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/siefmwhk' target=\"_blank\">https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/siefmwhk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute/unlp-span-ident-task/runs/siefmwhk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7eeeca695400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize with team/entity\n",
    "wandb.init(\n",
    "    project=\"unlp-span-ident-task\",\n",
    "    entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\", \n",
    "    name='gemma2-27b-mlm-finetune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22925695654588976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "positive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\n",
    "positive_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7abe3ad-0619-40d3-90cc-70fd52617d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import Trainer, pipeline, TrainingArguments\n",
    "from typing import Any\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "def extract_chars_from_spans(spans):\n",
    "    \"\"\"\n",
    "    Given a list of spans (each a tuple (start, end)),\n",
    "    return a set of character indices for all spans.\n",
    "    \"\"\"\n",
    "    char_set = set()\n",
    "    for start, end in spans:\n",
    "        # Each span covers positions start, start+1, ..., end-1.\n",
    "        char_set.update(range(start, end))\n",
    "    return char_set\n",
    "\n",
    "class SpanEvaluationTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any = None,\n",
    "        args: TrainingArguments = None,\n",
    "        data_collator: Any = None,\n",
    "        train_dataset: Any = None,\n",
    "        eval_dataset: Any = None,\n",
    "        tokenizer: Any = None,\n",
    "        desired_positive_ratio: float = 0.25,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer with our custom compute_metrics.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.desired_positive_ratio = desired_positive_ratio\n",
    "\n",
    "    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n",
    "        total_true_chars = 0\n",
    "        total_pred_chars = 0\n",
    "        total_overlap_chars = 0\n",
    "        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n",
    "            if isinstance(true_spans, str):\n",
    "                try:\n",
    "                    true_spans = eval(true_spans)\n",
    "                except Exception:\n",
    "                    true_spans = []\n",
    "                    \n",
    "            # Convert spans to sets of character indices.\n",
    "            true_chars = extract_chars_from_spans(true_spans)\n",
    "            pred_chars = extract_chars_from_spans(pred_spans)\n",
    "            \n",
    "            total_true_chars += len(true_chars)\n",
    "            total_pred_chars += len(pred_chars)\n",
    "            total_overlap_chars += len(true_chars.intersection(pred_chars))\n",
    "            \n",
    "            union_chars = true_chars.union(pred_chars)\n",
    "            \n",
    "        # Compute precision, recall, and F1.\n",
    "        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n",
    "        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _find_optimal_threshold(self, probabilities, labels):\n",
    "        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n",
    "        best_th = 0.5  # Default starting point\n",
    "        best_diff = float(\"inf\")\n",
    "        optimal_th = best_th\n",
    "        \n",
    "        for thold in np.linspace(0.01, 0.99, num=100):\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n",
    "            total = sum([len(prediction) for prediction in true_predictions])\n",
    "            \n",
    "            positive_ratio = total_pos / total if total > 0 else 0\n",
    "            \n",
    "            diff = abs(positive_ratio - self.desired_positive_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                optimal_th = thold\n",
    "        \n",
    "        return optimal_th\n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n",
    "        eval_dataset = self.eval_dataset\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "    \n",
    "        #thresholds = np.linspace(0.1, 0.5, num=41)\n",
    "        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n",
    "        results = []\n",
    "        best_f1 = -1\n",
    "        best_th = 0\n",
    "        best_metrics = None\n",
    "    \n",
    "        for thold in tqdm(thresholds):\n",
    "            # Apply thresholding instead of argmax\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    \n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    \n",
    "            pred_spans_all = []\n",
    "            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n",
    "                samplewise_spans = []\n",
    "                current_span = None\n",
    "                for token_label, span in zip(pred, offsets):\n",
    "                    if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                        if current_span is None:\n",
    "                            current_span = [span[0], span[1]]  # Start a new span\n",
    "                        else:\n",
    "                            current_span[1] = span[1]  # Extend the span to include the current token\n",
    "                    else:  # If token_label == 0 (not an entity)\n",
    "                        if current_span is not None:\n",
    "                            samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                            current_span = None  # Reset for the next entity\n",
    "    \n",
    "                # If the last token was part of a span, save it\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))\n",
    "    \n",
    "                pred_spans_all.append(samplewise_spans)\n",
    "    \n",
    "            # Store results for this threshold\n",
    "            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n",
    "            if current_metrics['f1'] >= best_f1:\n",
    "                best_f1 = current_metrics['f1']\n",
    "                best_th = thold\n",
    "                best_metrics = current_metrics\n",
    "                best_metrics['thold'] = thold\n",
    "                \n",
    "            \n",
    "            results.append(current_metrics)\n",
    "        return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e33e84-5eac-48fe-948e-f53acd597002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1354428/3170772390.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = SpanEvaluationTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    desired_positive_ratio=positive_class_balance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835e231-e15e-4299-b4b1-b47d464db8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/abazdyrev/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/1146 01:28 < 2:08:05, 0.15 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4dbe9-e9bb-45cf-b13c-bb19bff01d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e54265-24e4-4445-abf5-1fd43614e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c372773dced4153b4ff43ad0e41790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at /home/abazdyrev/pretrained_models/gemma-2-27b-it/ and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,218 || all params: 27,683,867,140 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Gemma2ForTokenClassification, BitsAndBytesConfig\n",
    "from peft import PeftModel, get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Gemma2ForTokenClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=nf4_config\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"./model_checkpoints_gemma2_27b_binary/checkpoint-600/\")\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3913ada-b31b-404f-9745-7babf156ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4800edb9-d35d-4289-ad21-b0672254edd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a94f2ada594dafa3c11cf016ffe422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_preds = trainer.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92bd64bc-afd9-4a8a-8de4-514824f6d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((764, 1440, 2), (764, 1440))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.predictions.shape, valid_preds.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc734f81-5211-4312-b248-f1da909f69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8052d1a471bb4f28818ab0e00e849b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6380175550535446,\n",
       " 'recall': 0.6279042139741697,\n",
       " 'f1': 0.6329204872040508,\n",
       " 'thold': 0.5049494949494949}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd22ce-5c37-41b6-bf6d-ab1e50791687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='491' max='2868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 491/2868 04:49 < 23:24, 1.69 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23204c4d-70aa-4b44-9edd-a62794e4be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e950513f-2a4d-4e6b-ac59-6e27836023a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45545454545454545"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9bec8dd-19c1-441b-b467-aa0238dc1af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36019725"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_th = (0.50494+0.4554545)/2 - 0.12\n",
    "final_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "042dc4ff-378c-46ef-bbe4-7613de11f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_aggregation(probabilities, labels, offset_mappings, thold):\n",
    "    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    pred_spans_all = []\n",
    "    for pred, offsets in zip(true_predictions, offset_mappings):\n",
    "        samplewise_spans = []\n",
    "        current_span = None\n",
    "        for token_label, span in zip(pred, offsets):\n",
    "            if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                if current_span is None:\n",
    "                    current_span = [span[0], span[1]]  # Start a new span\n",
    "                else:\n",
    "                    current_span[1] = span[1]  # Extend the span to include the current token\n",
    "            else:  # If token_label == 0 (not an entity)\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                    current_span = None  # Reset for the next entity\n",
    "        \n",
    "                    # If the last token was part of a span, save it\n",
    "        if current_span is not None:\n",
    "            samplewise_spans.append(tuple(current_span))\n",
    "        \n",
    "        pred_spans_all.append(samplewise_spans)\n",
    "    return [str(row) for row in pred_spans_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "537572ee-be33-4911-bc82-8abd6b9d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdc10345-650a-4e78-8e7b-2f923faefe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Custom exception for participant-visible errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute span-level F1 score based on overlap.\n",
    "\n",
    "    Parameters:\n",
    "    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n",
    "    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n",
    "    - row_id_column_name (str): Column name for the row identifier.\n",
    "\n",
    "    Returns:\n",
    "    - float: The token-level weighted F1 score.\n",
    "\n",
    "    Example:\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     \"id\": [1, 2, 3],\n",
    "    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n",
    "    ... })\n",
    "    >>> score(solution, submission, \"id\")\n",
    "    0.16296296296296295\n",
    "    \"\"\"\n",
    "    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n",
    "        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n",
    "    \n",
    "    def safe_parse_spans(trigger_words):\n",
    "        if isinstance(trigger_words, str):\n",
    "            try:\n",
    "                return ast.literal_eval(trigger_words)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n",
    "            return trigger_words\n",
    "        return []\n",
    "\n",
    "    def extract_tokens_from_spans(spans):\n",
    "        tokens = set()\n",
    "        for start, end in spans:\n",
    "            tokens.update(range(start, end))\n",
    "        return tokens\n",
    "    \n",
    "    solution = solution.copy()\n",
    "    submission = submission.copy()\n",
    "\n",
    "    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n",
    "    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n",
    "\n",
    "    merged = pd.merge(\n",
    "        solution,\n",
    "        submission,\n",
    "        on=\"id\",\n",
    "        suffixes=(\"_solution\", \"_submission\")\n",
    "    )\n",
    "\n",
    "    total_true_tokens = 0\n",
    "    total_pred_tokens = 0\n",
    "    overlapping_tokens = 0\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        true_spans = row[\"trigger_words_solution\"]\n",
    "        pred_spans = row[\"trigger_words_submission\"]\n",
    "\n",
    "        true_tokens = extract_tokens_from_spans(true_spans)\n",
    "        pred_tokens = extract_tokens_from_spans(pred_spans)\n",
    "\n",
    "        total_true_tokens += len(true_tokens)\n",
    "        total_pred_tokens += len(pred_tokens)\n",
    "        overlapping_tokens += len(true_tokens & pred_tokens)\n",
    "\n",
    "    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n",
    "    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffbb019a-659b-4bd8-a84f-b1c3ceb5698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.650320400396776"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_pred = deepcopy(df_gt)\n",
    "df_pred['trigger_words'] = valid_results\n",
    "score(df_gt, df_pred, row_id_column_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4a952c8-9492-4b3a-819b-a061f03657d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2f59c7e-a282-4609-921e-b6879978818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('sample_submission.csv')\n",
    "ss['trigger_words'] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e832f9a-ef93-4400-9b67-5bf843a90205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submissions/gemma2-27b-binary-cv0.65blackmagic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e50c7-ddf0-420d-bfe6-dc224c078265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfa57761-043c-443b-8f06-3738b0b373e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26b138bef9641a3ae816a377faeee8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6380175550535446,\n",
       " 'recall': 0.6279042139741697,\n",
       " 'f1': 0.6329204872040508,\n",
       " 'thold': 0.5049494949494949}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a927a08-448a-414d-b458-28bff79d3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THoptimizer:\n",
    "    def __init__(self, ds):\n",
    "        self.eval_dataset = ds\n",
    "        \n",
    "    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n",
    "        total_true_chars = 0\n",
    "        total_pred_chars = 0\n",
    "        total_overlap_chars = 0\n",
    "        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n",
    "            if isinstance(true_spans, str):\n",
    "                try:\n",
    "                    true_spans = eval(true_spans)\n",
    "                except Exception:\n",
    "                    true_spans = []\n",
    "                    \n",
    "            # Convert spans to sets of character indices.\n",
    "            true_chars = extract_chars_from_spans(true_spans)\n",
    "            pred_chars = extract_chars_from_spans(pred_spans)\n",
    "            \n",
    "            total_true_chars += len(true_chars)\n",
    "            total_pred_chars += len(pred_chars)\n",
    "            total_overlap_chars += len(true_chars.intersection(pred_chars))\n",
    "            \n",
    "            union_chars = true_chars.union(pred_chars)\n",
    "            \n",
    "        # Compute precision, recall, and F1.\n",
    "        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n",
    "        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _find_optimal_threshold(self, probabilities, labels):\n",
    "        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n",
    "        best_th = 0.5  # Default starting point\n",
    "        best_diff = float(\"inf\")\n",
    "        optimal_th = best_th\n",
    "        \n",
    "        for thold in np.linspace(0.01, 0.99, num=100):\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n",
    "            total = sum([len(prediction) for prediction in true_predictions])\n",
    "            \n",
    "            positive_ratio = total_pos / total if total > 0 else 0\n",
    "            \n",
    "            diff = abs(positive_ratio - self.desired_positive_ratio)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                optimal_th = thold\n",
    "        \n",
    "        return optimal_th\n",
    "        \n",
    "    def compute_metrics(self, eval_pred) -> dict:\n",
    "        eval_dataset = self.eval_dataset\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
    "    \n",
    "        thresholds = np.linspace(0.1, 0.5, num=41)\n",
    "        #thresholds = [self._find_optimal_threshold(probabilities, labels)]\n",
    "        results = []\n",
    "        best_f1 = -1\n",
    "        best_th = 0\n",
    "        best_metrics = None\n",
    "    \n",
    "        for thold in tqdm(thresholds):\n",
    "            # Apply thresholding instead of argmax\n",
    "            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n",
    "    \n",
    "            true_predictions = [\n",
    "                [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    \n",
    "            pred_spans_all = []\n",
    "            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n",
    "                samplewise_spans = []\n",
    "                current_span = None\n",
    "                for token_label, span in zip(pred, offsets):\n",
    "                    if token_label == 1:  # If the current token is labeled as an entity (1)\n",
    "                        if current_span is None:\n",
    "                            current_span = [span[0], span[1]]  # Start a new span\n",
    "                        else:\n",
    "                            current_span[1] = span[1]  # Extend the span to include the current token\n",
    "                    else:  # If token_label == 0 (not an entity)\n",
    "                        if current_span is not None:\n",
    "                            samplewise_spans.append(tuple(current_span))  # Save completed span\n",
    "                            current_span = None  # Reset for the next entity\n",
    "    \n",
    "                # If the last token was part of a span, save it\n",
    "                if current_span is not None:\n",
    "                    samplewise_spans.append(tuple(current_span))\n",
    "    \n",
    "                pred_spans_all.append(samplewise_spans)\n",
    "    \n",
    "            # Store results for this threshold\n",
    "            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n",
    "            if current_metrics['f1'] >= best_f1:\n",
    "                best_f1 = current_metrics['f1']\n",
    "                best_th = thold\n",
    "                best_metrics = current_metrics\n",
    "                best_metrics['thold'] = thold\n",
    "                \n",
    "            \n",
    "            results.append(current_metrics)\n",
    "        return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5df2f0de-b3f6-4c08-a640-4fc9161c2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "thoptimizer = THoptimizer(trainer.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "377be072-a882-474e-a5f9-259772c285d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6445e9a616aa4293b2fde8d46a1fc8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5853007860760009,\n",
       " 'recall': 0.7316582658724098,\n",
       " 'f1': 0.6503469603258409,\n",
       " 'thold': 0.36}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoptimizer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17b16b11-5fcf-40c3-9456-938538e7168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.415"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_th = (0.36 + 0.47)/2\n",
    "final_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92e10245-d59e-4c15-a6ea-1571d0448837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6469428475473636"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)\n",
    "df_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_pred = deepcopy(df_gt)\n",
    "df_pred['trigger_words'] = valid_results\n",
    "score(df_gt, df_pred, row_id_column_name='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e1f2d55-f00e-4ecc-8be1-a876570e193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6f7c01b-005d-43a8-96f5-89e1193f773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('sample_submission.csv')\n",
    "ss['trigger_words'] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0329c239-23af-4f33-8127-9303c66cf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submissions/gemma2-27b-binary-cv0.646-regularized-gs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70368d6a-ed86-401c-94be-9b8232867b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f44fd-ab5e-439e-b99e-abd9b34c7177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43edf7bc-5a95-4174-984b-4cfabce5252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(valid_preds, open('local_preds/valid_preds_gemma2_binary.pkl', 'wb'))\n",
    "pickle.dump(test_preds, open('local_preds/test_preds_gemma2_binary.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63083323-8398-4291-8f8a-f4cfa54dfe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
