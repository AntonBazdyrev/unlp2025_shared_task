{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10668454,"sourceType":"datasetVersion","datasetId":6607405}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9283525b-ba75-467a-8b90-9c190ce7f885","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet\")#'train.parquet')\ncv = pd.read_csv(\"/kaggle/input/span-ident-cv-split-csv/cv_split.csv\")#'cv_split.csv')\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(\"/kaggle/input/unlp-2025-shared-task-span-identification/test.csv\")#'test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:11.884564Z","iopub.execute_input":"2025-02-06T10:48:11.884854Z","iopub.status.idle":"2025-02-06T10:48:13.570612Z","shell.execute_reply.started":"2025-02-06T10:48:11.884814Z","shell.execute_reply":"2025-02-06T10:48:13.569569Z"}},"outputs":[],"execution_count":1},{"id":"443e4c9f-7dba-4634-a8b0-5b65f78044b3","cell_type":"code","source":"import spacy\n\nfrom spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\nfrom tqdm.autonotebook import tqdm\ntqdm.pandas()\n\ndf.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:13.571813Z","iopub.execute_input":"2025-02-06T10:48:13.572154Z","iopub.status.idle":"2025-02-06T10:48:21.057813Z","shell.execute_reply.started":"2025-02-06T10:48:13.572125Z","shell.execute_reply":"2025-02-06T10:48:21.056869Z"}},"outputs":[],"execution_count":2},{"id":"3de39a37-a1fb-49a7-af42-f78be7037717","cell_type":"code","source":"PRETRAINED_MODEL = 'microsoft/mdeberta-v3-base'\nTRAIN_LEN = 512\nFULL_LEN = 1550","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:21.058873Z","iopub.execute_input":"2025-02-06T10:48:21.059390Z","iopub.status.idle":"2025-02-06T10:48:21.064064Z","shell.execute_reply.started":"2025-02-06T10:48:21.059359Z","shell.execute_reply":"2025-02-06T10:48:21.062955Z"}},"outputs":[],"execution_count":3},{"id":"db0d2f0e-3850-4750-b6e0-d8ff71910992","cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:21.066740Z","iopub.execute_input":"2025-02-06T10:48:21.067011Z","iopub.status.idle":"2025-02-06T10:48:30.045039Z","shell.execute_reply.started":"2025-02-06T10:48:21.066989Z","shell.execute_reply":"2025-02-06T10:48:30.043871Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f7e82f980f4f4ea6223c5a8e607e13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de658e0b65de48cc88bb0c43ab12a1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbaaee3799f4450ab092730a899e17c6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"id":"d9177cae-4472-480b-b1b2-947e80d1a65d","cell_type":"code","source":"# from transformers import AutoTokenizer\n# import pandas as pd\n# from datasets import Dataset\n# from tqdm.autonotebook import tqdm\n\n# # Load the tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n\n# def convert_to_seq_labeling(text, tokenizer, trigger_spans=None, seq_len=TRAIN_LEN):\n#     tokenized_output = tokenizer(\n#         text, return_offsets_mapping=True, add_special_tokens=True, max_length=seq_len,\n#         truncation=True, padding=False\n#     )\n#     tokens = tokenized_output[\"input_ids\"]\n#     offsets = tokenized_output[\"offset_mapping\"]\n#     token_strings = tokenizer.convert_ids_to_tokens(tokens)\n    \n#     labels = [0] * len(tokens)\n#     if trigger_spans is not None:\n#         for start, end in trigger_spans:\n#             for i, (tok_start, tok_end) in enumerate(offsets):\n#                 if tok_start == 0 and tok_end == 0:\n#                     continue\n#                 if tok_start < end and tok_end > start:\n#                     labels[i] = 1\n#     tokenized_output['labels'] = labels\n#     return tokenized_output\n\n\n# # df['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words'], seq_len=FULL_LEN), axis=1)\n# # for column in df.seq_labels.iloc[0].keys():\n# #     df[column] = df.seq_labels.apply(lambda x: x.get(column))\n\n# # Process training data\n# tqdm.pandas()\n# train_df = df[df.fold != 4].copy()\n# train_df['seq_labels'] = train_df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words'], seq_len=TRAIN_LEN), axis=1)\n# for column in train_df.seq_labels.iloc[0].keys():\n#     train_df[column] = train_df.seq_labels.apply(lambda x: x.get(column))\n\n# # Process validation data\n# val_df = df[df.fold == 4].copy()\n# val_df['seq_labels'] = val_df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words'], seq_len=FULL_LEN), axis=1)\n# for column in val_df.seq_labels.iloc[0].keys():\n#     val_df[column] = val_df.seq_labels.apply(lambda x: x.get(column))\n\n# # Process test data\n# df_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None, seq_len=FULL_LEN), axis=1)\n# for column in df_test.seq_labels.iloc[0].keys():\n#     df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))\n\n# # Create datasets\n# columns = list(train_df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\n# ds_train = Dataset.from_pandas(train_df[columns].reset_index(drop=True))\n# ds_valid = Dataset.from_pandas(val_df[columns].reset_index(drop=True))\n\n# columns = list(df_test.seq_labels.iloc[0].keys()) + ['content']\n# ds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))\n\n\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Calculate sequence lengths\n# df['seq_len'] = df['input_ids'].apply(len)\n\n# # Get statistics\n# total_sequences = len(df)\n# threshold = TRAIN_LEN\n# long_sequences = (df['seq_len'] > threshold).sum()\n# percentage = (long_sequences / total_sequences) * 100\n\n# print(f\"Sequences longer than {threshold}: {long_sequences} ({percentage:.2f}%)\")\n# print(f\"Max sequence length: {df['seq_len'].max()}\")\n# print(f\"Mean sequence length: {df['seq_len'].mean():.2f}\")\n\n# # Plot distribution\n# plt.figure(figsize=(10, 5))\n# sns.histplot(data=df['seq_len'], bins=50)\n# plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n# plt.title('Sequence Length Distribution')\n# plt.xlabel('Sequence Length')\n# plt.ylabel('Count')\n# plt.legend()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:30.047066Z","iopub.execute_input":"2025-02-06T10:48:30.047589Z","iopub.status.idle":"2025-02-06T10:48:30.052368Z","shell.execute_reply.started":"2025-02-06T10:48:30.047561Z","shell.execute_reply":"2025-02-06T10:48:30.051275Z"}},"outputs":[],"execution_count":5},{"id":"27dd35b8-6373-496f-b72f-12631ecb6813","cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\nfrom datasets import Dataset\nfrom tqdm.autonotebook import tqdm\n\ndef convert_to_seq_labeling(text, tokenizer, trigger_spans=None, chunk_size=512):\n    # Tokenize entire text without truncation\n    tokenized = tokenizer(\n        text, \n        return_offsets_mapping=True, \n        add_special_tokens=False,\n        truncation=False\n    )\n    \n    chunks = []\n    offsets = tokenized[\"offset_mapping\"]\n    tokens = tokenized[\"input_ids\"]\n    \n    # Split into chunks (account for [CLS]/[SEP])\n    for i in range(0, len(tokens), chunk_size - 2):\n        chunk_tokens = tokens[i:i+chunk_size-2]\n        chunk_offsets = offsets[i:i+chunk_size-2]\n        \n        # Add special tokens\n        chunk_tokens = [tokenizer.cls_token_id] + chunk_tokens + [tokenizer.sep_token_id]\n        chunk_offsets = [(0, 0)] + chunk_offsets + [(0, 0)]\n        \n        chunk_data = {\n            \"input_ids\": chunk_tokens,\n            \"attention_mask\": [1] * len(chunk_tokens),\n            \"offset_mapping\": chunk_offsets\n        }\n        \n        # Get actual character span from offset mapping\n        chunk_start = chunk_offsets[1][0]  # First real token's start\n        chunk_end = chunk_offsets[-2][1]   # Last real token's end\n        \n        labels = [0] * len(chunk_tokens)\n        if trigger_spans is not None:\n            for span_start, span_end in trigger_spans:\n                # Check if span overlaps with this chunk's character range\n                if not (span_end <= chunk_start or span_start >= chunk_end):\n                    # Find overlap within this chunk\n                    for idx, (tok_start, tok_end) in enumerate(chunk_offsets):\n                        # Skip special tokens\n                        if tok_start == 0 and tok_end == 0:\n                            continue\n                            \n                        # Directly use original offsets from tokenizer\n                        token_abs_start = tok_start\n                        token_abs_end = tok_end\n                        \n                        # Check overlap with original span\n                        if (token_abs_start < span_end and token_abs_end > span_start):\n                            labels[idx] = 1\n\n        chunk_data[\"labels\"] = labels\n        chunks.append(chunk_data)\n    \n    return chunks\n\ndef convert_val_to_seq_labeling(text, tokenizer, trigger_spans=None, seq_len=TRAIN_LEN):\n    tokenized_output = tokenizer(\n        text, return_offsets_mapping=True, add_special_tokens=True, max_length=seq_len,\n        truncation=True, padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    labels = [0] * len(tokens)\n    if trigger_spans is not None:\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:\n                    labels[i] = 1\n    tokenized_output['labels'] = labels\n    return tokenized_output\n    \ndef process_train_data(df, chunk_size):\n    \"\"\"Process training data with chunking\"\"\"\n    tqdm.pandas()\n    \n    # Generate chunks for each row\n    df[\"chunks\"] = df.progress_apply(\n        lambda row: convert_to_seq_labeling(\n            row[\"content\"],\n            tokenizer,\n            row.get(\"trigger_words\"),\n            chunk_size=chunk_size\n        ),\n        axis=1\n    )\n    \n    # Explode chunks into separate rows\n    df = df.explode(\"chunks\").reset_index(drop=True)\n    \n    # Extract chunk components\n    for key in [\"input_ids\", \"attention_mask\", \"labels\", \"offset_mapping\"]:\n        df[key] = df[\"chunks\"].apply(lambda x: x.get(key))\n    \n    # Add token_type_ids (if not present)\n    if \"token_type_ids\" not in df.chunks.iloc[0]:\n        df[\"token_type_ids\"] = df[\"input_ids\"].apply(lambda x: [0]*len(x))\n    else:\n        df[\"token_type_ids\"] = df[\"chunks\"].apply(lambda x: x.get(\"token_type_ids\"))\n    \n    return df.drop(columns=[\"chunks\"])\n\n\ndef process_val_test_data(df, seq_len):\n    \"\"\"Modified processing incorporating trigger span handling\"\"\"\n    tqdm.pandas()\n    \n    df['seq_labels'] = df.progress_apply(\n        lambda row: convert_val_to_seq_labeling(\n            row['content'],\n            tokenizer,\n            row.get('trigger_words', None),  # Handle both validation and test cases\n            seq_len=seq_len\n        ),\n        axis=1\n    )\n    \n    # Extract all tokenizer outputs\n    for column in df.seq_labels.iloc[0].keys():\n        df[column] = df.seq_labels.apply(lambda x: x.get(column))\n    \n    return df\n\n\n# Process training data with chunking\ntrain_df_init = df[df.fold != 4].copy()\ntrain_df_processed = process_train_data(train_df_init, chunk_size=TRAIN_LEN)\n\n# Process validation data (original method)\nval_df_init = df[df.fold == 4].copy()\nval_df_processed = process_val_test_data(val_df_init, seq_len=FULL_LEN)\n\n# Process test data (original method)\ndf_test_processed = process_val_test_data(df_test.copy(), seq_len=FULL_LEN)\n\n# Create final datasets\ntrain_columns = ['input_ids', 'attention_mask', 'token_type_ids', 'labels', \n                 'content', 'trigger_words', 'offset_mapping']\nval_columns = ['input_ids', 'token_type_ids', 'attention_mask','offset_mapping', 'labels', 'content',  'trigger_words' ]\ntest_columns = ['input_ids', 'token_type_ids', 'attention_mask','offset_mapping', 'labels', 'content' ]\n\nds_train = Dataset.from_pandas(train_df_processed[train_columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(val_df_processed[val_columns].reset_index(drop=True))\nds_test = Dataset.from_pandas(df_test_processed[test_columns].reset_index(drop=True))\n\n\n# to calculate positive class balance later\ndf['seq_labels'] = df.progress_apply(lambda row: convert_val_to_seq_labeling(row['content'], tokenizer, row['trigger_words'], seq_len=FULL_LEN), axis=1)\nfor column in df.seq_labels.iloc[0].keys():\n    df[column] = df.seq_labels.apply(lambda x: x.get(column))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:30.053424Z","iopub.execute_input":"2025-02-06T10:48:30.053741Z","iopub.status.idle":"2025-02-06T10:48:52.732240Z","shell.execute_reply.started":"2025-02-06T10:48:30.053697Z","shell.execute_reply":"2025-02-06T10:48:52.731307Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3058 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0647e8bd4544035a04a416507917f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/764 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d1edf008a04ba4812b7f0240355da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c4bd939e814c7bb283025a6b444250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3822 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c69e48c272942ed96d9589892aa8ba2"}},"metadata":{}}],"execution_count":6},{"id":"b7a03884-e6e8-4c8d-9289-c622c88ab449","cell_type":"code","source":"def test_case():\n    #text = \"The Industrial Revolution began in Britain in the late 18th century and marked a major turning point in history.\"\n    #trigger_spans = [[4, 25], [35, 42], [50, 67], [87, 100] ]\n    text = '–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π—Å—å–∫–æ–≤–æ–≥–æ –µ–∫—Å–ø–µ—Ä—Ç–∞, –∫—É—Ö–∞—Ä–∞ –ø—É—Ç—ñ–Ω–∞ 2 —Ä–æ–∑—Ä—è–¥—É, —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞ –ø–æ —Å–Ω–∞—Ä—è–¥–Ω–æ–º—É –≥–æ–ª–æ–¥—É —Ç–∞ —Ä–µ–∫—Ç–æ—Ä–∞ –º—É–∑–∏—á–Ω–æ—ó –∞–∫–∞–¥–µ–º—ñ—ó –º—ñ–Ω–æ–±–æ—Ä–æ–Ω–∏ —Ä—Ñ –Ñ–≤–≥—î–Ω—ñ—è –ü—Ä–∏–≥–æ–∂–∏–Ω–∞. \\n–ü—Ä–∏–≥–æ–∂–∏–Ω –ø—Ä–æ–≥–Ω–æ–∑—É—î, —â–æ –Ω–µ–≤–¥–æ–≤–∑—ñ –Ω–∞—Å—Ç–∞–Ω–µ –¥–µ–Ω—å –∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –ö—Ä–∏–º—É —ñ –¥–µ–Ω—å —Ä–æ–∑–ø–∞–¥—É —Ä–æ—Å—ñ—ó. –ö–∞–∂–µ, —â–æ –ø–µ—Ä–µ–¥—É–º–æ–≤–∏ —Ü—å–æ–≥–æ –≤–∂–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ. \\n*–í—ñ–¥–µ–æ –≤–∑—è–ª–∏ –∑ –∫–∞–Ω–∞–ª—É \\n–§–î\\n. \\n@informnapalm'\n    trigger_spans = [[27, 63], [65, 88], [90, 183], [186, 308]]\n    \n    # > 512 seq from val set\n    #text = '–†–æ—Å—Å–∏–π—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ —Å–¥–µ–ª–∞–ª–æ —Ç–µ—Ä—Ä–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –≤–Ω–µ—à–Ω–µ–π –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏.\\n–ù–∞—á–∏–Ω–∞—è —Å 1994-–≥–æ –≥–æ–¥–∞ –†–§ –≤–æ—é–µ—Ç —Å –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—è–º–∏ –∏—Å–ª–∞–º—Å–∫–æ–≥–æ –º–∏—Ä–∞. –ï—Å–ª–∏ –±—Ä–∞—Ç—å —Å–æ–≤–µ—Ç—Å–∫–∏–π –ø–µ—Ä–∏–æ–¥, —Ç–æ —Å 1979-–≥–æ (—Å –≤–≤–æ–¥–∞ –≤–æ–π—Å–∫ –≤ –ê—Ñ–≥–∞–Ω–∏—Å—Ç–∞–Ω).\\n–í —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –°–ú–ò –ø–æ—Ç–µ—Ä—è–ª–∏—Å—å —Å–æ–æ–±—â–µ–Ω–∏—è –æ –µ—â—ë –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–¥–∞–≤–Ω–µ–π –≤–æ–π–Ω–µ —Å –ò—Å–ª–∞–º—Å–∫–∏–º –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ–º –≤ –°–∏—Ä–∏–∏. –ú–µ–¥–∞–ª–∏ –∑–∞ –±–æ–º–±–∞—Ä–¥–∏—Ä–æ–≤–∫–∏ —Å–∏—Ä–∏–π—Ü–µ–≤ —Ä–æ–∑–¥–∞–Ω—ã, –≥–µ—Ä–æ–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω—ã, –∞ –ø–µ—Ä–º–∞–Ω–µ–Ω—Ç–Ω—ã–µ –ø–µ—Ä–µ—Å—Ç—Ä–µ–ª–∫–∏ –Ω–∞ –°–µ–≤–µ—Ä–Ω–æ–º –ö–∞–≤–∫–∞–∑–µ —Å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –≥–∏–±–µ–ª—å—é —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏—Ö –Ω–µ –≤—ã—Å–≤–µ—á–∏–≤–∞—é—Ç—Å—è –≤ –°–ú–ò.\\n–ù–æ —ç—Ç–æ –Ω–µ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –Ω–µ—Ç —É –ö—Ä–µ–º–ª—è –Ω–µ–¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª–µ–π.\\n–û–¥–∏–Ω —Ç–µ—Ä–∞–∫—Ç –≤ –ú–æ—Å–∫–≤–µ –≤—ã—Å–≤–µ—Ç–∏–ª –≤—Å–µ –ø–æ—Ä–æ–∫–∏ –∏ –±–æ–ª–µ–∑–Ω–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ –∏ –≤—ã—Ä–æ–∂–¥–µ–Ω—á–µ—Å–∫–∏–π –∏–Ω—Ñ–∞–Ω—Ç–∏–ª–∏–∑–º —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞ –≤ —Ü–µ–ª–æ–º.\\n–ù–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±—ã—Å—Ç—Ä–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–æ–≤—É—é —É–≥—Ä–æ–∑—É, —Å–∏—Å—Ç–µ–º–Ω–æ–µ –≤–æ—Ä–æ–≤—Å—Ç–≤–æ, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ–±—â–µ—Å—Ç–≤–∞ –æ—Ç –Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤ –°–ú–ò, –≥–¥–µ –≤–æ –≤—Å—ë–º –≤–∏–Ω–æ–≤–Ω—ã —É–∫—Ä–∞–∏–Ω—Ü—ã, —Å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –æ—Ç –Ω–∞—Å —Å–æ—á—É–≤—Å—Ç–≤–∏–π, –∞ —Ç–∞–∫–∂–µ –ª–æ—è–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –ø—ã—Ç–∫–∞–º, —Å—Ç–∞–≤—à–∏–º–∏ –Ω–æ—Ä–º–æ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–º —É–∂–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤ –ø—Ä—è–º–æ–º —ç—Ñ–∏—Ä–µ ‚Äì —ç—Ç–æ –º–∞—Å—Å–æ–≤—ã–π –ø—Å–∏—Ö–æ–∑, —Å—Ç–∞–≤—à–∏–π –Ω–æ—Ä–º–æ–π.\\n–°–∏—Å—Ç–µ–º–Ω–æ –ø—Ä–æ–¥—É—Ü–∏—Ä—É—è –Ω–µ–Ω–∞–≤–∏—Å—Ç—å –∏ —Å–º–µ—Ä—Ç—å, —Ä–∞–Ω–æ –∏–ª–∏ –ø–æ–∑–¥–Ω–æ –æ–Ω–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç –∫ —Å–∞–º–∏–º –∞–≤—Ç–æ—Ä–∞–º.\\n–ö—Ä–µ–º–ª—å —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ç–µ—Ä—Ä–æ—Ä –≤ –°–∏—Ä–∏–∏ –∏ –≤ –£–∫—Ä–∞–∏–Ω–µ, —É–±–∏–≤–∞—è —Ç—ã—Å—è—á–∏ –º–∏—Ä–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω –≤ –ø—ã—Ç–æ—á–Ω—ã—Ö, –æ–±—Å—Ç—Ä–µ–ª–∏–≤–∞—è —Å–Ω–∞—Ä—è–¥–∞–º–∏ –∏ —Ä–∞–∫–µ—Ç–∞–º–∏ –≥–æ—Ä–æ–¥–∞, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –æ–±–≤–∏–Ω—è–µ—Ç –≤—Å–µ—Ö –≤–æ–∫—Ä—É–≥ –≤ —Å–ª—É—á–∏–≤—à–µ–º—Å—è –≤ –ú–æ—Å–∫–≤–µ.\\n–ì–∏–±–µ–ª—å –±–µ–∑–∑–∞—â–∏—Ç–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏—Ö, –∞ –æ—Å–æ–±–µ–Ω–Ω–æ –¥–µ—Ç–µ–π ‚Äì —ç—Ç–æ –≤—Å–µ–≥–¥–∞ —É–∂–∞—Å–Ω–∞—è —Ç—Ä–∞–≥–µ–¥–∏—è, –Ω–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –≥—Ä–∞–∂–¥–∞–Ω–∞–º, —Ç—Ä–µ–±—É—é—â–∏—Ö –±–æ–ª—å—à–æ–≥–æ —Å–æ—á—É–≤—Å—Ç–≤–∏—è, —Å—Ç–æ–∏—Ç –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∞–≤–¥–µ –≤ –≥–ª–∞–∑–∞ –∏ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –º–∏—Ä–æ–≤—ã–º —Ä–µ–∫–æ—Ä–¥—Å–º–µ–Ω–æ–º –ø–æ —É–±–∏–π—Å—Ç–≤—É –Ω–∏ –≤ —á–µ–º –Ω–µ –ø–æ–≤–∏–Ω–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–µ—Ç–µ–π, —è–≤–ª—è–µ—Ç—Å—è –∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞–Ω–∞. \\n–ß—Ç–æ-—Ç–æ –º—ã –Ω–µ —Å–ª—ã—à–∞–ª–∏ —Å–æ—á—É–≤—Å—Ç–≤–∏—è –æ —Ç–µ—Ä—Ä–æ—Ä–µ –≤ –ë—É—á–µ, –ò–∑—é–º–µ, –ú–∞—Ä–∏—É–ø–æ–ª–µ, –•–µ—Ä—Å–æ–Ω–µ, –∫–æ–≥–¥–∞ —Ä–æ—Å—Å–∏—è–Ω–µ –ø—ã—Ç–∞–ª–∏ —É —É–±–∏–≤–∞–ª–∏ –ª—é–¥–µ–π —Ç—ã—Å—è—á–∞–º–∏, —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—è –ø–æ —Å–≤–æ–∏–º —Ç–µ–ª–µ–∫–∞–Ω–∞–ª–∞–º –ø—Ä–æ ¬´–∏–Ω—Å—Ü–µ–Ω–∏—Ä–æ–≤–∫–∏¬ª –∏ ¬´–∑–≤–µ—Ä—Å—Ç–≤–∞ –ê–∑–æ–≤–∞¬ª.\\n–†–µ–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–≥–æ–≤–∞—Ä–∏–≤–∞–Ω–∏—è –µ—ë —Å–ª–æ–≤–∞–º–∏, –∏ –Ω–æ–≤—ã–π —Ñ–∞–∫—Ç–æ—Ä –≤ –≤–∏–¥–µ –º–µ—Å—Ç–∏ –ò–ì –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–∞–∫–∏–º –∂–µ —Å–∏—Å—Ç–µ–º–Ω—ã–º, –∫–∞–∫ –∏ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π —Ç–µ—Ä—Ä–æ—Ä.\\nüëâ\\n \\n–ü–æ–ª–Ω–æ–µ –≤–∏–¥–µ–æ.'\n    #trigger_spans = [[0, 80], [1034, 1049], [1076, 1105], [1171, 1187], [1209, 1248], [1348, 1396], [1554, 1620], [1738, 1778], [1985, 2002]]\n    for i in trigger_spans:\n        print(text[i[0]:i[1]])\n    \n    chunk_size = 8 # 512\n\n    # Tokenize and chunk\n    chunks = convert_to_seq_labeling(text, tokenizer, trigger_spans, chunk_size)\n\n    # Print results\n    for i, chunk in enumerate(chunks):\n        print(f\"\\nChunk {i+1}:\")\n        print(f\"Tokens: {tokenizer.convert_ids_to_tokens(chunk['input_ids'])}\")\n        print(f\"Offsets: {chunk['offset_mapping']}\")\n        print(f\"Labels: {chunk['labels']}\")\n\ntest_case()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:52.733127Z","iopub.execute_input":"2025-02-06T10:48:52.733662Z","iopub.status.idle":"2025-02-06T10:48:52.754235Z","shell.execute_reply.started":"2025-02-06T10:48:52.733617Z","shell.execute_reply":"2025-02-06T10:48:52.753212Z"}},"outputs":[{"name":"stdout","text":"–≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π—Å—å–∫–æ–≤–æ–≥–æ –µ–∫—Å–ø–µ—Ä—Ç–∞\n–∫—É—Ö–∞—Ä–∞ –ø—É—Ç—ñ–Ω–∞ 2 —Ä–æ–∑—Ä—è–¥—É\n—Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞ –ø–æ —Å–Ω–∞—Ä—è–¥–Ω–æ–º—É –≥–æ–ª–æ–¥—É —Ç–∞ —Ä–µ–∫—Ç–æ—Ä–∞ –º—É–∑–∏—á–Ω–æ—ó –∞–∫–∞–¥–µ–º—ñ—ó –º—ñ–Ω–æ–±–æ—Ä–æ–Ω–∏ —Ä—Ñ –Ñ–≤–≥—î–Ω—ñ—è –ü—Ä–∏–≥–æ–∂–∏–Ω–∞\n–ü—Ä–∏–≥–æ–∂–∏–Ω –ø—Ä–æ–≥–Ω–æ–∑—É—î, —â–æ –Ω–µ–≤–¥–æ–≤–∑—ñ –Ω–∞—Å—Ç–∞–Ω–µ –¥–µ–Ω—å –∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –ö—Ä–∏–º—É —ñ –¥–µ–Ω—å —Ä–æ–∑–ø–∞–¥—É —Ä–æ—Å—ñ—ó. –ö–∞–∂–µ, —â–æ –ø–µ—Ä–µ–¥—É–º–æ–≤–∏ —Ü—å–æ–≥–æ –≤–∂–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ\n\nChunk 1:\nTokens: ['[CLS]', '‚ñÅ–ù–æ–≤–∏', '–π', '‚ñÅ', '–æ–≥–ª—è–¥', '‚ñÅ–º–∞', '–ø–∏', '[SEP]']\nOffsets: [(0, 0), (0, 4), (4, 5), (5, 6), (6, 11), (11, 14), (14, 16), (0, 0)]\nLabels: [0, 0, 0, 0, 0, 0, 0, 0]\n\nChunk 2:\nTokens: ['[CLS]', '‚ñÅDeep', 'State', '‚ñÅ–≤—ñ–¥', '‚ñÅ', '—Ä–æ—Å—ñ–π—Å—å–∫', '–æ–≥–æ', '[SEP]']\nOffsets: [(0, 0), (16, 21), (21, 26), (26, 30), (30, 31), (31, 39), (39, 42), (0, 0)]\nLabels: [0, 0, 0, 1, 1, 1, 1, 0]\n\nChunk 3:\nTokens: ['[CLS]', '‚ñÅ–≤—ñ–π—Å—å–∫–æ–≤', '–æ–≥–æ', '‚ñÅ–µ–∫—Å–ø–µ—Ä—Ç', '–∞', ',', '‚ñÅ–∫—É—Ö', '[SEP]']\nOffsets: [(0, 0), (42, 51), (51, 54), (54, 62), (62, 63), (63, 64), (64, 68), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 0, 1, 0]\n\nChunk 4:\nTokens: ['[CLS]', '–∞—Ä–∞', '‚ñÅ–ø—É—Ç', '—ñ–Ω–∞', '‚ñÅ2', '‚ñÅ—Ä–æ–∑—Ä', '—è–¥—É', '[SEP]']\nOffsets: [(0, 0), (68, 71), (71, 75), (75, 78), (78, 80), (80, 85), (85, 88), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 5:\nTokens: ['[CLS]', ',', '‚ñÅ—Å–ø–µ—Ü—ñ–∞–ª', '—ñ—Å—Ç–∞', '‚ñÅ–ø–æ', '‚ñÅ—Å', '–Ω–∞—Ä—è–¥', '[SEP]']\nOffsets: [(0, 0), (88, 89), (89, 97), (97, 101), (101, 104), (104, 106), (106, 111), (0, 0)]\nLabels: [0, 0, 1, 1, 1, 1, 1, 0]\n\nChunk 6:\nTokens: ['[CLS]', '–Ω–æ–º—É', '‚ñÅ–≥–æ–ª–æ–¥', '—É', '‚ñÅ—Ç–∞', '‚ñÅ', '—Ä–µ–∫—Ç–æ—Ä', '[SEP]']\nOffsets: [(0, 0), (111, 115), (115, 121), (121, 122), (122, 125), (125, 126), (126, 132), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 7:\nTokens: ['[CLS]', '–∞', '‚ñÅ–º—É–∑', '–∏—á–Ω–æ—ó', '‚ñÅ–∞–∫–∞–¥–µ–º', '—ñ—ó', '‚ñÅ', '[SEP]']\nOffsets: [(0, 0), (132, 133), (133, 137), (137, 142), (142, 149), (149, 151), (151, 152), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 8:\nTokens: ['[CLS]', '–º—ñ–Ω', '–æ–±–æ—Ä–æ–Ω', '–∏', '‚ñÅ', '—Ä—Ñ', '‚ñÅ–Ñ–≤', '[SEP]']\nOffsets: [(0, 0), (152, 155), (155, 161), (161, 162), (162, 163), (163, 165), (165, 168), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 9:\nTokens: ['[CLS]', '–≥', '—î', '–Ω—ñ—è', '‚ñÅ–ü—Ä–∏', '–≥–æ', '–∂–∏–Ω–∞', '[SEP]']\nOffsets: [(0, 0), (168, 169), (169, 170), (170, 173), (173, 177), (177, 179), (179, 183), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 10:\nTokens: ['[CLS]', '.', '‚ñÅ–ü—Ä–∏', '–≥–æ', '–∂–∏–Ω', '‚ñÅ–ø—Ä–æ–≥–Ω–æ–∑', '—É—î', '[SEP]']\nOffsets: [(0, 0), (183, 184), (185, 189), (189, 191), (191, 194), (194, 202), (202, 204), (0, 0)]\nLabels: [0, 0, 1, 1, 1, 1, 1, 0]\n\nChunk 11:\nTokens: ['[CLS]', ',', '‚ñÅ—â–æ', '‚ñÅ–Ω–µ–≤', '–¥–æ–≤', '–∑—ñ', '‚ñÅ–Ω–∞', '[SEP]']\nOffsets: [(0, 0), (204, 205), (205, 208), (208, 212), (212, 215), (215, 217), (217, 220), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 12:\nTokens: ['[CLS]', '—Å—Ç–∞–Ω–µ', '‚ñÅ–¥–µ–Ω—å', '‚ñÅ–∑', '–≤—ñ–ª—å–Ω', '–µ–Ω–Ω—è', '‚ñÅ–ö—Ä–∏', '[SEP]']\nOffsets: [(0, 0), (220, 225), (225, 230), (230, 232), (232, 237), (237, 241), (241, 245), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 13:\nTokens: ['[CLS]', '–º—É', '‚ñÅ', '—ñ', '‚ñÅ–¥–µ–Ω—å', '‚ñÅ—Ä–æ–∑', '–ø–∞–¥', '[SEP]']\nOffsets: [(0, 0), (245, 247), (247, 248), (248, 249), (249, 254), (254, 258), (258, 261), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 14:\nTokens: ['[CLS]', '—É', '‚ñÅ—Ä–æ', '—Å—ñ—ó', '.', '‚ñÅ–ö–∞', '–∂–µ', '[SEP]']\nOffsets: [(0, 0), (261, 262), (262, 265), (265, 268), (268, 269), (269, 272), (272, 274), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 15:\nTokens: ['[CLS]', ',', '‚ñÅ—â–æ', '‚ñÅ–ø–µ—Ä–µ–¥', '—É–º–æ–≤', '–∏', '‚ñÅ', '[SEP]']\nOffsets: [(0, 0), (274, 275), (275, 278), (278, 284), (284, 288), (288, 289), (289, 290), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 16:\nTokens: ['[CLS]', '—Ü—å', '–æ–≥–æ', '‚ñÅ–≤', '–∂–µ', '‚ñÅ—Å—Ç–≤–æ—Ä', '–µ–Ω—ñ', '[SEP]']\nOffsets: [(0, 0), (290, 292), (292, 295), (295, 297), (297, 299), (299, 305), (305, 308), (0, 0)]\nLabels: [0, 1, 1, 1, 1, 1, 1, 0]\n\nChunk 17:\nTokens: ['[CLS]', '.', '‚ñÅ*', '–í', '—ñ–¥–µ–æ', '‚ñÅ–≤–∑', '—è–ª–∏', '[SEP]']\nOffsets: [(0, 0), (308, 309), (310, 312), (312, 313), (313, 317), (317, 320), (320, 323), (0, 0)]\nLabels: [0, 0, 0, 0, 0, 0, 0, 0]\n\nChunk 18:\nTokens: ['[CLS]', '‚ñÅ–∑', '‚ñÅ–∫–∞–Ω–∞–ª', '—É', '‚ñÅ–§', '–î', '‚ñÅ', '[SEP]']\nOffsets: [(0, 0), (323, 325), (325, 331), (331, 332), (333, 335), (335, 336), (336, 337), (0, 0)]\nLabels: [0, 0, 0, 0, 0, 0, 0, 0]\n\nChunk 19:\nTokens: ['[CLS]', '.', '‚ñÅ@', 'inform', 'na', 'palm', '[SEP]']\nOffsets: [(0, 0), (337, 338), (339, 341), (341, 347), (347, 349), (349, 353), (0, 0)]\nLabels: [0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":7},{"id":"42eb7cde-798e-49d9-8fd6-3fdcb65d64cd","cell_type":"code","source":"max_length_train = max(len(x) for x in ds_train['input_ids'])\nmax_length_val = max(len(x) for x in ds_valid['input_ids'])\nmax_length_test = max(len(x) for x in ds_test['input_ids'])\n\nprint(max_length_train)\nprint(max_length_val)\nprint(max_length_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:52.755352Z","iopub.execute_input":"2025-02-06T10:48:52.755618Z","iopub.status.idle":"2025-02-06T10:48:53.745104Z","shell.execute_reply.started":"2025-02-06T10:48:52.755596Z","shell.execute_reply":"2025-02-06T10:48:53.744040Z"}},"outputs":[{"name":"stdout","text":"512\n1516\n1445\n","output_type":"stream"}],"execution_count":8},{"id":"07a1c206-05d8-4fa6-8735-65827a485fe7","cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\n\nEPOCHS = 1\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    PRETRAINED_MODEL,\n    id2label={0: 0, 1: 1},\n    label2id={0: 0, 1: 1},\n)\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:48:53.746020Z","iopub.execute_input":"2025-02-06T10:48:53.746277Z","iopub.status.idle":"2025-02-06T10:49:19.081522Z","shell.execute_reply.started":"2025-02-06T10:48:53.746255Z","shell.execute_reply":"2025-02-06T10:49:19.080381Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e7ada0ac38493e9e4483a7ccf883d7"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"id":"35fcc28d-281d-4a1f-931e-a88378bc6286","cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:19.083343Z","iopub.execute_input":"2025-02-06T10:49:19.084492Z","iopub.status.idle":"2025-02-06T10:49:19.097405Z","shell.execute_reply.started":"2025-02-06T10:49:19.084456Z","shell.execute_reply":"2025-02-06T10:49:19.096358Z"}},"outputs":[],"execution_count":10},{"id":"7e3b6fad-eacf-427f-b17b-ff9a4224e755","cell_type":"code","source":"os.environ['WANDB_DISABLED'] = 'true'\n\nimport math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom transformers.trainer_utils import EvalPrediction\n\n\ntrain_args = TrainingArguments(\n    output_dir='model_checkpoints_mdebertav3_binary',\n    logging_dir='./model_logs_mdebertav3_binary',\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.0,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    #bf16=True,\n    # report_to=\"wandb\",\n    optim='adamw_torch',\n    eval_strategy='steps',\n    save_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    save_steps=100,\n    save_total_limit=10,\n    metric_for_best_model='eval_f1',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:19.098410Z","iopub.execute_input":"2025-02-06T10:49:19.098761Z","iopub.status.idle":"2025-02-06T10:49:22.245766Z","shell.execute_reply.started":"2025-02-06T10:49:19.098732Z","shell.execute_reply":"2025-02-06T10:49:22.244865Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":11},{"id":"006bb1d7-e8cf-4b56-8323-14b40552494e","cell_type":"code","source":"# import wandb\n\n# # Initialize with team/entity\n# wandb.init(\n#     project=\"unlp-span-ident-task\",\n#     entity=\"bazdyrev99-igor-sikorsky-kyiv-polytechnic-institute\", \n#     name='mdebertav3-binary'\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:22.246827Z","iopub.execute_input":"2025-02-06T10:49:22.247181Z","iopub.status.idle":"2025-02-06T10:49:22.251849Z","shell.execute_reply.started":"2025-02-06T10:49:22.247152Z","shell.execute_reply":"2025-02-06T10:49:22.250220Z"}},"outputs":[],"execution_count":12},{"id":"dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5","cell_type":"code","source":"from itertools import chain\n\npositive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\npositive_class_balance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:22.256228Z","iopub.execute_input":"2025-02-06T10:49:22.256530Z","iopub.status.idle":"2025-02-06T10:49:27.780166Z","shell.execute_reply.started":"2025-02-06T10:49:22.256506Z","shell.execute_reply":"2025-02-06T10:49:27.779010Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0.2453814229012187"},"metadata":{}}],"execution_count":13},{"id":"d7abe3ad-0619-40d3-90cc-70fd52617d8b","cell_type":"code","source":"import math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\nclass SpanEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        desired_positive_ratio: float = 0.25,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.desired_positive_ratio = desired_positive_ratio\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.desired_positive_ratio)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th\n        \n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n        eval_dataset = self.eval_dataset\n        logits, labels = eval_pred\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:27.782081Z","iopub.execute_input":"2025-02-06T10:49:27.782402Z","iopub.status.idle":"2025-02-06T10:49:27.800461Z","shell.execute_reply.started":"2025-02-06T10:49:27.782374Z","shell.execute_reply":"2025-02-06T10:49:27.799277Z"}},"outputs":[],"execution_count":14},{"id":"ab553c72-45e9-46f6-8787-dfd9a12008b5","cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:27.801603Z","iopub.execute_input":"2025-02-06T10:49:27.801981Z","iopub.status.idle":"2025-02-06T10:49:27.822792Z","shell.execute_reply.started":"2025-02-06T10:49:27.801952Z","shell.execute_reply":"2025-02-06T10:49:27.821737Z"}},"outputs":[],"execution_count":15},{"id":"e2e33e84-5eac-48fe-948e-f53acd597002","cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = SpanEvaluationTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    desired_positive_ratio=positive_class_balance\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:49:27.823773Z","iopub.execute_input":"2025-02-06T10:49:27.824068Z","iopub.status.idle":"2025-02-06T10:58:16.531983Z","shell.execute_reply.started":"2025-02-06T10:49:27.824045Z","shell.execute_reply":"2025-02-06T10:58:16.530134Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-1175221e7f1c>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='357' max='1025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 357/1025 08:44 < 16:26, 0.68 it/s, Epoch 1.74/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Thold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.403100</td>\n      <td>0.426032</td>\n      <td>0.596996</td>\n      <td>0.581819</td>\n      <td>0.589310</td>\n      <td>0.485152</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.383300</td>\n      <td>0.421938</td>\n      <td>0.603660</td>\n      <td>0.585552</td>\n      <td>0.594468</td>\n      <td>0.415859</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.418300</td>\n      <td>0.413985</td>\n      <td>0.609958</td>\n      <td>0.590771</td>\n      <td>0.600211</td>\n      <td>0.415859</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [717/717 03:10]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7474b198013d4eba8deaa3bf21d11891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a794d247f8d34e849f2bf06e4b896299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59c94fce7d574396a991272ef9ca1867"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-813b0de7c609>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdesired_positive_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_class_balance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"id":"481ae1ff-38b0-4006-a03e-afed5ed806b6","cell_type":"code","source":"FINETUNED_MODEL = '/kaggle/working/model_checkpoints_mdebertav3_binary/checkpoint-300'\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    FINETUNED_MODEL,\n    id2label={0: 0, 1: 1},\n    label2id={0: 0, 1: 1},\n)\ntokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:59:15.207754Z","iopub.execute_input":"2025-02-06T10:59:15.208126Z","iopub.status.idle":"2025-02-06T10:59:15.941123Z","shell.execute_reply.started":"2025-02-06T10:59:15.208100Z","shell.execute_reply":"2025-02-06T10:59:15.939983Z"}},"outputs":[],"execution_count":17},{"id":"d3913ada-b31b-404f-9745-7babf156ec94","cell_type":"code","source":"trainer.model = model.cuda().eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:59:20.876307Z","iopub.execute_input":"2025-02-06T10:59:20.876672Z","iopub.status.idle":"2025-02-06T10:59:21.281210Z","shell.execute_reply.started":"2025-02-06T10:59:20.876641Z","shell.execute_reply":"2025-02-06T10:59:21.280263Z"}},"outputs":[],"execution_count":18},{"id":"4800edb9-d35d-4289-ad21-b0672254edd2","cell_type":"code","source":"valid_preds = trainer.predict(ds_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:59:21.282373Z","iopub.execute_input":"2025-02-06T10:59:21.282722Z","iopub.status.idle":"2025-02-06T11:00:10.480847Z","shell.execute_reply.started":"2025-02-06T10:59:21.282686Z","shell.execute_reply":"2025-02-06T11:00:10.479356Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb04ff0e3f3417f97ab23e4f593f4b0"}},"metadata":{}}],"execution_count":19},{"id":"92bd64bc-afd9-4a8a-8de4-514824f6d4a1","cell_type":"code","source":"valid_preds.predictions.shape, valid_preds.label_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:00:10.482655Z","iopub.execute_input":"2025-02-06T11:00:10.483050Z","iopub.status.idle":"2025-02-06T11:00:10.488956Z","shell.execute_reply.started":"2025-02-06T11:00:10.483021Z","shell.execute_reply":"2025-02-06T11:00:10.488024Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"((764, 1516, 2), (764, 1516))"},"metadata":{}}],"execution_count":20},{"id":"fc734f81-5211-4312-b248-f1da909f69d6","cell_type":"code","source":"trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:00:10.490391Z","iopub.execute_input":"2025-02-06T11:00:10.490834Z","iopub.status.idle":"2025-02-06T11:00:33.439764Z","shell.execute_reply.started":"2025-02-06T11:00:10.490788Z","shell.execute_reply":"2025-02-06T11:00:33.438552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30b5a2bd95944bb48b4d6065d6b61552"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'precision': 0.6099580336256669,\n 'recall': 0.5907710062284677,\n 'f1': 0.600211220553053,\n 'thold': 0.41585858585858587}"},"metadata":{}}],"execution_count":21},{"id":"dfdd22ce-5c37-41b6-bf6d-ab1e50791687","cell_type":"code","source":"test_preds = trainer.predict(ds_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:00:33.440745Z","iopub.execute_input":"2025-02-06T11:00:33.441130Z","iopub.status.idle":"2025-02-06T11:06:32.789034Z","shell.execute_reply.started":"2025-02-06T11:00:33.441090Z","shell.execute_reply":"2025-02-06T11:06:32.788048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a9eff3d71b42a88d325ad1b51b161f"}},"metadata":{}}],"execution_count":22},{"id":"23204c4d-70aa-4b44-9edd-a62794e4be1b","cell_type":"code","source":"test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:06:32.789977Z","iopub.execute_input":"2025-02-06T11:06:32.790246Z","iopub.status.idle":"2025-02-06T11:06:33.278449Z","shell.execute_reply.started":"2025-02-06T11:06:32.790224Z","shell.execute_reply":"2025-02-06T11:06:33.277399Z"}},"outputs":[],"execution_count":23},{"id":"e950513f-2a4d-4e6b-ac59-6e27836023a3","cell_type":"code","source":"trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:06:33.280492Z","iopub.execute_input":"2025-02-06T11:06:33.280854Z","iopub.status.idle":"2025-02-06T11:09:19.030064Z","shell.execute_reply.started":"2025-02-06T11:06:33.280811Z","shell.execute_reply":"2025-02-06T11:09:19.028968Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.39606060606060606"},"metadata":{}}],"execution_count":24},{"id":"d9bec8dd-19c1-441b-b467-aa0238dc1af5","cell_type":"code","source":"# optimal th on (val set + test set) / 2\nfinal_th = (0.4158+0.396)/2\nfinal_th","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:13:56.673405Z","iopub.execute_input":"2025-02-06T11:13:56.673860Z","iopub.status.idle":"2025-02-06T11:13:56.680489Z","shell.execute_reply.started":"2025-02-06T11:13:56.673824Z","shell.execute_reply":"2025-02-06T11:13:56.679184Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.40590000000000004"},"metadata":{}}],"execution_count":26},{"id":"042dc4ff-378c-46ef-bbe4-7613de11f19c","cell_type":"code","source":"def inference_aggregation(probabilities, labels, offset_mappings, thold):\n    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n    ]\n    pred_spans_all = []\n    for pred, offsets in zip(true_predictions, offset_mappings):\n        samplewise_spans = []\n        current_span = None\n        for token_label, span in zip(pred, offsets):\n            if token_label == 1:  # If the current token is labeled as an entity (1)\n                if current_span is None:\n                    current_span = [span[0], span[1]]  # Start a new span\n                else:\n                    current_span[1] = span[1]  # Extend the span to include the current token\n            else:  # If token_label == 0 (not an entity)\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))  # Save completed span\n                    current_span = None  # Reset for the next entity\n        \n                    # If the last token was part of a span, save it\n        if current_span is not None:\n            samplewise_spans.append(tuple(current_span))\n        \n        pred_spans_all.append(samplewise_spans)\n    return [str(row) for row in pred_spans_all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:13:58.619948Z","iopub.execute_input":"2025-02-06T11:13:58.620336Z","iopub.status.idle":"2025-02-06T11:13:58.628144Z","shell.execute_reply.started":"2025-02-06T11:13:58.620301Z","shell.execute_reply":"2025-02-06T11:13:58.626906Z"}},"outputs":[],"execution_count":27},{"id":"537572ee-be33-4911-bc82-8abd6b9d2823","cell_type":"code","source":"valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\nvalid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:14:00.564381Z","iopub.execute_input":"2025-02-06T11:14:00.564788Z","iopub.status.idle":"2025-02-06T11:14:01.888511Z","shell.execute_reply.started":"2025-02-06T11:14:00.564756Z","shell.execute_reply":"2025-02-06T11:14:01.887166Z"}},"outputs":[],"execution_count":28},{"id":"bdc10345-650a-4e78-8e7b-2f923faefe8b","cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    # print(solution)\n    # print()\n    # print(submission)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n        # print(true_spans)\n        # print()\n        # print(pred_spans)\n        # print()\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        # print(true_tokens)\n        # print()\n        # print(pred_tokens)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    # print(true_tokens)\n    # print()\n    # print(pred_tokens)\n    \n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:14:02.973314Z","iopub.execute_input":"2025-02-06T11:14:02.973880Z","iopub.status.idle":"2025-02-06T11:14:02.985245Z","shell.execute_reply.started":"2025-02-06T11:14:02.973824Z","shell.execute_reply":"2025-02-06T11:14:02.983968Z"}},"outputs":[],"execution_count":29},{"id":"ffbb019a-659b-4bd8-a84f-b1c3ceb5698f","cell_type":"code","source":"from copy import deepcopy\n\ndf_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\ndf_pred = deepcopy(df_gt)\ndf_pred['trigger_words'] = valid_results\nscore(df_gt, df_pred, row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:14:05.014117Z","iopub.execute_input":"2025-02-06T11:14:05.014441Z","iopub.status.idle":"2025-02-06T11:14:05.133035Z","shell.execute_reply.started":"2025-02-06T11:14:05.014416Z","shell.execute_reply":"2025-02-06T11:14:05.132088Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.6035708682950821"},"metadata":{}}],"execution_count":30},{"id":"b4a952c8-9492-4b3a-819b-a061f03657d4","cell_type":"code","source":"# test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:58:16.549524Z","iopub.status.idle":"2025-02-06T10:58:16.549873Z","shell.execute_reply":"2025-02-06T10:58:16.549739Z"}},"outputs":[],"execution_count":null},{"id":"e2f59c7e-a282-4609-921e-b6879978818f","cell_type":"code","source":"# ss = pd.read_csv('sample_submission.csv')\n# ss['trigger_words'] = test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:58:16.550565Z","iopub.status.idle":"2025-02-06T10:58:16.550874Z","shell.execute_reply":"2025-02-06T10:58:16.550748Z"}},"outputs":[],"execution_count":null},{"id":"5e832f9a-ef93-4400-9b67-5bf843a90205","cell_type":"code","source":"# ss.to_csv('submissions/mdebertav3-binary-cv0.605.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T10:58:16.551841Z","iopub.status.idle":"2025-02-06T10:58:16.552165Z","shell.execute_reply":"2025-02-06T10:58:16.552028Z"}},"outputs":[],"execution_count":null},{"id":"615fa661-1641-42f0-8752-8ed8ab006bc0","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}