{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10668454,"sourceType":"datasetVersion","datasetId":6607405}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f9906349-e332-40f3-a0a8-a766fad53464","cell_type":"code","source":"!pip install bitsandbytes==0.43.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:41:25.173529Z","iopub.execute_input":"2025-02-26T08:41:25.173795Z","iopub.status.idle":"2025-02-26T08:41:35.759799Z","shell.execute_reply.started":"2025-02-26T08:41:25.173766Z","shell.execute_reply":"2025-02-26T08:41:35.758975Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.43.2\n  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.2) (2.5.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.2) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.43.2) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes==0.43.2) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.2) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.43.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.43.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bitsandbytes==0.43.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bitsandbytes==0.43.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bitsandbytes==0.43.2) (2024.2.0)\nDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.2\n","output_type":"stream"}],"execution_count":1},{"id":"0f36883a-1d31-40ba-8d7f-bfc46f02fa52","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hugginface_key\")\nsecret_value_1 = user_secrets.get_secret(\"wandb_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:41:35.760706Z","iopub.execute_input":"2025-02-26T08:41:35.760930Z","iopub.status.idle":"2025-02-26T08:41:36.367789Z","shell.execute_reply.started":"2025-02-26T08:41:35.760911Z","shell.execute_reply":"2025-02-26T08:41:36.366885Z"}},"outputs":[],"execution_count":2},{"id":"11242624-d025-4abe-8af0-7ed5335f83ae","cell_type":"code","source":"from huggingface_hub import login\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:41:36.368693Z","iopub.execute_input":"2025-02-26T08:41:36.368973Z","iopub.status.idle":"2025-02-26T08:41:37.201110Z","shell.execute_reply.started":"2025-02-26T08:41:36.368940Z","shell.execute_reply":"2025-02-26T08:41:37.200449Z"}},"outputs":[],"execution_count":3},{"id":"9283525b-ba75-467a-8b90-9c190ce7f885","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"/kaggle/input/unlp-2025-shared-task-span-identification/train.parquet\")#'train.parquet')\ncv = pd.read_csv(\"/kaggle/input/span-ident-cv-split-csv/cv_split.csv\")#'cv_split.csv')\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(\"/kaggle/input/unlp-2025-shared-task-span-identification/test.csv\")#'test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:41:37.202795Z","iopub.execute_input":"2025-02-26T08:41:37.203104Z","iopub.status.idle":"2025-02-26T08:41:38.003890Z","shell.execute_reply.started":"2025-02-26T08:41:37.203082Z","shell.execute_reply":"2025-02-26T08:41:38.002973Z"}},"outputs":[],"execution_count":4},{"id":"443e4c9f-7dba-4634-a8b0-5b65f78044b3","cell_type":"code","source":"import spacy\n\nfrom spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\nfrom tqdm.autonotebook import tqdm\ntqdm.pandas()\n\ndf.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:41:38.004902Z","iopub.execute_input":"2025-02-26T08:41:38.005155Z","iopub.status.idle":"2025-02-26T08:41:44.100105Z","shell.execute_reply.started":"2025-02-26T08:41:38.005134Z","shell.execute_reply":"2025-02-26T08:41:44.099434Z"}},"outputs":[],"execution_count":5},{"id":"3de39a37-a1fb-49a7-af42-f78be7037717","cell_type":"code","source":"PRETRAINED_MODEL =\"unsloth/gemma-2-2b-it-bnb-4bit\"\n#\"unsloth/gemma-2-9b-it-bnb-4bit\"\n#\"unsloth/gemma-2-2b-it-bnb-4bit\"\n#'google/gemma-2-2b-it'\n#'google/gemma-2-9b-it'\n\nMAX_LEN = 1400","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:44:31.173208Z","iopub.execute_input":"2025-02-26T08:44:31.173495Z","iopub.status.idle":"2025-02-26T08:44:31.177077Z","shell.execute_reply.started":"2025-02-26T08:44:31.173473Z","shell.execute_reply":"2025-02-26T08:44:31.176289Z"}},"outputs":[],"execution_count":10},{"id":"db0d2f0e-3850-4750-b6e0-d8ff71910992","cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:44:34.905258Z","iopub.execute_input":"2025-02-26T08:44:34.905545Z","iopub.status.idle":"2025-02-26T08:44:36.325956Z","shell.execute_reply.started":"2025-02-26T08:44:34.905522Z","shell.execute_reply":"2025-02-26T08:44:36.325261Z"}},"outputs":[],"execution_count":11},{"id":"020ee767-721c-4ae4-867d-f880cb02d398","cell_type":"code","source":"def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n    tokenized_output = tokenizer(\n        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n        truncation=True, padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n\n    # Get subword tokenized versions of the text\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    \n    # Initialize labels as 'O'\n    labels = [0] * len(tokens)\n\n    if trigger_spans is not None:\n        # Assign 'TRIGGER' to overlapping tokens\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n                    labels[i] = 1\n\n    tokenized_output['labels'] = labels\n    # tokenized_output['token_strings'] = token_strings\n    return tokenized_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:44:37.331390Z","iopub.execute_input":"2025-02-26T08:44:37.331727Z","iopub.status.idle":"2025-02-26T08:44:37.336875Z","shell.execute_reply.started":"2025-02-26T08:44:37.331699Z","shell.execute_reply":"2025-02-26T08:44:37.336017Z"}},"outputs":[],"execution_count":12},{"id":"c668d782-c0ef-42ae-a366-c6a2fe032d25","cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ntqdm.pandas()\n\ndf['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\nfor column in df.seq_labels.iloc[0].keys():\n    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n\ndf_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\nfor column in df_test.seq_labels.iloc[0].keys():\n    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:44:39.116882Z","iopub.execute_input":"2025-02-26T08:44:39.117172Z","iopub.status.idle":"2025-02-26T08:44:46.554784Z","shell.execute_reply.started":"2025-02-26T08:44:39.117150Z","shell.execute_reply":"2025-02-26T08:44:46.553769Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3822 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57fbb9aba4b14807a5b47c7cbe025bba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a857443dde441ebcfe0f7425937d43"}},"metadata":{}}],"execution_count":13},{"id":"3d0f5bb9-fa7f-40c2-9c18-1d180f59b57f","cell_type":"code","source":"from datasets import Dataset\nimport numpy as np\n\ndf['is_valid'] = df.fold == 4\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words']\nds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content']\nds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T08:44:46.556178Z","iopub.execute_input":"2025-02-26T08:44:46.556514Z","iopub.status.idle":"2025-02-26T08:44:49.021101Z","shell.execute_reply.started":"2025-02-26T08:44:46.556481Z","shell.execute_reply":"2025-02-26T08:44:49.020119Z"}},"outputs":[],"execution_count":14},{"id":"5f10ef27-dd8e-4d92-b3cb-c50cf82a3f73","cell_type":"code","source":"import torch\nfrom transformers import Gemma2ForTokenClassification, LlamaForTokenClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n\nid2label = {0: 0, 1: 1}\nlabel2id = {0: 0, 1: 1}\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=False,\n   bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = Gemma2ForTokenClassification.from_pretrained(\n    PRETRAINED_MODEL,\n    id2label=id2label,\n    label2id=label2id,\n    torch_dtype=torch.float16,\n    device_map=\"cuda:0\",\n    quantization_config=nf4_config\n)\n\nlora_config = LoraConfig(\n    r=64,  # the dimension of the low-rank matrices\n    lora_alpha=32, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout=0.05, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.TOKEN_CLS,\n    target_modules=['o_proj', 'v_proj', \"q_proj\", \"k_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n) \n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:19:51.762436Z","iopub.execute_input":"2025-02-22T15:19:51.763074Z","iopub.status.idle":"2025-02-22T15:20:52.307574Z","shell.execute_reply.started":"2025-02-22T15:19:51.763037Z","shell.execute_reply":"2025-02-22T15:20:52.306840Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df336560e2f4694acd0cabfe6f032d8"}},"metadata":{}},{"name":"stderr","text":"Some weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at unsloth/gemma-2-2b-it-bnb-4bit and are newly initialized: ['score.bias', 'score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 83,071,490 || all params: 2,697,417,988 || trainable%: 3.0797\n","output_type":"stream"}],"execution_count":17},{"id":"590521f4-b55f-4573-85c4-6429979a39fa","cell_type":"code","source":"# for name, param in model.named_parameters():\n#     print(name, param.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"35fcc28d-281d-4a1f-931e-a88378bc6286","cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:21:23.157718Z","iopub.execute_input":"2025-02-22T15:21:23.158374Z","iopub.status.idle":"2025-02-22T15:21:23.166722Z","shell.execute_reply.started":"2025-02-22T15:21:23.158344Z","shell.execute_reply":"2025-02-22T15:21:23.166025Z"}},"outputs":[],"execution_count":18},{"id":"7e3b6fad-eacf-427f-b17b-ff9a4224e755","cell_type":"code","source":"import math\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom transformers.trainer_utils import EvalPrediction\n\n\ntrain_args = TrainingArguments(\n    output_dir='model_checkpoints_gemma2_2b_binary',\n    logging_dir='./model_logs_gemma2_2b_binary',\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.0,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,#2\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=2,#4\n    # bf16=True,\n    report_to=\"wandb\",\n    optim='adamw_8bit',\n    eval_strategy='steps',\n    save_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=20,\n    save_steps=200,\n    save_total_limit=10,\n    metric_for_best_model='eval_f1',\n    greater_is_better=True,\n    load_best_model_at_end=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:22:19.542469Z","iopub.execute_input":"2025-02-22T15:22:19.542814Z","iopub.status.idle":"2025-02-22T15:22:20.859853Z","shell.execute_reply.started":"2025-02-22T15:22:19.542786Z","shell.execute_reply":"2025-02-22T15:22:20.858962Z"}},"outputs":[],"execution_count":19},{"id":"006bb1d7-e8cf-4b56-8323-14b40552494e","cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_1)\n\n# Initialize with team/entity\nwandb.init(\n    project=\"unlp-span-ident-task\",\n    entity=\"IASA-BA-Diploma-Ivan-Bashtovyi\", \n    name='unsloth-gemma2-2b-binary-full-seq',\n    settings=wandb.Settings(init_timeout=180)  # Increase timeout\n)\n\nimport os\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:22:40.741955Z","iopub.execute_input":"2025-02-22T15:22:40.742291Z","iopub.status.idle":"2025-02-22T15:22:55.335208Z","shell.execute_reply.started":"2025-02-22T15:22:40.742258Z","shell.execute_reply":"2025-02-22T15:22:55.334565Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivanbashtovyi1\u001b[0m (\u001b[33mIASA-BA-Diploma-Ivan-Bashtovyi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250222_152248-6c7f3t23</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/6c7f3t23' target=\"_blank\">unsloth-gemma2-2b-binary-full-seq</a></strong> to <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task' target=\"_blank\">https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/6c7f3t23' target=\"_blank\">https://wandb.ai/IASA-BA-Diploma-Ivan-Bashtovyi/unlp-span-ident-task/runs/6c7f3t23</a>"},"metadata":{}}],"execution_count":20},{"id":"dc6c7b96-9fcb-42ff-a9b9-50d27414b0e5","cell_type":"code","source":"from itertools import chain\n\npositive_class_balance = pd.Series(list(chain(*df.labels.tolist()))).mean()\npositive_class_balance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:22:55.336295Z","iopub.execute_input":"2025-02-22T15:22:55.336581Z","iopub.status.idle":"2025-02-22T15:22:55.535713Z","shell.execute_reply.started":"2025-02-22T15:22:55.336552Z","shell.execute_reply":"2025-02-22T15:22:55.534998Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0.2291704630193249"},"metadata":{}}],"execution_count":21},{"id":"d7abe3ad-0619-40d3-90cc-70fd52617d8b","cell_type":"code","source":"import math\nfrom transformers import Trainer, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\nclass SpanEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        desired_positive_ratio: float = 0.25,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.desired_positive_ratio = desired_positive_ratio\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.desired_positive_ratio)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th\n        \n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:\n        eval_dataset = self.eval_dataset\n        logits, labels = eval_pred\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:23:09.487219Z","iopub.execute_input":"2025-02-22T15:23:09.487531Z","iopub.status.idle":"2025-02-22T15:23:09.502223Z","shell.execute_reply.started":"2025-02-22T15:23:09.487498Z","shell.execute_reply":"2025-02-22T15:23:09.501409Z"}},"outputs":[],"execution_count":22},{"id":"e2e33e84-5eac-48fe-948e-f53acd597002","cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\ntrainer = SpanEvaluationTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    desired_positive_ratio=positive_class_balance\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:23:12.366190Z","iopub.execute_input":"2025-02-22T15:23:12.366479Z","iopub.status.idle":"2025-02-22T17:55:09.013108Z","shell.execute_reply.started":"2025-02-22T15:23:12.366453Z","shell.execute_reply":"2025-02-22T17:55:09.011656Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-22-fd5c10b25dba>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpanEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1070' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1070/1146 2:31:47 < 10:48, 0.12 it/s, Epoch 2.79/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Thold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.448300</td>\n      <td>0.426781</td>\n      <td>0.575162</td>\n      <td>0.563956</td>\n      <td>0.569504</td>\n      <td>0.435657</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.372400</td>\n      <td>0.419625</td>\n      <td>0.592053</td>\n      <td>0.572484</td>\n      <td>0.582104</td>\n      <td>0.415859</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.406400</td>\n      <td>0.415686</td>\n      <td>0.589642</td>\n      <td>0.585094</td>\n      <td>0.587359</td>\n      <td>0.524747</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.414800</td>\n      <td>0.419200</td>\n      <td>0.590919</td>\n      <td>0.585213</td>\n      <td>0.588052</td>\n      <td>0.405960</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.382500</td>\n      <td>0.418223</td>\n      <td>0.590361</td>\n      <td>0.583007</td>\n      <td>0.586661</td>\n      <td>0.485152</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='2868' max='2868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2868/2868 24:34]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\nThe 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b071b16e5f4ca2bb0b5b2cab585243"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_checkpoints_gemma2_2b_binary/checkpoint-200)... Done. 1.3s\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f55889800a3645d68c7923d6509ce0f8"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_checkpoints_gemma2_2b_binary/checkpoint-400)... Done. 1.2s\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34029bad1f6448d4ba70a1314bd2c779"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_checkpoints_gemma2_2b_binary/checkpoint-600)... Done. 1.2s\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7ea88529574c0ca331db2ee640619a"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_checkpoints_gemma2_2b_binary/checkpoint-800)... Done. 1.2s\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27286088bd67493691512dca51a07c80"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./model_checkpoints_gemma2_2b_binary/checkpoint-1000)... Done. 1.3s\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-813b0de7c609>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdesired_positive_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_class_balance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3653\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3707\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   2315\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1255\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    753\u001b[0m                     \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_feedforward_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_feedforward_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# See https://github.com/huggingface/transformers/pull/29402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36m_norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"id":"a1e54265-24e4-4445-abf5-1fd43614e6fb","cell_type":"code","source":"import torch\nfrom transformers import Gemma2ForTokenClassification, BitsAndBytesConfig\nfrom peft import PeftModel, get_peft_config, prepare_model_for_kbit_training, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n\n\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=False,\n   bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = Gemma2ForTokenClassification.from_pretrained(\n    PRETRAINED_MODEL,\n    id2label=id2label,\n    label2id=label2id,\n    torch_dtype=torch.float16,\n    device_map=\"cuda:0\",\n    quantization_config=nf4_config\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\nmodel = PeftModel.from_pretrained(model, \"./model_checkpoints_gemma2_2b_binary/checkpoint-800/\")\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:55:27.261523Z","iopub.execute_input":"2025-02-22T17:55:27.261851Z","iopub.status.idle":"2025-02-22T17:55:31.633757Z","shell.execute_reply.started":"2025-02-22T17:55:27.261822Z","shell.execute_reply":"2025-02-22T17:55:31.632868Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\nSome weights of Gemma2ForTokenClassification were not initialized from the model checkpoint at unsloth/gemma-2-2b-it-bnb-4bit and are newly initialized: ['score.bias', 'score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 4,610 || all params: 2,697,417,988 || trainable%: 0.0002\n","output_type":"stream"}],"execution_count":24},{"id":"d3913ada-b31b-404f-9745-7babf156ec94","cell_type":"code","source":"trainer.model = model.cuda().eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:55:31.634847Z","iopub.execute_input":"2025-02-22T17:55:31.635086Z","iopub.status.idle":"2025-02-22T17:55:31.661736Z","shell.execute_reply.started":"2025-02-22T17:55:31.635062Z","shell.execute_reply":"2025-02-22T17:55:31.660924Z"}},"outputs":[],"execution_count":25},{"id":"4800edb9-d35d-4289-ad21-b0672254edd2","cell_type":"code","source":"valid_preds = trainer.predict(ds_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:55:31.818600Z","iopub.execute_input":"2025-02-22T17:55:31.818844Z","iopub.status.idle":"2025-02-22T17:59:08.040368Z","shell.execute_reply.started":"2025-02-22T17:55:31.818824Z","shell.execute_reply":"2025-02-22T17:59:08.039359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43222c18dec4b43bf5bf97481a95a53"}},"metadata":{}}],"execution_count":26},{"id":"92bd64bc-afd9-4a8a-8de4-514824f6d4a1","cell_type":"code","source":"valid_preds.predictions.shape, valid_preds.label_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:59:08.041625Z","iopub.execute_input":"2025-02-22T17:59:08.041970Z","iopub.status.idle":"2025-02-22T17:59:08.047545Z","shell.execute_reply.started":"2025-02-22T17:59:08.041935Z","shell.execute_reply":"2025-02-22T17:59:08.046836Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((764, 1400, 2), (764, 1400))"},"metadata":{}}],"execution_count":27},{"id":"fc734f81-5211-4312-b248-f1da909f69d6","cell_type":"code","source":"trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:59:08.048867Z","iopub.execute_input":"2025-02-22T17:59:08.049064Z","iopub.status.idle":"2025-02-22T17:59:26.508852Z","shell.execute_reply.started":"2025-02-22T17:59:08.049045Z","shell.execute_reply":"2025-02-22T17:59:26.507942Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad8116604ab14c4aab1dd53bd5bdd791"}},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'precision': 0.5909192171916235,\n 'recall': 0.5852129049776827,\n 'f1': 0.5880522182525134,\n 'thold': 0.40595959595959596}"},"metadata":{}}],"execution_count":28},{"id":"dfdd22ce-5c37-41b6-bf6d-ab1e50791687","cell_type":"code","source":"test_preds = trainer.predict(ds_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T17:59:26.509767Z","iopub.execute_input":"2025-02-22T17:59:26.510025Z","iopub.status.idle":"2025-02-22T18:26:18.341758Z","shell.execute_reply.started":"2025-02-22T17:59:26.510002Z","shell.execute_reply":"2025-02-22T18:26:18.341100Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31fc0183ddb4b0a80ba1cc399e4bfbd"}},"metadata":{}}],"execution_count":29},{"id":"23204c4d-70aa-4b44-9edd-a62794e4be1b","cell_type":"code","source":"test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:26:18.342518Z","iopub.execute_input":"2025-02-22T18:26:18.342747Z","iopub.status.idle":"2025-02-22T18:26:18.752897Z","shell.execute_reply.started":"2025-02-22T18:26:18.342725Z","shell.execute_reply":"2025-02-22T18:26:18.752194Z"}},"outputs":[],"execution_count":30},{"id":"e950513f-2a4d-4e6b-ac59-6e27836023a3","cell_type":"code","source":"trainer._find_optimal_threshold(test_probabilities, test_preds.label_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:26:18.753594Z","iopub.execute_input":"2025-02-22T18:26:18.753846Z","iopub.status.idle":"2025-02-22T18:28:33.511234Z","shell.execute_reply.started":"2025-02-22T18:26:18.753824Z","shell.execute_reply":"2025-02-22T18:28:33.510537Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.36636363636363634"},"metadata":{}}],"execution_count":31},{"id":"d9bec8dd-19c1-441b-b467-aa0238dc1af5","cell_type":"code","source":"final_th = (0.405959+0.366363)/2\nfinal_th","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:00.617399Z","iopub.execute_input":"2025-02-22T18:31:00.617756Z","iopub.status.idle":"2025-02-22T18:31:00.623903Z","shell.execute_reply.started":"2025-02-22T18:31:00.617725Z","shell.execute_reply":"2025-02-22T18:31:00.623184Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.386161"},"metadata":{}}],"execution_count":32},{"id":"042dc4ff-378c-46ef-bbe4-7613de11f19c","cell_type":"code","source":"def inference_aggregation(probabilities, labels, offset_mappings, thold):\n    predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    true_predictions = [\n        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n    ]\n    pred_spans_all = []\n    for pred, offsets in zip(true_predictions, offset_mappings):\n        samplewise_spans = []\n        current_span = None\n        for token_label, span in zip(pred, offsets):\n            if token_label == 1:  # If the current token is labeled as an entity (1)\n                if current_span is None:\n                    current_span = [span[0], span[1]]  # Start a new span\n                else:\n                    current_span[1] = span[1]  # Extend the span to include the current token\n            else:  # If token_label == 0 (not an entity)\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))  # Save completed span\n                    current_span = None  # Reset for the next entity\n        \n                    # If the last token was part of a span, save it\n        if current_span is not None:\n            samplewise_spans.append(tuple(current_span))\n        \n        pred_spans_all.append(samplewise_spans)\n    return [str(row) for row in pred_spans_all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:02.757831Z","iopub.execute_input":"2025-02-22T18:31:02.758144Z","iopub.status.idle":"2025-02-22T18:31:02.765303Z","shell.execute_reply.started":"2025-02-22T18:31:02.758117Z","shell.execute_reply":"2025-02-22T18:31:02.764462Z"}},"outputs":[],"execution_count":33},{"id":"537572ee-be33-4911-bc82-8abd6b9d2823","cell_type":"code","source":"valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\nvalid_results = inference_aggregation(valid_probabilities, valid_preds.label_ids, ds_valid['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:04.641769Z","iopub.execute_input":"2025-02-22T18:31:04.642077Z","iopub.status.idle":"2025-02-22T18:31:05.734256Z","shell.execute_reply.started":"2025-02-22T18:31:04.642049Z","shell.execute_reply":"2025-02-22T18:31:05.733486Z"}},"outputs":[],"execution_count":34},{"id":"bdc10345-650a-4e78-8e7b-2f923faefe8b","cell_type":"code","source":"import pandas as pd\nimport pandas.api.types\nfrom sklearn.metrics import f1_score\nimport ast\n\n\nclass ParticipantVisibleError(Exception):\n    \"\"\"Custom exception for participant-visible errors.\"\"\"\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Compute span-level F1 score based on overlap.\n\n    Parameters:\n    - solution (pd.DataFrame): Ground truth DataFrame with row ID and token labels.\n    - submission (pd.DataFrame): Submission DataFrame with row ID and token labels.\n    - row_id_column_name (str): Column name for the row identifier.\n\n    Returns:\n    - float: The token-level weighted F1 score.\n\n    Example:\n    >>> solution = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (725, 831)], [(300, 312)], []]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     \"id\": [1, 2, 3],\n    ...     \"trigger_words\": [[(612, 622), (700, 720)], [(300, 312)], [(100, 200)]]\n    ... })\n    >>> score(solution, submission, \"id\")\n    0.16296296296296295\n    \"\"\"\n    if not all(col in solution.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Solution DataFrame must contain 'id' and 'trigger_words' columns.\")\n    if not all(col in submission.columns for col in [\"id\", \"trigger_words\"]):\n        raise ValueError(\"Submission DataFrame must contain 'id' and 'trigger_words' columns.\")\n    \n    def safe_parse_spans(trigger_words):\n        if isinstance(trigger_words, str):\n            try:\n                return ast.literal_eval(trigger_words)\n            except (ValueError, SyntaxError):\n                return []\n        if isinstance(trigger_words, (list, tuple, np.ndarray)):\n            return trigger_words\n        return []\n\n    def extract_tokens_from_spans(spans):\n        tokens = set()\n        for start, end in spans:\n            tokens.update(range(start, end))\n        return tokens\n    \n    solution = solution.copy()\n    submission = submission.copy()\n\n    solution[\"trigger_words\"] = solution[\"trigger_words\"].apply(safe_parse_spans)\n    submission[\"trigger_words\"] = submission[\"trigger_words\"].apply(safe_parse_spans)\n\n    merged = pd.merge(\n        solution,\n        submission,\n        on=\"id\",\n        suffixes=(\"_solution\", \"_submission\")\n    )\n\n    total_true_tokens = 0\n    total_pred_tokens = 0\n    overlapping_tokens = 0\n\n    for _, row in merged.iterrows():\n        true_spans = row[\"trigger_words_solution\"]\n        pred_spans = row[\"trigger_words_submission\"]\n\n        true_tokens = extract_tokens_from_spans(true_spans)\n        pred_tokens = extract_tokens_from_spans(pred_spans)\n\n        total_true_tokens += len(true_tokens)\n        total_pred_tokens += len(pred_tokens)\n        overlapping_tokens += len(true_tokens & pred_tokens)\n\n    precision = overlapping_tokens / total_pred_tokens if total_pred_tokens > 0 else 0\n    recall = overlapping_tokens / total_true_tokens if total_true_tokens > 0 else 0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:06.189917Z","iopub.execute_input":"2025-02-22T18:31:06.190203Z","iopub.status.idle":"2025-02-22T18:31:06.199634Z","shell.execute_reply.started":"2025-02-22T18:31:06.190181Z","shell.execute_reply":"2025-02-22T18:31:06.198727Z"}},"outputs":[],"execution_count":35},{"id":"ffbb019a-659b-4bd8-a84f-b1c3ceb5698f","cell_type":"code","source":"from copy import deepcopy\n\ndf_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\ndf_pred = deepcopy(df_gt)\ndf_pred['trigger_words'] = valid_results\nscore(df_gt, df_pred, row_id_column_name='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:09.037708Z","iopub.execute_input":"2025-02-22T18:31:09.038083Z","iopub.status.idle":"2025-02-22T18:31:09.134437Z","shell.execute_reply.started":"2025-02-22T18:31:09.038052Z","shell.execute_reply":"2025-02-22T18:31:09.133730Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.5935790586726836"},"metadata":{}}],"execution_count":36},{"id":"b4a952c8-9492-4b3a-819b-a061f03657d4","cell_type":"code","source":"test_results = inference_aggregation(test_probabilities, test_preds.label_ids, ds_test['offset_mapping'], final_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:10.287123Z","iopub.execute_input":"2025-02-22T18:31:10.287408Z","iopub.status.idle":"2025-02-22T18:31:16.053056Z","shell.execute_reply.started":"2025-02-22T18:31:10.287386Z","shell.execute_reply":"2025-02-22T18:31:16.052241Z"}},"outputs":[],"execution_count":37},{"id":"e2f59c7e-a282-4609-921e-b6879978818f","cell_type":"code","source":"ss = pd.read_csv(\"/kaggle/input/unlp-2025-shared-task-span-identification/sample_submission.csv\")#'sample_submission.csv')\nss['trigger_words'] = test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:16.151630Z","iopub.execute_input":"2025-02-22T18:31:16.151923Z","iopub.status.idle":"2025-02-22T18:31:16.176224Z","shell.execute_reply.started":"2025-02-22T18:31:16.151898Z","shell.execute_reply":"2025-02-22T18:31:16.175585Z"}},"outputs":[],"execution_count":38},{"id":"5e832f9a-ef93-4400-9b67-5bf843a90205","cell_type":"code","source":"ss.to_csv('unsloth-full-seq-gemma2-2b-binary-cv0.593.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T18:31:55.493016Z","iopub.execute_input":"2025-02-22T18:31:55.493307Z","iopub.status.idle":"2025-02-22T18:31:55.518649Z","shell.execute_reply.started":"2025-02-22T18:31:55.493285Z","shell.execute_reply":"2025-02-22T18:31:55.517691Z"}},"outputs":[],"execution_count":39},{"id":"43edf7bc-5a95-4174-984b-4cfabce5252d","cell_type":"code","source":"import pickle\n\npickle.dump(valid_preds, open('valid_preds_gemma2_binary.pkl', 'wb'))\npickle.dump(test_preds, open('test_preds_gemma2_binary.pkl', 'wb'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc896de7-aa34-430b-82d4-d6b103cb720d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}