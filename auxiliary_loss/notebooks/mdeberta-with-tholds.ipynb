{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89664,"databundleVersionId":10931355,"sourceType":"competition"},{"sourceId":10664686,"sourceType":"datasetVersion","datasetId":6604871}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nfrom tqdm.autonotebook import tqdm\n\ndef set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n\nset_seeds(seed=42)\ntqdm.pandas()","metadata":{"_uuid":"1dfe9b25-83da-47f0-9209-18dbd2b54ad4","_cell_guid":"cee816df-1067-4736-9235-308def338c0b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:14.962765Z","iopub.execute_input":"2025-02-06T00:35:14.963049Z","iopub.status.idle":"2025-02-06T00:35:18.533810Z","shell.execute_reply.started":"2025-02-06T00:35:14.963025Z","shell.execute_reply":"2025-02-06T00:35:18.532897Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"<ipython-input-1-c8c0fad08936>:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/unlp-2025-shared-task-span-identification/'\nCV_PATH = \"/kaggle/input/unlp25-cross-validation-split/cv_split.csv\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nPRETRAINED_MODEL = \"microsoft/mdeberta-v3-base\"\nMAX_LEN = 512","metadata":{"_uuid":"0111bc09-0f06-44ec-bbb1-87ba0c16c05a","_cell_guid":"99ddbffc-acb9-4df4-a4de-4cbb505cce27","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:18.534952Z","iopub.execute_input":"2025-02-06T00:35:18.535366Z","iopub.status.idle":"2025-02-06T00:35:18.538960Z","shell.execute_reply.started":"2025-02-06T00:35:18.535332Z","shell.execute_reply":"2025-02-06T00:35:18.538250Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data","metadata":{"_uuid":"d15467c8-74a0-4a4e-9538-fc45f7b93fde","_cell_guid":"c345ddbe-6098-44f7-8c0d-f5452f2acbc7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(DATA_PATH + \"train.parquet\")\ncv = pd.read_csv(CV_PATH)\ndf = df.merge(cv, on='id', how='left')\n\ndf_test = pd.read_csv(DATA_PATH + \"test.csv\")","metadata":{"_uuid":"fd1a8000-7ba9-436e-b753-7bf6d6a410ef","_cell_guid":"5a3b5d33-3dbd-4b3e-acd2-7b87358e2ee9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:18.540368Z","iopub.execute_input":"2025-02-06T00:35:18.540596Z","iopub.status.idle":"2025-02-06T00:35:19.078961Z","shell.execute_reply.started":"2025-02-06T00:35:18.540567Z","shell.execute_reply":"2025-02-06T00:35:19.078070Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"36de962c-497e-4ed1-b5af-f2dc26594442","_cell_guid":"f96bb3b2-7c58-4782-9444-e2b5674bb5bc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:19.080268Z","iopub.execute_input":"2025-02-06T00:35:19.080533Z","iopub.status.idle":"2025-02-06T00:35:19.101701Z","shell.execute_reply.started":"2025-02-06T00:35:19.080505Z","shell.execute_reply":"2025-02-06T00:35:19.100969Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  fold  \n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1  \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3  \n2                                    [[55, 100]]     1  \n3                                           None     2  \n4                                   [[114, 144]]     2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[loaded_language, euphoria]</td>\n      <td>[[55, 100]]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>uk</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language]</td>\n      <td>[[114, 144]]</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Targets Prep","metadata":{"_uuid":"d705e9d3-0e60-486d-b19f-1e3c4b6c5a29","_cell_guid":"bbe093d1-722e-471e-b742-35ae296e7b60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Classification","metadata":{"_uuid":"59ec63a9-088c-45fc-a8f3-245ebef7266e","_cell_guid":"ce4b51ec-d63b-438c-ae23-c45e8cc7a642","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from collections.abc import Iterable\n\ntechniques = ['straw_man', 'appeal_to_fear', 'fud', 'bandwagon', 'whataboutism', 'loaded_language', 'glittering_generalities', 'euphoria', 'cherry_picking', 'cliche']\n\nfor col in techniques:\n    df[col] = 0\n\nimport numpy as np\nfor ind, row in df.iterrows():\n    if isinstance(row['techniques'], Iterable):\n        for t in row['techniques']:\n            df.loc[ind, t] = 1\n\ndf['sequence_labels'] = list(df[techniques].values)\n# df.drop(columns=techniques, inplace=True)","metadata":{"_uuid":"552a2940-f222-48e3-9975-93caae44b4d3","_cell_guid":"7018698c-346e-48a7-a9fe-bb625be626e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:19.102531Z","iopub.execute_input":"2025-02-06T00:35:19.102836Z","iopub.status.idle":"2025-02-06T00:35:20.103237Z","shell.execute_reply.started":"2025-02-06T00:35:19.102802Z","shell.execute_reply":"2025-02-06T00:35:20.102383Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"22fed7b3-8d7f-4f2f-8a00-08011d18f74e","_cell_guid":"60512e36-a2b1-467e-8bcc-0e86a0ba24b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:20.104082Z","iopub.execute_input":"2025-02-06T00:35:20.104383Z","iopub.status.idle":"2025-02-06T00:35:20.120613Z","shell.execute_reply.started":"2025-02-06T00:35:20.104352Z","shell.execute_reply":"2025-02-06T00:35:20.119797Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n2  e6a427f1-211f-405f-bd8b-70798458d656   \n3  1647a352-4cd3-40f6-bfa1-d87d42e34eea   \n4  9c01de00-841f-4b50-9407-104e9ffb03bf   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n2  ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...   uk          True   \n3  –í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...   uk         False   \n4  –†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n2        [loaded_language, euphoria]   \n3                               None   \n4                  [loaded_language]   \n\n                                   trigger_words  fold  straw_man  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1          0   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3          0   \n2                                    [[55, 100]]     1          0   \n3                                           None     2          0   \n4                                   [[114, 144]]     2          0   \n\n   appeal_to_fear  fud  bandwagon  whataboutism  loaded_language  \\\n0               0    0          0             0                1   \n1               0    0          0             0                1   \n2               0    0          0             0                1   \n3               0    0          0             0                0   \n4               0    0          0             0                1   \n\n   glittering_generalities  euphoria  cherry_picking  cliche  \\\n0                        0         1               0       0   \n1                        0         0               1       0   \n2                        0         1               0       0   \n3                        0         0               0       0   \n4                        0         0               0       0   \n\n                  sequence_labels  \n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]  \n2  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n      <th>straw_man</th>\n      <th>appeal_to_fear</th>\n      <th>fud</th>\n      <th>bandwagon</th>\n      <th>whataboutism</th>\n      <th>loaded_language</th>\n      <th>glittering_generalities</th>\n      <th>euphoria</th>\n      <th>cherry_picking</th>\n      <th>cliche</th>\n      <th>sequence_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e6a427f1-211f-405f-bd8b-70798458d656</td>\n      <td>ü§©\\n–¢–∏–º —á–∞—Å–æ–º –π–¥–µ –µ–≤–∞–∫—É–∞—Ü—ñ—è –ë—î–ª–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ –∞–≤—Ç–æ...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[loaded_language, euphoria]</td>\n      <td>[[55, 100]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1647a352-4cd3-40f6-bfa1-d87d42e34eea</td>\n      <td>–í –£–∫—Ä–∞—ó–Ω—ñ –Ω–∞–π–±–ª–∏–∂—á–∏–º —á–∞—Å–æ–º –º–∞—é—Ç—å –Ω–∞–º—ñ—Ä –ø–æ—Å–∏–ª–∏—Ç...</td>\n      <td>uk</td>\n      <td>False</td>\n      <td>None</td>\n      <td>None</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c01de00-841f-4b50-9407-104e9ffb03bf</td>\n      <td>–†–∞—Å—á—ë—Ç—ã 122-–º–º –°–ê–£ 2–°1 \"–ì–≤–æ–∑–¥–∏–∫–∞\" 132-–π –±—Ä–∏–≥–∞–¥...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language]</td>\n      <td>[[114, 144]]</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Span","metadata":{"_uuid":"cfb25aac-35f8-453a-ab30-46ae2efd94fb","_cell_guid":"53e697bd-47d3-432b-8032-e927a3f7aa79","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:20.121326Z","iopub.execute_input":"2025-02-06T00:35:20.121597Z","iopub.status.idle":"2025-02-06T00:35:27.800829Z","shell.execute_reply.started":"2025-02-06T00:35:20.121568Z","shell.execute_reply":"2025-02-06T00:35:27.800125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26274bfd90b543119a68e8196c099e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02902e56d79a47bb8b91a6f43735504d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bab45d14ff44f6180afbbc5853cbd4e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def convert_to_seq_labeling(text, tokenizer, trigger_spans=None):\n    tokenized_output = tokenizer(\n        text, return_offsets_mapping=True, add_special_tokens=True, max_length=MAX_LEN,\n        truncation=True, padding=False\n    )\n    tokens = tokenized_output[\"input_ids\"]\n    offsets = tokenized_output[\"offset_mapping\"]\n\n    # Get subword tokenized versions of the text\n    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n\n    \n    # Initialize labels as 'O'\n    labels = [0] * len(tokens)\n\n    if trigger_spans is not None:\n        # Assign 'TRIGGER' to overlapping tokens\n        for start, end in trigger_spans:\n            for i, (tok_start, tok_end) in enumerate(offsets):\n                if tok_start == 0 and tok_end == 0:\n                    continue\n                if tok_start < end and tok_end > start:  # If token overlaps with the trigger span\n                    labels[i] = 1\n\n    tokenized_output['labels'] = labels\n    return tokenized_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:27.803575Z","iopub.execute_input":"2025-02-06T00:35:27.803966Z","iopub.status.idle":"2025-02-06T00:35:27.808976Z","shell.execute_reply.started":"2025-02-06T00:35:27.803945Z","shell.execute_reply":"2025-02-06T00:35:27.808205Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.trigger_words = df.trigger_words.apply(lambda x: [] if x is None else x)\n\ndf['seq_labels'] = df.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, row['trigger_words']), axis=1)\nfor column in df.seq_labels.iloc[0].keys():\n    df[column] = df.seq_labels.apply(lambda x: x.get(column))\n\ndf_test['seq_labels'] = df_test.progress_apply(lambda row: convert_to_seq_labeling(row['content'], tokenizer, None), axis=1)\nfor column in df_test.seq_labels.iloc[0].keys():\n    df_test[column] = df_test.seq_labels.apply(lambda x: x.get(column))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:27.810792Z","iopub.execute_input":"2025-02-06T00:35:27.811058Z","iopub.status.idle":"2025-02-06T00:35:37.376949Z","shell.execute_reply.started":"2025-02-06T00:35:27.811025Z","shell.execute_reply":"2025-02-06T00:35:37.376002Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3822 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c33a783d982440c8ecb29a4f6db4cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5735 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ddcdd924d74d758fbd1979a564c5ed"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df.head(2)","metadata":{"_uuid":"68e64a3c-0781-464d-bb88-c73f099c4a98","_cell_guid":"dfd286ad-5857-4340-aa7c-9c049b02708f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:37.377899Z","iopub.execute_input":"2025-02-06T00:35:37.378226Z","iopub.status.idle":"2025-02-06T00:35:37.442387Z","shell.execute_reply.started":"2025-02-06T00:35:37.378178Z","shell.execute_reply":"2025-02-06T00:35:37.441590Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  0bb0c7fa-101b-4583-a5f9-9d503339141c   \n1  7159f802-6f99-4e9d-97bd-6f565a4a0fae   \n\n                                             content lang  manipulative  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   uk          True   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   ru          True   \n\n                          techniques  \\\n0        [euphoria, loaded_language]   \n1  [loaded_language, cherry_picking]   \n\n                                   trigger_words  fold  straw_man  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]     1          0   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]     3          0   \n\n   appeal_to_fear  fud  ...  euphoria  cherry_picking  cliche  \\\n0               0    0  ...         1               0       0   \n1               0    0  ...         0               1       0   \n\n                  sequence_labels  \\\n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]   \n\n                                          seq_labels  \\\n0  [input_ids, token_type_ids, attention_mask, of...   \n1  [input_ids, token_type_ids, attention_mask, of...   \n\n                                           input_ids  \\\n0  [1, 55816, 544, 260, 84748, 3554, 14381, 29189...   \n1  [1, 1909, 21922, 6943, 64148, 1774, 20485, 456...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                      offset_mapping  \\\n0  [(0, 0), (0, 4), (4, 5), (5, 6), (6, 11), (11,...   \n1  [(0, 0), (0, 2), (2, 7), (7, 10), (10, 18), (1...   \n\n                                              labels  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...  \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n\n[2 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>lang</th>\n      <th>manipulative</th>\n      <th>techniques</th>\n      <th>trigger_words</th>\n      <th>fold</th>\n      <th>straw_man</th>\n      <th>appeal_to_fear</th>\n      <th>fud</th>\n      <th>...</th>\n      <th>euphoria</th>\n      <th>cherry_picking</th>\n      <th>cliche</th>\n      <th>sequence_labels</th>\n      <th>seq_labels</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>offset_mapping</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bb0c7fa-101b-4583-a5f9-9d503339141c</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>uk</td>\n      <td>True</td>\n      <td>[euphoria, loaded_language]</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n      <td>[1, 55816, 544, 260, 84748, 3554, 14381, 29189...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[(0, 0), (0, 4), (4, 5), (5, 6), (6, 11), (11,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7159f802-6f99-4e9d-97bd-6f565a4a0fae</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>ru</td>\n      <td>True</td>\n      <td>[loaded_language, cherry_picking]</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n      <td>[input_ids, token_type_ids, attention_mask, of...</td>\n      <td>[1, 1909, 21922, 6943, 64148, 1774, 20485, 456...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[(0, 0), (0, 2), (2, 7), (7, 10), (10, 18), (1...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows √ó 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Datasets","metadata":{"_uuid":"b5bfb354-29d5-4ccd-9cf9-6bddbeda581c","_cell_guid":"652ffd3a-60ba-4f46-b7cc-ca69a5f17e77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from datasets import Dataset\nimport numpy as np\n\ndf['is_valid'] = (df.fold == 4)\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content', 'trigger_words', 'sequence_labels']\nds_train = Dataset.from_pandas(df[df.is_valid==0][columns].reset_index(drop=True))\nds_valid = Dataset.from_pandas(df[df.is_valid==1][columns].reset_index(drop=True))\n\ncolumns = list(df.seq_labels.iloc[0].keys()) + ['content']\nds_test = Dataset.from_pandas(df_test[columns].reset_index(drop=True))","metadata":{"_uuid":"3a3bb6af-f65b-4675-8ef4-c02676550bf7","_cell_guid":"08edad22-fba4-4c5c-b676-365cfaec9db9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:37.443261Z","iopub.execute_input":"2025-02-06T00:35:37.443570Z","iopub.status.idle":"2025-02-06T00:35:39.821924Z","shell.execute_reply.started":"2025-02-06T00:35:37.443538Z","shell.execute_reply":"2025-02-06T00:35:39.821002Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"ds_train.to_pandas().head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:39.822863Z","iopub.execute_input":"2025-02-06T00:35:39.823328Z","iopub.status.idle":"2025-02-06T00:35:40.072802Z","shell.execute_reply.started":"2025-02-06T00:35:39.823295Z","shell.execute_reply":"2025-02-06T00:35:40.071904Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0  [1, 55816, 544, 260, 84748, 3554, 14381, 29189...   \n1  [1, 1909, 21922, 6943, 64148, 1774, 20485, 456...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                      offset_mapping  \\\n0  [[0, 0], [0, 4], [4, 5], [5, 6], [6, 11], [11,...   \n1  [[0, 0], [0, 2], [2, 7], [7, 10], [10, 18], [1...   \n\n                                              labels  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...   \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n\n                                             content  \\\n0  –ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...   \n1  –ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...   \n\n                                   trigger_words  \\\n0    [[27, 63], [65, 88], [90, 183], [186, 308]]   \n1  [[0, 40], [123, 137], [180, 251], [253, 274]]   \n\n                  sequence_labels  \n0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]  \n1  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>offset_mapping</th>\n      <th>labels</th>\n      <th>content</th>\n      <th>trigger_words</th>\n      <th>sequence_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1, 55816, 544, 260, 84748, 3554, 14381, 29189...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[[0, 0], [0, 4], [4, 5], [5, 6], [6, 11], [11,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>–ù–æ–≤–∏–π –æ–≥–ª—è–¥ –º–∞–ø–∏ DeepState –≤—ñ–¥ —Ä–æ—Å—ñ–π—Å—å–∫–æ–≥–æ –≤—ñ–π...</td>\n      <td>[[27, 63], [65, 88], [90, 183], [186, 308]]</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1, 1909, 21922, 6943, 64148, 1774, 20485, 456...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[[0, 0], [0, 2], [2, 7], [7, 10], [10, 18], [1...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n      <td>–ù–µ–¥–∞–≤–Ω–æ 95 –∫–≤–∞—Ä—Ç–∞–ª –∂—ë—Å—Ç–∫–æ –ø–æ–≥–ª—É–º–∏–ª—Å—è –Ω–∞–¥ —Ä—É—Å—Å–∫...</td>\n      <td>[[0, 40], [123, 137], [180, 251], [253, 274]]</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"14bdff46-04eb-4dcc-9b7c-04ad3fa91db2","_cell_guid":"49877ab0-b991-4efe-82b0-5d48877c2032","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\nfrom transformers import DebertaV2Model, DebertaV2PreTrainedModel","metadata":{"_uuid":"01428bd6-c61c-4af6-861e-bee770f2104d","_cell_guid":"4161e2e5-0143-4869-b756-b5efdf457f26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:40.073582Z","iopub.execute_input":"2025-02-06T00:35:40.073831Z","iopub.status.idle":"2025-02-06T00:35:51.920988Z","shell.execute_reply.started":"2025-02-06T00:35:40.073810Z","shell.execute_reply":"2025-02-06T00:35:51.920082Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class BertForTokenSequenceClassification(DebertaV2PreTrainedModel):\n    def __init__(self, model_name, num_token_labels, num_sequence_labels):\n        bert_model = DebertaV2Model.from_pretrained(model_name)\n        super().__init__(bert_model.config)\n        self.bert = bert_model\n        hidden_size = self.config.hidden_size\n\n        # Token Classification Head\n        self.token_classifier = nn.Linear(hidden_size, num_token_labels)\n\n        # Sequence Classification Head\n        self.sequence_classifier = nn.Linear(hidden_size, num_sequence_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(self, input_ids, attention_mask, labels=None, sequence_labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state  # Shape: (batch, seq_len, hidden)\n\n        # Token Classification Output (Apply to each token)\n        token_logits = self.token_classifier(sequence_output)  # (batch, seq_len, num_token_labels)\n\n        # Sequence Classification Output (Use [CLS] token's representation)\n        cls_output = sequence_output[:, 0, :]  # Take first token (CLS)\n        sequence_logits = self.sequence_classifier(cls_output)  # (batch, num_sequence_labels)\n\n        loss = None\n        if labels is not None and sequence_labels is not None:\n            token_loss_fn = nn.CrossEntropyLoss()\n            seq_loss_fn = nn.BCEWithLogitsLoss()  # For multi-label classification\n\n            token_loss = token_loss_fn(token_logits.view(-1, token_logits.shape[-1]), labels.view(-1))\n            seq_loss = seq_loss_fn(sequence_logits, sequence_labels.float())\n\n            loss = token_loss + seq_loss  # Combine losses\n\n        return {\n            \"loss\": loss,\n            \"token_logits\": token_logits,\n            \"sequence_logits\": sequence_logits,\n        }","metadata":{"_uuid":"9936ba82-7e0e-4653-8d2a-e6b152d38ba5","_cell_guid":"5ff939da-38dc-41b4-97eb-5c3d4771c110","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:51.921909Z","iopub.execute_input":"2025-02-06T00:35:51.922434Z","iopub.status.idle":"2025-02-06T00:35:51.928620Z","shell.execute_reply.started":"2025-02-06T00:35:51.922406Z","shell.execute_reply":"2025-02-06T00:35:51.927873Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Init and Test","metadata":{"_uuid":"300483b3-fecb-442d-b346-fffcd5bf751e","_cell_guid":"697962c3-ce08-4fa0-9019-7d17ce437c0e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sample = ds_train[0]\n\n# Convert input to batch format (add batch dimension)\ninput_ids = torch.tensor([sample[\"input_ids\"]])\nattention_mask = torch.tensor([sample[\"attention_mask\"]])\ntoken_labels = torch.tensor([sample[\"labels\"]])\nsequence_labels = torch.tensor([sample[\"sequence_labels\"]])","metadata":{"_uuid":"07084858-848a-462a-894b-6c37a6b7bbb2","_cell_guid":"2268950a-7760-411f-860a-949178a5c112","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:51.929651Z","iopub.execute_input":"2025-02-06T00:35:51.929993Z","iopub.status.idle":"2025-02-06T00:35:52.061981Z","shell.execute_reply.started":"2025-02-06T00:35:51.929960Z","shell.execute_reply":"2025-02-06T00:35:52.061292Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model = BertForTokenSequenceClassification(\n    model_name=PRETRAINED_MODEL,\n    num_token_labels=2,\n    num_sequence_labels=10\n)\n\nmodel","metadata":{"_uuid":"2569323d-c395-4134-9b52-886bc339308d","_cell_guid":"94ea0525-f66c-4750-9183-ecbb883ca9e0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:35:52.062917Z","iopub.execute_input":"2025-02-06T00:35:52.063223Z","iopub.status.idle":"2025-02-06T00:35:59.246331Z","shell.execute_reply.started":"2025-02-06T00:35:52.063174Z","shell.execute_reply":"2025-02-06T00:35:59.245062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5cebefcd17840c89dc88cc6ed6d27bd"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BertForTokenSequenceClassification(\n  (bert): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (token_classifier): Linear(in_features=768, out_features=2, bias=True)\n  (sequence_classifier): Linear(in_features=768, out_features=10, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Metrics","metadata":{"_uuid":"8ed49dfa-9f75-405e-bcbc-e74ac2377cad","_cell_guid":"1d3e034e-8cfe-482f-a689-af35ca573a2f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from itertools import chain\n\nTOKEN_CLASS_DISTRIBUTION = pd.Series(list(chain(*df.labels.tolist()))).mean()\nSEQUENCE_CLASS_DISTRIBUTION = df[techniques].mean().values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:59.247868Z","iopub.execute_input":"2025-02-06T00:35:59.248288Z","iopub.status.idle":"2025-02-06T00:35:59.448510Z","shell.execute_reply.started":"2025-02-06T00:35:59.248249Z","shell.execute_reply":"2025-02-06T00:35:59.447770Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import math\nfrom scipy.optimize import minimize_scalar\nfrom transformers import Trainer, pipeline, TrainingArguments\nfrom typing import Any\nfrom tqdm.autonotebook import tqdm\nfrom transformers.trainer_utils import EvalPrediction\nfrom sklearn.metrics import f1_score\n\n\ndef extract_chars_from_spans(spans):\n    \"\"\"\n    Given a list of spans (each a tuple (start, end)),\n    return a set of character indices for all spans.\n    \"\"\"\n    char_set = set()\n    for start, end in spans:\n        # Each span covers positions start, start+1, ..., end-1.\n        char_set.update(range(start, end))\n    return char_set\n\n\nclass TokenSequenceEvaluationTrainer(Trainer):\n    def __init__(\n        self,\n        model: Any = None,\n        args: TrainingArguments = None,\n        data_collator: Any = None,\n        train_dataset: Any = None,\n        eval_dataset: Any = None,\n        tokenizer: Any = None,\n        sequence_class_distribution: list[float] = [0.1]*10,\n        token_class_distribution: float = 0.25, # mean\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Trainer with our custom compute_metrics.\n        \"\"\"\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            compute_metrics=self.compute_metrics,  # assign our custom compute_metrics\n            **kwargs,\n        )\n        self.sequence_class_distribution = sequence_class_distribution\n        self.token_class_distribution = token_class_distribution\n        \n    def compute_metrics(self, eval_pred: EvalPrediction) -> dict:        \n        token_logits, sequence_logits = eval_pred.predictions\n        token_labels, sequence_labels = eval_pred.label_ids\n\n        # Sequence classification metrics (multi-label)\n        sequence_metrics = self._compute_sequence_metrics(sequence_logits, sequence_labels)\n    \n        # Token classification metrics\n        token_metrics = self._compute_token_metrics(token_logits, token_labels)\n        \n        return {\n            **{f\"sequence_{key}\": value for key, value in sequence_metrics.items()},\n            **{f\"token_{key}\": value for key, value in token_metrics.items()}\n        }\n\n\n    # SEQUENCE\n    def _compute_sequence_metrics(self, logits, labels):\n        proba = torch.nn.functional.sigmoid(torch.tensor(logits)).numpy()\n        optimal_thresholds = self._find_thresholds_for_distribution(\n            proba, desired_distribution=self.sequence_class_distribution\n        )\n        binarized_preds = (proba >= np.array(optimal_thresholds)).astype(int)\n        \n        return {\"f1\": f1_score(labels, binarized_preds, average=\"macro\")}\n\n        \n    def _find_thresholds_for_distribution(self, preds, desired_distribution):\n        \"\"\"\n        Find thresholds for each class to achieve the desired class distribution.\n    \n        Args:\n            preds (ndarray): Array of shape (num_samples, num_classes) with probabilities (after sigmoid).\n            desired_distribution (list): Desired proportion of positive samples for each class.\n    \n        Returns:\n            thresholds (list): List of thresholds for each class.\n        \"\"\"\n        num_classes = preds.shape[1]\n        thresholds = []\n    \n        for class_idx in range(num_classes):\n            probs = preds[:, class_idx]\n            desired_ratio = desired_distribution[class_idx]\n    \n            # Function to minimize the difference between actual and desired positive ratios\n            def objective(threshold):\n                predicted_ratio = (probs >= threshold).mean()\n                return abs(predicted_ratio - desired_ratio)\n    \n            # Find the threshold using optimization\n            result = minimize_scalar(objective, bounds=(0, 1), method=\"bounded\")\n            thresholds.append(result.x)\n    \n        return thresholds\n\n\n    # TOKEN\n\n    def _compute_token_metrics(self, logits, labels):\n        eval_dataset = self.eval_dataset\n        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n    \n        #thresholds = np.linspace(0.1, 0.5, num=41)\n        thresholds = [self._find_optimal_threshold(probabilities, labels)]\n        results = []\n        best_f1 = -1\n        best_th = 0\n        best_metrics = None\n    \n        for thold in tqdm(thresholds):\n            # Apply thresholding instead of argmax\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n    \n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n    \n            pred_spans_all = []\n            for pred, offsets in zip(true_predictions, eval_dataset['offset_mapping']):\n                samplewise_spans = []\n                current_span = None\n                for token_label, span in zip(pred, offsets):\n                    if token_label == 1:  # If the current token is labeled as an entity (1)\n                        if current_span is None:\n                            current_span = [span[0], span[1]]  # Start a new span\n                        else:\n                            current_span[1] = span[1]  # Extend the span to include the current token\n                    else:  # If token_label == 0 (not an entity)\n                        if current_span is not None:\n                            samplewise_spans.append(tuple(current_span))  # Save completed span\n                            current_span = None  # Reset for the next entity\n    \n                # If the last token was part of a span, save it\n                if current_span is not None:\n                    samplewise_spans.append(tuple(current_span))\n    \n                pred_spans_all.append(samplewise_spans)\n    \n            # Store results for this threshold\n            current_metrics = self._calculate_inner_metric(eval_dataset['trigger_words'], pred_spans_all)\n            if current_metrics['f1'] >= best_f1:\n                best_f1 = current_metrics['f1']\n                best_th = thold\n                best_metrics = current_metrics\n                best_metrics['thold'] = thold\n                \n            \n            results.append(current_metrics)\n        return best_metrics\n\n\n    def _calculate_inner_metric(self, gt_spans_all, pred_spans_all):\n        total_true_chars = 0\n        total_pred_chars = 0\n        total_overlap_chars = 0\n        for true_spans, pred_spans in zip(gt_spans_all, pred_spans_all):\n            if isinstance(true_spans, str):\n                try:\n                    true_spans = eval(true_spans)\n                except Exception:\n                    true_spans = []\n                    \n            # Convert spans to sets of character indices.\n            true_chars = extract_chars_from_spans(true_spans)\n            pred_chars = extract_chars_from_spans(pred_spans)\n            \n            total_true_chars += len(true_chars)\n            total_pred_chars += len(pred_chars)\n            total_overlap_chars += len(true_chars.intersection(pred_chars))\n            \n            union_chars = true_chars.union(pred_chars)\n            \n        # Compute precision, recall, and F1.\n        precision = total_overlap_chars / total_pred_chars if total_pred_chars > 0 else 0\n        recall = total_overlap_chars / total_true_chars if total_true_chars > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        metrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1\n        }\n        return metrics\n\n    def _find_optimal_threshold(self, probabilities, labels):\n        \"\"\"Finds the threshold that achieves the desired positive class balance.\"\"\"\n        best_th = 0.5  # Default starting point\n        best_diff = float(\"inf\")\n        optimal_th = best_th\n        \n        for thold in np.linspace(0.01, 0.99, num=100):\n            predictions = (probabilities[:, :, 1] >= thold).astype(int)\n            true_predictions = [\n                [p for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            total_pos = sum([sum(row for row in prediction) for prediction in true_predictions])\n            total = sum([len(prediction) for prediction in true_predictions])\n            \n            positive_ratio = total_pos / total if total > 0 else 0\n            \n            diff = abs(positive_ratio - self.token_class_distribution)\n            if diff < best_diff:\n                best_diff = diff\n                optimal_th = thold\n        \n        return optimal_th","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:35:59.449593Z","iopub.execute_input":"2025-02-06T00:35:59.449934Z","iopub.status.idle":"2025-02-06T00:36:01.664033Z","shell.execute_reply.started":"2025-02-06T00:35:59.449891Z","shell.execute_reply":"2025-02-06T00:36:01.663351Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Train","metadata":{"_uuid":"f6e9954c-d610-4acc-92af-647aa1299f72","_cell_guid":"c103352a-8cf6-442d-b13c-aaa4b97f6af8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"EPOCHS = 5","metadata":{"_uuid":"c0df1238-cb4a-4d68-9954-0d6cd01152c7","_cell_guid":"e437c553-d5bf-493a-808c-6e31f86a1e5e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:36:01.664892Z","iopub.execute_input":"2025-02-06T00:36:01.665179Z","iopub.status.idle":"2025-02-06T00:36:01.668634Z","shell.execute_reply.started":"2025-02-06T00:36:01.665156Z","shell.execute_reply":"2025-02-06T00:36:01.667840Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\nclass CustomDataCollator(DataCollatorForTokenClassification):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n    \n    def __call__(self, features):\n        # Separate token-level and sequence-level labels\n        sequence_labels = [f.pop(\"sequence_labels\") for f in features]\n        \n        # Use Hugging Face's built-in collator for token classification\n        batch = super().torch_call(features)\n        \n        # Convert sequence labels to tensor\n        batch[\"sequence_labels\"] = torch.tensor(sequence_labels, dtype=torch.int64)\n        \n        return batch\n\n# Use the custom data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer)","metadata":{"_uuid":"4dba46cf-1458-48ea-8e01-6fd8b44828ab","_cell_guid":"2b300f6e-5b6c-4b54-afe0-b428b8d22f64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:36:01.669441Z","iopub.execute_input":"2025-02-06T00:36:01.669688Z","iopub.status.idle":"2025-02-06T00:36:01.777492Z","shell.execute_reply.started":"2025-02-06T00:36:01.669663Z","shell.execute_reply":"2025-02-06T00:36:01.776694Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW([\n    {'params': list(model.bert.parameters()), 'lr': 2e-5},\n    {'params': list(model.token_classifier.parameters()), 'lr': 1e-4},\n    {'params': list(model.sequence_classifier.parameters()), 'lr': 1e-4}\n])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0.1*EPOCHS*(ds_train.num_rows/16),\n    num_training_steps=EPOCHS*(ds_train.num_rows/16)\n)","metadata":{"_uuid":"d06fb774-8183-4478-aa4e-a7251d953493","_cell_guid":"d873684b-08ac-4bb7-85e0-b89cec712e68","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:36:01.778484Z","iopub.execute_input":"2025-02-06T00:36:01.778779Z","iopub.status.idle":"2025-02-06T00:36:01.795759Z","shell.execute_reply.started":"2025-02-06T00:36:01.778746Z","shell.execute_reply":"2025-02-06T00:36:01.794833Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=EPOCHS,\n    \n    output_dir=\"./results\",\n    logging_strategy=\"steps\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=\"none\"\n)\n\ntrainer = TokenSequenceEvaluationTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, scheduler),\n    sequence_class_distribution=SEQUENCE_CLASS_DISTRIBUTION,\n    token_class_distribution=TOKEN_CLASS_DISTRIBUTION\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:36:01.796722Z","iopub.execute_input":"2025-02-06T00:36:01.797009Z","iopub.status.idle":"2025-02-06T00:36:02.407808Z","shell.execute_reply.started":"2025-02-06T00:36:01.796978Z","shell.execute_reply":"2025-02-06T00:36:02.406903Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-18-e506cba59927>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `TokenSequenceEvaluationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"trainer.train()","metadata":{"_uuid":"227045e6-6321-4b21-881e-68c2fe25457d","_cell_guid":"05530542-136f-4239-ae55-7be22d786c74","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T00:36:02.410924Z","iopub.execute_input":"2025-02-06T00:36:02.411158Z","iopub.status.idle":"2025-02-06T01:00:51.406252Z","shell.execute_reply.started":"2025-02-06T00:36:02.411138Z","shell.execute_reply":"2025-02-06T01:00:51.405384Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [960/960 24:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Sequence F1</th>\n      <th>Token Precision</th>\n      <th>Token Recall</th>\n      <th>Token F1</th>\n      <th>Token Thold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.665100</td>\n      <td>0.713234</td>\n      <td>0.107196</td>\n      <td>0.611214</td>\n      <td>0.555080</td>\n      <td>0.581797</td>\n      <td>0.534646</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.682500</td>\n      <td>0.720087</td>\n      <td>0.227798</td>\n      <td>0.619421</td>\n      <td>0.551075</td>\n      <td>0.583253</td>\n      <td>0.316869</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.629800</td>\n      <td>0.676411</td>\n      <td>0.297652</td>\n      <td>0.614783</td>\n      <td>0.551915</td>\n      <td>0.581655</td>\n      <td>0.504949</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.544200</td>\n      <td>0.747947</td>\n      <td>0.299959</td>\n      <td>0.619465</td>\n      <td>0.550804</td>\n      <td>0.583120</td>\n      <td>0.326768</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.421400</td>\n      <td>0.735057</td>\n      <td>0.338227</td>\n      <td>0.615062</td>\n      <td>0.550804</td>\n      <td>0.581162</td>\n      <td>0.425758</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5aa55fe032a4a89bbbc788f0b2b07aa"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cfe2bdbbd3a4b56a93078e54eeeecb4"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e631096552be4f3dbbb0c09887742636"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eced3e76f3b14582bc996c49aac23647"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f03d9b098d47de9569444b457b6c53"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=960, training_loss=0.6608349964022636, metrics={'train_runtime': 1488.5825, 'train_samples_per_second': 10.272, 'train_steps_per_second': 0.645, 'total_flos': 3638254796488656.0, 'train_loss': 0.6608349964022636, 'epoch': 5.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FINETUNED_MODEL = '/kaggle/working/results/checkpoint-960'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:32:50.106933Z","iopub.execute_input":"2025-02-06T00:32:50.107227Z","iopub.status.idle":"2025-02-06T00:32:50.111018Z","shell.execute_reply.started":"2025-02-06T00:32:50.107206Z","shell.execute_reply":"2025-02-06T00:32:50.109997Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}